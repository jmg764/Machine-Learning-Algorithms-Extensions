{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Softmax Extension with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits # The MNIST data set is in scikit learn data set\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import numpy as np\n",
    "import numpy.random as r # We will randomly initialize our weights\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits=load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Scale training features\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test set.  60% training and %40 test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert digits to vectors\n",
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(30, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "# Compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.0974\n",
      "Epoch 2/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.1025 - accuracy: 0.1150\n",
      "Epoch 3/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0933 - accuracy: 0.1187\n",
      "Epoch 4/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0910 - accuracy: 0.1475\n",
      "Epoch 5/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0902 - accuracy: 0.1549\n",
      "Epoch 6/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0897 - accuracy: 0.2245\n",
      "Epoch 7/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0895 - accuracy: 0.2458\n",
      "Epoch 8/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0894 - accuracy: 0.2291\n",
      "Epoch 9/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0892 - accuracy: 0.2440\n",
      "Epoch 10/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0891 - accuracy: 0.2393\n",
      "Epoch 11/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0890 - accuracy: 0.2551\n",
      "Epoch 12/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0889 - accuracy: 0.2477\n",
      "Epoch 13/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0889 - accuracy: 0.2393\n",
      "Epoch 14/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0888 - accuracy: 0.2607\n",
      "Epoch 15/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0887 - accuracy: 0.2356\n",
      "Epoch 16/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0886 - accuracy: 0.2746\n",
      "Epoch 17/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0885 - accuracy: 0.2644\n",
      "Epoch 18/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0884 - accuracy: 0.2681\n",
      "Epoch 19/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0883 - accuracy: 0.2848\n",
      "Epoch 20/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0882 - accuracy: 0.2709\n",
      "Epoch 21/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0881 - accuracy: 0.2996\n",
      "Epoch 22/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0880 - accuracy: 0.3145\n",
      "Epoch 23/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0879 - accuracy: 0.2904\n",
      "Epoch 24/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0878 - accuracy: 0.3247\n",
      "Epoch 25/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0877 - accuracy: 0.3302\n",
      "Epoch 26/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0875 - accuracy: 0.3210\n",
      "Epoch 27/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0874 - accuracy: 0.3367\n",
      "Epoch 28/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0873 - accuracy: 0.3460\n",
      "Epoch 29/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0872 - accuracy: 0.3377\n",
      "Epoch 30/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0870 - accuracy: 0.3544\n",
      "Epoch 31/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0869 - accuracy: 0.3646\n",
      "Epoch 32/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0868 - accuracy: 0.3673: 0s - loss: 0.0868 - ac\n",
      "Epoch 33/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0866 - accuracy: 0.3711\n",
      "Epoch 34/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0865 - accuracy: 0.3794\n",
      "Epoch 35/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0863 - accuracy: 0.3868\n",
      "Epoch 36/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0862 - accuracy: 0.3878\n",
      "Epoch 37/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0860 - accuracy: 0.3980\n",
      "Epoch 38/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0858 - accuracy: 0.4119\n",
      "Epoch 39/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0856 - accuracy: 0.3980\n",
      "Epoch 40/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0855 - accuracy: 0.4109\n",
      "Epoch 41/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0853 - accuracy: 0.4360\n",
      "Epoch 42/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0851 - accuracy: 0.4147\n",
      "Epoch 43/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0849 - accuracy: 0.4425\n",
      "Epoch 44/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0846 - accuracy: 0.4425\n",
      "Epoch 45/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0844 - accuracy: 0.4527\n",
      "Epoch 46/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0842 - accuracy: 0.4527\n",
      "Epoch 47/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0840 - accuracy: 0.4638\n",
      "Epoch 48/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0837 - accuracy: 0.4564\n",
      "Epoch 49/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0835 - accuracy: 0.4703\n",
      "Epoch 50/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0832 - accuracy: 0.4731\n",
      "Epoch 51/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0830 - accuracy: 0.4842: 0s - loss: 0.0\n",
      "Epoch 52/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0827 - accuracy: 0.4852\n",
      "Epoch 53/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0824 - accuracy: 0.4824\n",
      "Epoch 54/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0822 - accuracy: 0.4898\n",
      "Epoch 55/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0819 - accuracy: 0.4944\n",
      "Epoch 56/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0816 - accuracy: 0.4926\n",
      "Epoch 57/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0813 - accuracy: 0.5074\n",
      "Epoch 58/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0810 - accuracy: 0.4935\n",
      "Epoch 59/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0807 - accuracy: 0.5121\n",
      "Epoch 60/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0803 - accuracy: 0.5158\n",
      "Epoch 61/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0800 - accuracy: 0.5121\n",
      "Epoch 62/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0797 - accuracy: 0.5241\n",
      "Epoch 63/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0793 - accuracy: 0.5362\n",
      "Epoch 64/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0790 - accuracy: 0.5417\n",
      "Epoch 65/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0787 - accuracy: 0.5362\n",
      "Epoch 66/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0783 - accuracy: 0.5501\n",
      "Epoch 67/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0780 - accuracy: 0.5529\n",
      "Epoch 68/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0776 - accuracy: 0.5622\n",
      "Epoch 69/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0772 - accuracy: 0.5659\n",
      "Epoch 70/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0769 - accuracy: 0.5677\n",
      "Epoch 71/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0765 - accuracy: 0.5788\n",
      "Epoch 72/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0761 - accuracy: 0.5751\n",
      "Epoch 73/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0758 - accuracy: 0.5853\n",
      "Epoch 74/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0754 - accuracy: 0.5928\n",
      "Epoch 75/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0750 - accuracy: 0.5955\n",
      "Epoch 76/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0746 - accuracy: 0.5993\n",
      "Epoch 77/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0742 - accuracy: 0.5974\n",
      "Epoch 78/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0738 - accuracy: 0.6058\n",
      "Epoch 79/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0734 - accuracy: 0.6141: 0s - loss: 0.0\n",
      "Epoch 80/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0730 - accuracy: 0.6113\n",
      "Epoch 81/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0726 - accuracy: 0.6197\n",
      "Epoch 82/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0722 - accuracy: 0.6262\n",
      "Epoch 83/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0718 - accuracy: 0.6243\n",
      "Epoch 84/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0714 - accuracy: 0.6243\n",
      "Epoch 85/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0710 - accuracy: 0.6206\n",
      "Epoch 86/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0706 - accuracy: 0.6197\n",
      "Epoch 87/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0701 - accuracy: 0.6299\n",
      "Epoch 88/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0697 - accuracy: 0.6280\n",
      "Epoch 89/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0693 - accuracy: 0.6336\n",
      "Epoch 90/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0689 - accuracy: 0.6373\n",
      "Epoch 91/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0684 - accuracy: 0.6429\n",
      "Epoch 92/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0680 - accuracy: 0.6429\n",
      "Epoch 93/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0676 - accuracy: 0.6373\n",
      "Epoch 94/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0671 - accuracy: 0.6484\n",
      "Epoch 95/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0667 - accuracy: 0.6447\n",
      "Epoch 96/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0662 - accuracy: 0.6438\n",
      "Epoch 97/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0658 - accuracy: 0.6466\n",
      "Epoch 98/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0653 - accuracy: 0.6475\n",
      "Epoch 99/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0649 - accuracy: 0.6429\n",
      "Epoch 100/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0645 - accuracy: 0.6466: 0s - loss: 0.0646 - accuracy\n",
      "Epoch 101/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0640 - accuracy: 0.6475\n",
      "Epoch 102/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0636 - accuracy: 0.6521\n",
      "Epoch 103/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0631 - accuracy: 0.6401\n",
      "Epoch 104/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0627 - accuracy: 0.6531\n",
      "Epoch 105/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0622 - accuracy: 0.6540\n",
      "Epoch 106/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0618 - accuracy: 0.6512\n",
      "Epoch 107/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0614 - accuracy: 0.6531\n",
      "Epoch 108/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0609 - accuracy: 0.6531\n",
      "Epoch 109/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0605 - accuracy: 0.6586\n",
      "Epoch 110/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0601 - accuracy: 0.6623\n",
      "Epoch 111/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0597 - accuracy: 0.6605\n",
      "Epoch 112/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0592 - accuracy: 0.6642\n",
      "Epoch 113/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0588 - accuracy: 0.6660\n",
      "Epoch 114/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0584 - accuracy: 0.6688\n",
      "Epoch 115/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0580 - accuracy: 0.6698\n",
      "Epoch 116/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0576 - accuracy: 0.6642\n",
      "Epoch 117/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0572 - accuracy: 0.6781\n",
      "Epoch 118/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0568 - accuracy: 0.6725\n",
      "Epoch 119/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0564 - accuracy: 0.6735\n",
      "Epoch 120/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0560 - accuracy: 0.6772\n",
      "Epoch 121/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0557 - accuracy: 0.6800\n",
      "Epoch 122/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0553 - accuracy: 0.6809\n",
      "Epoch 123/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0549 - accuracy: 0.6865\n",
      "Epoch 124/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0545 - accuracy: 0.6846\n",
      "Epoch 125/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0542 - accuracy: 0.6902\n",
      "Epoch 126/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0538 - accuracy: 0.6883\n",
      "Epoch 127/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0535 - accuracy: 0.6920\n",
      "Epoch 128/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0531 - accuracy: 0.6985\n",
      "Epoch 129/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0528 - accuracy: 0.6976\n",
      "Epoch 130/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0525 - accuracy: 0.7078\n",
      "Epoch 131/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0521 - accuracy: 0.7096\n",
      "Epoch 132/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0518 - accuracy: 0.7124\n",
      "Epoch 133/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0515 - accuracy: 0.7245\n",
      "Epoch 134/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0512 - accuracy: 0.7236\n",
      "Epoch 135/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0508 - accuracy: 0.7245\n",
      "Epoch 136/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0505 - accuracy: 0.7291\n",
      "Epoch 137/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0502 - accuracy: 0.7301\n",
      "Epoch 138/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0499 - accuracy: 0.7393\n",
      "Epoch 139/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0496 - accuracy: 0.7384\n",
      "Epoch 140/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0493 - accuracy: 0.7430\n",
      "Epoch 141/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0490 - accuracy: 0.7458\n",
      "Epoch 142/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0488 - accuracy: 0.7449\n",
      "Epoch 143/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0485 - accuracy: 0.7514\n",
      "Epoch 144/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0482 - accuracy: 0.7560\n",
      "Epoch 145/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0479 - accuracy: 0.7579\n",
      "Epoch 146/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0476 - accuracy: 0.7625\n",
      "Epoch 147/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0473 - accuracy: 0.7653\n",
      "Epoch 148/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0471 - accuracy: 0.7616\n",
      "Epoch 149/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0468 - accuracy: 0.7681\n",
      "Epoch 150/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0465 - accuracy: 0.7746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x152ee1d90>"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "model.fit(X_train, y_v_train, epochs = 150, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/719 [==============================] - 0s 243us/step\n",
      "Accuracy: 77.75\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the keras model\n",
    "accuracy = model.evaluate(X_test, y_v_test)\n",
    "print('Accuracy: %.2f' % (accuracy[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras implementation plus extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keras model with softmax outer layer\n",
    "model_plus_ext = Sequential()\n",
    "model_plus_ext.add(Dense(64, activation='sigmoid'))\n",
    "model_plus_ext.add(Dense(30, activation='sigmoid'))\n",
    "model_plus_ext.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the keras model\n",
    "model_plus_ext.compile(loss='mean_squared_error', optimizer='sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0939 - accuracy: 0.0853\n",
      "Epoch 2/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0918 - accuracy: 0.0937\n",
      "Epoch 3/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0909 - accuracy: 0.0974: 0s - loss: 0.0910 - accuracy: 0.\n",
      "Epoch 4/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0904 - accuracy: 0.0937\n",
      "Epoch 5/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0901 - accuracy: 0.1002\n",
      "Epoch 6/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0899 - accuracy: 0.1113\n",
      "Epoch 7/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0897 - accuracy: 0.1262\n",
      "Epoch 8/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0896 - accuracy: 0.1373\n",
      "Epoch 9/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0894 - accuracy: 0.1605\n",
      "Epoch 10/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0893 - accuracy: 0.1948\n",
      "Epoch 11/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0892 - accuracy: 0.2226\n",
      "Epoch 12/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0891 - accuracy: 0.2384\n",
      "Epoch 13/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0890 - accuracy: 0.2449\n",
      "Epoch 14/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0889 - accuracy: 0.2532\n",
      "Epoch 15/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0888 - accuracy: 0.2523\n",
      "Epoch 16/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0887 - accuracy: 0.2542\n",
      "Epoch 17/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0886 - accuracy: 0.2607\n",
      "Epoch 18/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0885 - accuracy: 0.2635\n",
      "Epoch 19/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0884 - accuracy: 0.2653\n",
      "Epoch 20/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0883 - accuracy: 0.2653\n",
      "Epoch 21/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0882 - accuracy: 0.2662\n",
      "Epoch 22/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0881 - accuracy: 0.2848\n",
      "Epoch 23/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0880 - accuracy: 0.2746\n",
      "Epoch 24/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0879 - accuracy: 0.2857\n",
      "Epoch 25/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0877 - accuracy: 0.2987\n",
      "Epoch 26/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0876 - accuracy: 0.2959\n",
      "Epoch 27/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0875 - accuracy: 0.3033\n",
      "Epoch 28/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0874 - accuracy: 0.3015\n",
      "Epoch 29/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0873 - accuracy: 0.3173\n",
      "Epoch 30/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0871 - accuracy: 0.3015\n",
      "Epoch 31/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0870 - accuracy: 0.3414\n",
      "Epoch 32/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0869 - accuracy: 0.3534\n",
      "Epoch 33/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0867 - accuracy: 0.3627\n",
      "Epoch 34/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0866 - accuracy: 0.3516\n",
      "Epoch 35/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0865 - accuracy: 0.3692\n",
      "Epoch 36/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0863 - accuracy: 0.3711\n",
      "Epoch 37/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0861 - accuracy: 0.3636\n",
      "Epoch 38/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0860 - accuracy: 0.3952\n",
      "Epoch 39/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0858 - accuracy: 0.4100\n",
      "Epoch 40/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0857 - accuracy: 0.4109\n",
      "Epoch 41/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0855 - accuracy: 0.4351\n",
      "Epoch 42/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0853 - accuracy: 0.4249\n",
      "Epoch 43/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0851 - accuracy: 0.4397\n",
      "Epoch 44/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0849 - accuracy: 0.4314: 0s - loss: 0.084\n",
      "Epoch 45/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0847 - accuracy: 0.4323\n",
      "Epoch 46/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0845 - accuracy: 0.4518\n",
      "Epoch 47/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0843 - accuracy: 0.4499\n",
      "Epoch 48/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0841 - accuracy: 0.4814\n",
      "Epoch 49/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0839 - accuracy: 0.4796\n",
      "Epoch 50/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0836 - accuracy: 0.4647\n",
      "Epoch 51/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0834 - accuracy: 0.4759\n",
      "Epoch 52/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0831 - accuracy: 0.4907\n",
      "Epoch 53/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0828 - accuracy: 0.4926\n",
      "Epoch 54/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0826 - accuracy: 0.4954\n",
      "Epoch 55/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0823 - accuracy: 0.4852\n",
      "Epoch 56/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0820 - accuracy: 0.4935\n",
      "Epoch 57/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0817 - accuracy: 0.5167\n",
      "Epoch 58/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0814 - accuracy: 0.5148\n",
      "Epoch 59/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0810 - accuracy: 0.5297\n",
      "Epoch 60/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0807 - accuracy: 0.5427\n",
      "Epoch 61/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0804 - accuracy: 0.5408\n",
      "Epoch 62/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0800 - accuracy: 0.5445\n",
      "Epoch 63/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0796 - accuracy: 0.5612\n",
      "Epoch 64/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0792 - accuracy: 0.5547\n",
      "Epoch 65/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0788 - accuracy: 0.5668\n",
      "Epoch 66/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0784 - accuracy: 0.5742\n",
      "Epoch 67/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0780 - accuracy: 0.5751\n",
      "Epoch 68/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0776 - accuracy: 0.5853\n",
      "Epoch 69/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0771 - accuracy: 0.5918\n",
      "Epoch 70/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0767 - accuracy: 0.5946: 0s - los\n",
      "Epoch 71/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0762 - accuracy: 0.6122\n",
      "Epoch 72/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0757 - accuracy: 0.6224\n",
      "Epoch 73/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0752 - accuracy: 0.6382\n",
      "Epoch 74/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0747 - accuracy: 0.6354\n",
      "Epoch 75/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0742 - accuracy: 0.6447\n",
      "Epoch 76/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0737 - accuracy: 0.6605\n",
      "Epoch 77/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0731 - accuracy: 0.6633\n",
      "Epoch 78/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0726 - accuracy: 0.6725\n",
      "Epoch 79/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0720 - accuracy: 0.6781\n",
      "Epoch 80/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0714 - accuracy: 0.6809\n",
      "Epoch 81/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0708 - accuracy: 0.6809\n",
      "Epoch 82/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0702 - accuracy: 0.6874\n",
      "Epoch 83/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0696 - accuracy: 0.6920\n",
      "Epoch 84/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0690 - accuracy: 0.6967\n",
      "Epoch 85/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0684 - accuracy: 0.6948\n",
      "Epoch 86/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0677 - accuracy: 0.7004\n",
      "Epoch 87/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0671 - accuracy: 0.7022\n",
      "Epoch 88/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0664 - accuracy: 0.7069\n",
      "Epoch 89/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0657 - accuracy: 0.7059\n",
      "Epoch 90/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0651 - accuracy: 0.7032\n",
      "Epoch 91/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0644 - accuracy: 0.7152\n",
      "Epoch 92/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0637 - accuracy: 0.7134\n",
      "Epoch 93/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0630 - accuracy: 0.7189\n",
      "Epoch 94/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0623 - accuracy: 0.7180\n",
      "Epoch 95/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0616 - accuracy: 0.7245\n",
      "Epoch 96/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0609 - accuracy: 0.7226\n",
      "Epoch 97/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0602 - accuracy: 0.7319\n",
      "Epoch 98/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0595 - accuracy: 0.7273\n",
      "Epoch 99/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0589 - accuracy: 0.7291\n",
      "Epoch 100/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0582 - accuracy: 0.7347\n",
      "Epoch 101/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0575 - accuracy: 0.7403\n",
      "Epoch 102/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0568 - accuracy: 0.7430\n",
      "Epoch 103/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0561 - accuracy: 0.7486\n",
      "Epoch 104/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0554 - accuracy: 0.7495\n",
      "Epoch 105/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0547 - accuracy: 0.7495\n",
      "Epoch 106/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0540 - accuracy: 0.7532\n",
      "Epoch 107/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0534 - accuracy: 0.7560\n",
      "Epoch 108/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0527 - accuracy: 0.7542\n",
      "Epoch 109/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0520 - accuracy: 0.7644\n",
      "Epoch 110/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0514 - accuracy: 0.7653\n",
      "Epoch 111/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0507 - accuracy: 0.7718\n",
      "Epoch 112/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0501 - accuracy: 0.7727\n",
      "Epoch 113/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0494 - accuracy: 0.7792\n",
      "Epoch 114/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0488 - accuracy: 0.7801\n",
      "Epoch 115/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0482 - accuracy: 0.7820\n",
      "Epoch 116/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0475 - accuracy: 0.7866\n",
      "Epoch 117/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0469 - accuracy: 0.7941\n",
      "Epoch 118/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0463 - accuracy: 0.7894\n",
      "Epoch 119/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0457 - accuracy: 0.7931\n",
      "Epoch 120/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0451 - accuracy: 0.7950\n",
      "Epoch 121/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0445 - accuracy: 0.7968\n",
      "Epoch 122/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0439 - accuracy: 0.7959\n",
      "Epoch 123/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0433 - accuracy: 0.7996\n",
      "Epoch 124/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0427 - accuracy: 0.7996\n",
      "Epoch 125/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0422 - accuracy: 0.8024\n",
      "Epoch 126/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0416 - accuracy: 0.8006: 0s - loss: 0.041\n",
      "Epoch 127/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0410 - accuracy: 0.8043\n",
      "Epoch 128/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0405 - accuracy: 0.8015\n",
      "Epoch 129/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0400 - accuracy: 0.8089\n",
      "Epoch 130/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0394 - accuracy: 0.8108\n",
      "Epoch 131/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0389 - accuracy: 0.8089\n",
      "Epoch 132/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0384 - accuracy: 0.8071\n",
      "Epoch 133/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0379 - accuracy: 0.8117\n",
      "Epoch 134/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0374 - accuracy: 0.8117\n",
      "Epoch 135/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0369 - accuracy: 0.8145\n",
      "Epoch 136/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0364 - accuracy: 0.8163\n",
      "Epoch 137/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0359 - accuracy: 0.8163\n",
      "Epoch 138/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0354 - accuracy: 0.8182\n",
      "Epoch 139/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0350 - accuracy: 0.8182\n",
      "Epoch 140/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0345 - accuracy: 0.8200\n",
      "Epoch 141/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0341 - accuracy: 0.8265\n",
      "Epoch 142/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0336 - accuracy: 0.8302\n",
      "Epoch 143/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0332 - accuracy: 0.8312\n",
      "Epoch 144/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0328 - accuracy: 0.8312\n",
      "Epoch 145/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0323 - accuracy: 0.8330\n",
      "Epoch 146/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0319 - accuracy: 0.8349\n",
      "Epoch 147/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0315 - accuracy: 0.8377\n",
      "Epoch 148/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0311 - accuracy: 0.8386\n",
      "Epoch 149/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0307 - accuracy: 0.8386\n",
      "Epoch 150/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0303 - accuracy: 0.8414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1532bd0d0>"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "model_plus_ext.fit(X_train, y_v_train, epochs = 150, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/719 [==============================] - 0s 256us/step\n",
      "Accuracy: 88.73\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the keras model\n",
    "accuracy = model_plus_ext.evaluate(X_test, y_v_test)\n",
    "print('Accuracy: %.2f' % (accuracy[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainX\n",
    "y_train = trainY\n",
    "X_test = testX\n",
    "y_test = testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the digits dataset:\n",
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAECCAYAAAD+eGJTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASWUlEQVR4nO3dfWxVdZoH8O9jQd5aeatIQVZ8gchm45a1ohE16ijB+UPxDYc/JhjHxZgxGZMxWeM/mpiNREdn5w9igi8ZJjpOTIRV49sQM4k7AZVKCK12dwVEBWoLgtDSltLy7B89uB2493nKPffeczrP95NMaO+3t/d3z63fOfee3/kdUVUQUVxnZT0AIsoWS4AoOJYAUXAsAaLgWAJEwbEEiILLpAREZKmI/I+I7BCRR7MYg0VEdotIi4hsE5HmHIznZRHpFJHWYbdNE5GNIvJl8u/UnI3vCRHZm2zDbSLy0wzHN0dE/iIibSLyuYj8Krk9F9vQGF9VtqFUe56AiNQA+F8ANwPYA2ALgBWq+kVVB2IQkd0AmlT1QNZjAQARuQ5AN4A/qOo/Jbc9DeCgqq5OinSqqv5bjsb3BIBuVf1NFmMaTkQaADSo6lYRqQPwGYBlAO5FDrahMb7lqMI2zGJPYBGAHaq6S1X7AfwJwG0ZjGPUUNWPABw85ebbAKxLvl6HoT+aTBQZX26oaruqbk2+7gLQBmA2crINjfFVRRYlMBvAt8O+34MqPuERUgB/FpHPRGRV1oMp4jxVbQeG/ogAzMh4PIU8JCLbk7cLmb1dGU5E5gJYCOAT5HAbnjI+oArbMIsSkAK35W3u8mJV/RcAtwD4ZbK7S2fmeQAXA2gE0A7g2WyHA4hILYA3ADysqkeyHs+pCoyvKtswixLYA2DOsO/PB7Avg3EUpar7kn87AWzA0FuYvOlI3kuefE/ZmfF4/oaqdqjqoKqeAPACMt6GIjIWQ/+Bvaqq65Obc7MNC42vWtswixLYAmCeiFwoImcD+BmAtzIYR0EiMin5cAYiMgnAEgCt9r0y8RaAlcnXKwG8meFYTnPyP67E7chwG4qIAHgJQJuqPjcsysU2LDa+am3Dqh8dAIDkUMd/AKgB8LKq/nvVB1GEiFyEof/3B4AxAP6Y9fhE5DUA1wOoB9AB4HEA/wngdQD/AOAbAHeraiYfzhUZ3/UY2o1VALsBPHDy/XcG47sGwH8BaAFwIrn5MQy97858GxrjW4EqbMNMSoCI8oMzBomCYwkQBccSIAqOJUAUHEuAKLhMSyDHU3IBcHxp5Xl8eR4bUN3xZb0nkOsXAhxfWnkeX57HBlRxfFmXABFlLNVkIRFZCuB3GJr596KqrnZ+njOTiDKiqoVO3iu9BEpZHIQlQJSdYiWQ5u0AFwch+juQpgRGw+IgROQYk+K+I1ocJDnUkfdPYonCSlMCI1ocRFXXAlgL8DMBojxK83Yg14uDENHIlLwnoKoDIvIQgA/w/4uDfF62kRFRVVR1URG+HSDKTiUOERLR3wGWAFFwLAGi4FgCRMGxBIiCYwkQBccSIAqOJUAUHEuAKDiWAFFwLAGi4FgCRMGxBIiCYwkQBZdmZSEaZUQKnkn6o7SnldfV1Zn5NddcY+bvvfdeqsf3nl9NTY2ZDwwMpHr8tLzxe0p9/bgnQBQcS4AoOJYAUXAsAaLgWAJEwbEEiIJjCRAFx3kCgZx1lt35g4ODZn7JJZeY+f3332/mvb29Zn706FEz7+vrM/NPP/3UzNPOA/CO43vb17t/2vFZ8yCs15Z7AkTBsQSIgmMJEAXHEiAKjiVAFBxLgCg4lgBRcJwnEIh3Pr03T+DGG28085tuusnM9+zZY+bjxo0z84kTJ5r5zTffbOYvvviimXd0dJi5d76+t/08tbW1Zn7ixAkz7+npKelxU5WAiOwG0AVgEMCAqjal+X1EVH3l2BO4QVUPlOH3EFEG+JkAUXBpS0AB/FlEPhORVeUYEBFVV9q3A4tVdZ+IzACwUUT+W1U/Gv4DSTmwIIhyKtWegKruS/7tBLABwKICP7NWVZv4oSFRPpVcAiIySUTqTn4NYAmA1nINjIiqI83bgfMAbEjOkR4D4I+q+n5ZRkUV0d/fn+r+V1xxhZnPnTvXzL15Ct75+B988IGZL1y40MyffvppM29ubjbzlpYWM29razPzRYtO21H+G9723bRpk5lv3ry5aNbd3V00K7kEVHUXgH8u9f5ElA88REgUHEuAKDiWAFFwLAGi4FgCRMGxBIiCk7TXpD+jBxOp3oMF5K1r773W3vn43nH2KVOmmPnx48fN3Dtf3rNlyxYz37Fjh5mnnUfR0NBg5t7z98Z/1113mfmaNWuKZs3NzThy5EjBPxDuCRAFxxIgCo4lQBQcS4AoOJYAUXAsAaLgWAJEwXGeQI54x/nT8l7rjz/+2My99QI83vMbGBgw87TH8fv6+szcm6ewdetWM/fmIXjPb+nSpWZ+0UUXmfns2bPNXFU5T4CITscSIAqOJUAUHEuAKDiWAFFwLAGi4FgCRMGV46rEVCbVnLNRyKFDh8zcO1++t7fXzMeNG2fmY8bYf461tbVm7s0DmDBhgpl78wSuvfZaM7/66qvN3LuuwowZM8z8/fcrc1kP7gkQBccSIAqOJUAUHEuAKDiWAFFwLAGi4FgCRMFxngD9aOLEiWbuHef28p6eHjM/fPiwmX///fdm7q134M3D8NY78J6ft/0GBwfN3JunMGfOHDMvlbsnICIvi0iniLQOu22aiGwUkS+Tf6dWZHREVHEjeTvwewCnLnnyKIAPVXUegA+T74loFHJLQFU/AnDwlJtvA7Au+XodgGVlHhcRVUmpHwyep6rtAJD8a096JqLcqvgHgyKyCsCqSj8OEZWm1D2BDhFpAIDk385iP6iqa1W1SVWbSnwsIqqgUkvgLQArk69XAnizPMMhompz3w6IyGsArgdQLyJ7ADwOYDWA10XkFwC+AXB3JQcZRdrj1N5xaO98/FmzZpn5sWPHUuXeegLedQW8eQZTpkwxc2+egXec/+yzzzbzrq4uM588ebKZb9++3cy916+pqfjO9hdffFE0c0tAVVcUiX7i3ZeI8o/ThomCYwkQBccSIAqOJUAUHEuAKDiWAFFwXE8gR7zz3Wtqaszcmydwzz33mPnMmTPNfP/+/Waedl3/SZMmmbl3Pr03z8Cbp3D8+HEz966L4D3/6dOnm/maNWvMvLGx0cyt8VlzULgnQBQcS4AoOJYAUXAsAaLgWAJEwbEEiIJjCRAFJ96x6bI+mEj1HmwU8o5DDwwMpPr9V155pZm/8847Zt7b22vmaecx1NXVmXlfX5+Ze+sFjB07NlXuzWM4dOiQmXu85/fMM8+Y+SuvvGLmqlpwsgD3BIiCYwkQBccSIAqOJUAUHEuAKDiWAFFwLAGi4EbVegLeuvzecWpv3X7v93vnm3vny3vSzgPwvPvuu2Z+9OhRM/fmCXjr8ntzUrz1CrzXd/z48WbuvX6etK+/N/7LLrvMzA8fPmzmpeKeAFFwLAGi4FgCRMGxBIiCYwkQBccSIAqOJUAUXK7mCaQ9H73Sx9kr7brrrjPzO++808wXL15s5j09PWbunY/vzQPw1kPwXj9vfN7fh3ddAW8egTePwRufx9t+3d3dZn7HHXeY+dtvv33GYwJGsCcgIi+LSKeItA677QkR2Ssi25L//bSkRyeizI3k7cDvASwtcPtvVbUx+Z89FY2IcsstAVX9CMDBKoyFiDKQ5oPBh0Rke/J2YWrZRkREVVVqCTwP4GIAjQDaATxb7AdFZJWINItIc4mPRUQVVFIJqGqHqg6q6gkALwBYZPzsWlVtUtWmUgdJRJVTUgmISMOwb28H0FrsZ4ko39zrDojIawCuB1APoAPA48n3jQAUwG4AD6hqu/tgGV93YNq0aWY+a9YsM583b16q+3vHeefPn2/mx44dM3NvvQTvfPgJEyaY+b59+8zcW7ffO04+ffp0M+/v7zfziRMnmvmmTZvMvLa21sy9eRzeegLeegDe9uvo6DDzBQsWmHmx6w64k4VUdUWBm1/y7kdEowOnDRMFxxIgCo4lQBQcS4AoOJYAUXAsAaLg3HkCZX0wZ57AVVddZd7/ySefNPNzzz3XzKdMmWLm3vnu3vnsP/zwg5l76x14x7m94+TedRO86wa0tbWZ+fLly828udmeGV5XV2fmU6fap6DMnTvXzD27du0yc298XV1dZu6tN+DNw/DmKZxzzjlm7v39FJsnwD0BouBYAkTBsQSIgmMJEAXHEiAKjiVAFBxLgCi4qs8TsI61b9682bx/Q0ODmXvH+dOue+/x5hF4x+nTmjx5spnX19eb+b333mvmS5YsMfMHH3zQzL31CPr6+sz8q6++MnNvHoC3HkTa9Qy89QC8eQje/b31Ci644AIz5zwBIiqIJUAUHEuAKDiWAFFwLAGi4FgCRMGxBIiCq+o8gfr6er311luL5qtXrzbvv3PnTjP3zsf2cu/69h7vOK93HP/bb781c+84u7eegnddgpkzZ5r5smXLzHz8+PFm7q0H4L0+l19+earce/7ePADv/t51FTzeehDe35e1Hsd3332H/v5+zhMgotOxBIiCYwkQBccSIAqOJUAUHEuAKDiWAFFw7qXJy2lgYACdnZ1Fc+84uXc+9rFjx8zc+/3ecWrvOLC3LvzBgwfN/OuvvzZzb3zeegXe+fredRE2bNhg5i0tLWbuzROYNm2amXvH8b3rPhw/ftzMvefvnc+fdj0Ab56A9/c3f/78opm1bdw9ARGZIyJ/EZE2EflcRH6V3D5NRDaKyJfJv/aVI4gol0bydmAAwK9VdQGAqwD8UkT+EcCjAD5U1XkAPky+J6JRxi0BVW1X1a3J110A2gDMBnAbgHXJj60DYM8pJaJcOqMPBkVkLoCFAD4BcJ6qtgNDRQFgRrkHR0SVN+ISEJFaAG8AeFhVj5zB/VaJSLOINHsf7BBR9Y2oBERkLIYK4FVVXZ/c3CEiDUneAKDgx/6qulZVm1S1Ke1ZVkRUfiM5OiAAXgLQpqrPDYveArAy+XolgDfLPzwiqrSRzBNYDODnAFpEZFty22MAVgN4XUR+AeAbAHd7v6i/vx979+4tmntrG+zZs8fMJ02aZObeuvveceYDBw6Y+f79+818zBh7c3vrGXjHob3z+b15Ft758t7zX7BggZkfPXrUzL15HIcOHTJzb/t54087j8C7/4QJE8zcW8/h8OHDZt7Y2Fg0a21tLZq5JaCqfwVQbBbDT7z7E1G+cdowUXAsAaLgWAJEwbEEiIJjCRAFxxIgCq6q6wn09vZi27ZtRfP169cXzQDgvvvuM3NvXX7v+vXe+fbe+fzecXzvOLE3o7KmpsbMvfUUBgcHzdybp9HT02Pm7e3tqX6/Nz5vnkXa1y/tegWVXs/gwgsvNPOOjo6SHpt7AkTBsQSIgmMJEAXHEiAKjiVAFBxLgCg4lgBRcOIduy3rg4mkerBbbrnFzB955BEznzHDXgbRO9/cOw7sHef2jvN78wS84+Te7/fWtff+Frx5EF7uPT/v/t74Pd79rePsI+E9P++6A956Atu3bzfz5cuXm7mqFtwA3BMgCo4lQBQcS4AoOJYAUXAsAaLgWAJEwbEEiIKr+jwBa2177zhqWjfccIOZP/XUU2buzTOYPHmymXvr+nvH+b15At48BU9nZ8GLSP3I+1uxrikB+K9vd3e3mXvbx+ON3zvf31tPwXt9N27caOZtbW1mvmnTJjP3cJ4AERXEEiAKjiVAFBxLgCg4lgBRcCwBouBYAkTBufMERGQOgD8AmAngBIC1qvo7EXkCwL8C2J/86GOq+q7zu6o3KSEDl156qZnX19ebubdewfnnn2/mu3fvNnPvOPjOnTvNnEa3YvMERnLxkQEAv1bVrSJSB+AzETk56+G3qvqbcg2SiKrPLQFVbQfQnnzdJSJtAGZXemBEVB1n9JmAiMwFsBDAJ8lND4nIdhF5WUSmlnlsRFQFIy4BEakF8AaAh1X1CIDnAVwMoBFDewrPFrnfKhFpFpHmMoyXiMpsRCUgImMxVACvqup6AFDVDlUdVNUTAF4AsKjQfVV1rao2qWpTuQZNROXjloAMLdH6EoA2VX1u2O0Nw37sdgCt5R8eEVXaSI4OLAbwcwAtInLyuuKPAVghIo0AFMBuAA9UZIREVFGj6roDRFQ6ridARAWxBIiCYwkQBccSIAqOJUAUHEuAKDiWAFFwLAGi4FgCRMGxBIiCYwkQBccSIAqOJUAUHEuAKDiWAFFwI1lUpJwOAPh62Pf1yW15xfGlk+fx5XlsQPnHd0GxoKqLipz24CLNeV57kONLJ8/jy/PYgOqOj28HiIJjCRAFl3UJrM348T0cXzp5Hl+exwZUcXyZfiZARNnLek+AiDLGEiAKjiVAFBxLgCg4lgBRcP8HHzj0EsIeTz8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
      "    0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
      "   54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
      "  144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
      "  107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
      "  216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
      "  223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
      "  235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
      "  180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
      "  169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
      "  198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
      "  232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
      "  222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
      "  211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
      "  224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
      "  255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
      "  188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
      "  168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
      "  239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
      "  199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
      "  195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
      "  210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
      "  182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The shape of the digits dataset:\") \n",
    "print(X_train.shape)\n",
    "plt.gray()\n",
    "plt.matshow(X_train[0])\n",
    "plt.show()\n",
    "print(y_train[0:1])\n",
    "print(X_train[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset\n",
    "\n",
    "X_scale = StandardScaler()\n",
    "X_train = [X_scale.fit_transform(X_train[i]).flatten() for i in range(len(X_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [X_scale.fit_transform(X_test[i]).flatten() for i in range(len(X_test))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34062577, -0.48590849, -0.55368775, -0.57830472, -0.61724799,\n",
       "       -0.65473466, -0.66524031, -0.69031625, -0.71089238, -0.73827078,\n",
       "       -0.77682253, -0.7644687 , -0.90444624, -1.24217044, -1.49218887,\n",
       "       -1.6432188 , -1.7458434 , -1.58582058, -1.56547495, -1.51925798,\n",
       "       -1.44902444, -1.39545527, -1.35520903, -1.32176483, -1.33646764,\n",
       "       -1.31334192, -1.22103858, -0.55220401, -0.34062577, -0.48590849,\n",
       "       -0.55368775, -0.57830472, -0.61724799, -0.65473466, -0.66524031,\n",
       "       -0.69031625, -0.71089238, -0.73827078, -0.77682253, -0.7644687 ,\n",
       "       -0.90444624, -1.24217044, -1.49218887, -1.6432188 , -1.7458434 ,\n",
       "       -1.58582058, -1.56547495, -1.51925798, -1.44902444, -1.39545527,\n",
       "       -1.35520903, -1.32176483, -1.33646764, -1.31334192, -1.22103858,\n",
       "       -0.55220401, -0.34062577, -0.48590849, -0.55368775, -0.57830472,\n",
       "       -0.61724799, -0.65473466, -0.66524031, -0.69031625, -0.71089238,\n",
       "       -0.73827078, -0.77682253, -0.7644687 , -0.90444624, -1.24217044,\n",
       "       -1.49218887, -1.6432188 , -1.7458434 , -1.58582058, -1.56547495,\n",
       "       -1.51925798, -1.44902444, -1.39545527, -1.35520903, -1.32176483,\n",
       "       -1.33646764, -1.31334192, -1.22103858, -0.55220401, -0.34062577,\n",
       "       -0.48590849, -0.55368775, -0.57830472, -0.61724799, -0.65473466,\n",
       "       -0.66524031, -0.69031625, -0.71089238, -0.73827078, -0.77682253,\n",
       "       -0.7644687 , -0.89474708, -1.24217044, -1.49218887, -1.50501833,\n",
       "       -0.92869114, -1.58582058, -1.56547495, -1.50882914, -1.40684909,\n",
       "       -1.39545527, -1.35520903, -1.32176483, -1.33646764, -1.30350416,\n",
       "       -1.21065938, -0.55220401, -0.34062577, -0.48590849, -0.55368775,\n",
       "       -0.57830472, -0.61724799, -0.65473466, -0.66524031, -0.69031625,\n",
       "       -0.71089238, -0.73827078, -0.77682253, -0.7644687 , -0.87534877,\n",
       "       -1.24217044, -1.11784184, -0.19742925, -0.32422235, -0.92052707,\n",
       "       -0.98117524, -1.51925798, -1.44902444, -1.39545527, -1.34501949,\n",
       "       -1.29113635, -1.2957372 , -1.31334192, -1.22103858, -0.45849666,\n",
       "       -0.34062577, -0.48590849, -0.55368775, -0.57830472, -0.61724799,\n",
       "       -0.65473466, -0.66524031, -0.69031625, -0.71089238, -0.73827078,\n",
       "       -0.77682253, -0.7644687 , -0.8462513 , -1.24217044, -0.43153894,\n",
       "        0.52546553,  0.22427711, -0.14792816, -0.00734239, -0.23651111,\n",
       "       -1.20651619, -1.39545527, -1.35520903, -1.32176483, -1.33646764,\n",
       "       -1.19528872, -1.11724659, -0.55220401, -0.34062577, -0.48590849,\n",
       "       -0.55368775, -0.57830472, -0.61724799, -0.65473466, -0.66524031,\n",
       "       -0.69031625, -0.71089238, -0.73827078, -0.77682253, -0.7644687 ,\n",
       "       -0.90444624, -1.24217044,  0.11958308,  0.86565131,  0.57128697,\n",
       "        0.32421561, -0.40769589,  0.10764049,  0.2485333 , -0.26636474,\n",
       "       -0.70307837, -1.08694647, -0.55240663, -0.03443219, -0.47373628,\n",
       "       -0.08366727, -0.34062577, -0.48590849, -0.55368775, -0.57830472,\n",
       "       -0.61724799, -0.65473466, -0.66524031, -0.69031625, -0.71089238,\n",
       "       -0.73827078, -0.77682253, -0.75460914, -0.90444624, -0.56347324,\n",
       "        0.66030658,  0.72745084,  0.69441951,  0.73197615,  0.77172389,\n",
       "        0.18064234, -0.10995716, -0.14206119, -0.11208496,  0.16882127,\n",
       "        0.09928045, -0.44761841,  0.56418358,  1.50935763, -0.34062577,\n",
       "       -0.48590849, -0.55368775, -0.57830472, -0.61724799, -0.65473466,\n",
       "       -0.66524031, -0.69031625, -0.71089238, -0.72787783, -0.76645007,\n",
       "       -0.75460914, -0.90444624,  0.72506781,  0.92026979,  0.82312809,\n",
       "        0.8623275 ,  0.87147317,  0.84746644,  0.80637252,  0.81790049,\n",
       "        0.81093265,  0.31587579, -0.02515911, -0.08400654,  0.61486045,\n",
       "        1.1557979 , -0.55220401, -0.34062577, -0.48590849, -0.55368775,\n",
       "       -0.57830472, -0.61724799, -0.65473466, -0.66524031, -0.69031625,\n",
       "       -0.71089238, -0.73827078, -0.77682253, -0.7644687 , -0.90444624,\n",
       "        0.55785256,  0.84748009,  0.6530352 ,  0.75038884,  0.86074263,\n",
       "        0.97731082,  0.84808787,  0.91279502,  0.90416031,  0.92724828,\n",
       "        0.93453332,  0.93425452,  1.09691105,  0.57456278, -0.55220401,\n",
       "       -0.34062577, -0.48590849, -0.55368775, -0.57830472, -0.61724799,\n",
       "       -0.65473466, -0.66524031, -0.69031625, -0.71089238, -0.73827078,\n",
       "       -0.77682253, -0.7644687 , -0.90444624,  0.65621447,  0.87867568,\n",
       "        0.67429681,  0.63845017,  0.53882642,  0.38219075,  0.69165532,\n",
       "        0.7651813 ,  0.79021539,  0.81516333,  0.9549523 ,  0.90370669,\n",
       "        1.07723551,  0.87555954, -0.55220401, -0.34062577, -0.48590849,\n",
       "       -0.55368775, -0.57830472, -0.61724799, -0.65473466, -0.66524031,\n",
       "       -0.69031625, -0.71089238, -0.72787783, -0.74570517, -0.7644687 ,\n",
       "       -0.78805637,  0.91195545,  0.79548745,  0.61051198,  0.69441951,\n",
       "        0.47444318,  0.26316673,  0.84808787,  0.74409363,  0.86272579,\n",
       "        0.92724828,  0.84264787,  0.96480235,  0.62469822,  0.94821393,\n",
       "        1.07205668, -0.34062577, -0.48590849, -0.55368775, -0.57830472,\n",
       "       -0.61724799, -0.65473466, -0.66524031, -0.69031625, -0.71089238,\n",
       "       -0.73827078, -0.71458782, -0.7644687 ,  0.05577014,  1.15786023,\n",
       "        0.81628451,  0.69555842,  0.69441951,  0.59247912,  0.57695732,\n",
       "        0.78551485,  0.81790049,  0.81093265,  0.9068692 ,  0.92432382,\n",
       "        1.15827196, -0.14264762,  0.51228759,  1.19699981, -0.34062577,\n",
       "       -0.48590849, -0.55368775, -0.57830472, -0.61724799, -0.65473466,\n",
       "       -0.66524031, -0.69031625, -0.71089238, -0.69669897, -0.77682253,\n",
       "       -0.7644687 , -0.37099269,  1.0791707 ,  0.87867568,  0.80186648,\n",
       "        0.80635817,  0.98950912,  0.94484973,  0.70208416,  0.849532  ,\n",
       "        0.91451894,  1.0291437 ,  0.89369534,  0.87315886,  0.74275143,\n",
       "       -0.26615231, -0.55220401, -0.34062577, -0.48590849, -0.54178048,\n",
       "       -0.53288387, -0.55051848, -0.57816664, -0.64363157, -0.69031625,\n",
       "       -0.71089238, -0.73827078, -0.77682253, -0.7644687 ,  1.39425358,\n",
       "        0.98080879,  0.76429186,  0.72745084,  0.73919497,  0.76416777,\n",
       "        0.83664608,  0.78551485,  0.82844432,  0.91451894,  0.97819599,\n",
       "        0.87327635,  0.88334147,  1.19528872, -0.42184029, -0.55220401,\n",
       "       -0.34062577, -0.44604881, -0.55368775, -0.57830472, -0.61724799,\n",
       "       -0.65473466, -0.66524031, -0.69031625, -0.71089238, -0.09390775,\n",
       "        0.72718294,  1.24688332,  1.30696118,  0.79392115,  0.72269775,\n",
       "        0.70618923,  0.69441951,  0.64613183,  0.71762206,  0.75422834,\n",
       "        0.91279502,  0.91451894,  0.87630058,  0.87327635,  0.94443713,\n",
       "        1.08707328,  0.429254  , -0.55220401, -0.34062577, -0.48590849,\n",
       "       -0.55368775, -0.57830472, -0.41705945, -0.17344999,  0.22071778,\n",
       "        0.44418045,  1.23231138,  1.63132231,  1.5051168 ,  1.42435555,\n",
       "        1.20027047,  0.98080879,  0.58751687,  0.53609634,  0.61606244,\n",
       "        0.88220372,  0.85828681,  0.92108972,  0.40669085,  0.55196693,\n",
       "        1.19217637,  1.21018965,  1.03608063,  1.02804668,  1.01048912,\n",
       "       -0.55220401, -0.34062577,  0.27142544,  1.6729705 ,  1.78357935,\n",
       "        1.87398715,  1.76262695,  1.75493789,  1.51506051,  1.38653391,\n",
       "        1.48582098,  1.38064738,  1.29618116,  1.03538483,  0.32178397,\n",
       "        1.05545067,  0.40852667,  0.56009311,  0.80708993,  1.19371812,\n",
       "        1.14009528,  0.88116351,  1.02846385,  0.89667966,  0.83243838,\n",
       "        0.90370669,  0.96902007,  1.33224427, -0.55220401, -0.2140215 ,\n",
       "        2.19797667,  2.1611683 ,  1.96526274,  1.84062239,  1.65324407,\n",
       "        1.61448112,  1.57867714,  1.39681541,  1.39228441,  1.34953003,\n",
       "        1.40463642,  1.42335105, -0.45527514,  0.06759044,  1.06763661,\n",
       "        0.81755204,  0.78562885,  0.46875367,  0.08678282,  0.5648484 ,\n",
       "        0.77985676,  0.72345745,  0.81201939,  0.92407191,  0.929669  ,\n",
       "        1.1142811 , -0.55220401,  3.79511366,  2.60986004,  1.8039504 ,\n",
       "        1.80628977,  1.85174398,  1.85013325,  1.80895972,  1.7907326 ,\n",
       "        1.84920147,  1.5481787 ,  1.23543306,  1.35533857,  1.20027047,\n",
       "        1.12835166, -0.81628451, -0.86716999, -0.55929355, -0.33034735,\n",
       "        0.25234637,  0.76465718,  0.88116351,  0.83164991,  0.85592149,\n",
       "        0.9549523 ,  0.93425452,  0.89031793,  1.1557979 ,  0.35363368,\n",
       "        2.82448094,  2.22454979,  1.97065208,  1.7381585 ,  1.529218  ,\n",
       "        1.58761435,  1.61448112,  1.69530764,  1.50991192,  1.18442537,\n",
       "        1.26655041,  1.26660245,  1.01598652,  0.8529383 ,  1.00345802,\n",
       "        0.42978828,  0.79516431,  1.04316182,  1.02059228,  0.80637252,\n",
       "        0.849532  ,  0.80057402,  0.77440516,  0.94474281,  0.90370669,\n",
       "        0.86080463,  1.1661771 ,  1.54059341,  1.68504252,  2.21126323,\n",
       "        1.62534145,  1.62460638,  1.75164971,  1.50010804,  1.33356758,\n",
       "        1.32421059,  1.28371889,  1.25717604,  1.31841267,  1.345479  ,\n",
       "        1.21966878,  0.93162783,  0.79548745,  0.86565131,  0.77277657,\n",
       "        0.73197615,  0.58777768,  0.6290823 ,  0.51212922,  0.47945653,\n",
       "        0.44833983,  0.43426812,  0.50658488,  0.70340036,  0.91707633,\n",
       "        3.03991097, -0.34062577,  1.13505186,  2.05400293,  1.61325117,\n",
       "        1.3735158 ,  1.21571256,  1.31195884,  1.38782723,  1.38653391,\n",
       "        1.44424917,  1.43250964,  1.27646202,  1.14207554,  0.82342973,\n",
       "        0.58751687,  0.44041909,  0.42576671,  0.46371264,  0.54449622,\n",
       "        0.47264976,  0.63865526,  0.59340145,  0.43815029,  0.27091621,\n",
       "        0.36402833,  0.42794287,  0.95859313,  2.32148797, -0.34062577,\n",
       "       -0.48590849,  0.32744974,  1.56783032,  1.74052812,  1.43447832,\n",
       "        1.22552391,  1.13336068,  1.08837036,  1.14285356,  1.142081  ,\n",
       "        1.08913022,  0.92869412,  0.60703352,  0.51472717,  0.4616807 ,\n",
       "        0.53770538,  0.65686237,  0.7068017 ,  0.67079765,  0.77572514,\n",
       "        0.55196693,  0.56042479,  0.65887698,  0.61859359,  0.8116158 ,\n",
       "        0.54342518, -0.55220401, -0.25622292, -0.48590849, -0.55368775,\n",
       "       -0.57830472,  0.11677665,  1.53292291,  1.73332915,  1.82254092,\n",
       "        1.74638646,  1.77682364,  1.77480054,  1.6314065 ,  1.46214767,\n",
       "        0.93162783,  0.79548745,  0.40852667,  0.39218511,  0.33494615,\n",
       "        0.40383148,  0.37879023,  0.45941003,  0.42766339,  0.33625487,\n",
       "        0.39343014, -0.32838919, -0.74275143, -1.22103858, -0.55220401,\n",
       "       -0.34062577, -0.48590849, -0.55368775, -0.57830472, -0.61724799,\n",
       "       -0.65473466, -0.66524031, -0.26620533, -0.0837208 , -0.28098089,\n",
       "       -0.03000602, -0.36022639, -0.5649758 , -1.24217044, -1.49218887,\n",
       "       -1.6432188 , -1.7458434 , -1.58582058, -1.56547495, -1.51925798,\n",
       "       -1.44902444, -1.39545527, -1.35520903, -1.32176483, -1.33646764,\n",
       "       -1.31334192, -1.22103858, -0.55220401, -0.34062577, -0.48590849,\n",
       "       -0.55368775, -0.57830472, -0.61724799, -0.65473466, -0.66524031,\n",
       "       -0.69031625, -0.71089238, -0.73827078, -0.77682253, -0.7644687 ,\n",
       "       -0.90444624, -1.24217044, -1.49218887, -1.6432188 , -1.7458434 ,\n",
       "       -1.58582058, -1.56547495, -1.51925798, -1.44902444, -1.39545527,\n",
       "       -1.35520903, -1.32176483, -1.33646764, -1.31334192, -1.22103858,\n",
       "       -0.55220401, -0.34062577, -0.48590849, -0.55368775, -0.57830472,\n",
       "       -0.61724799, -0.65473466, -0.66524031, -0.69031625, -0.71089238,\n",
       "       -0.73827078, -0.77682253, -0.7644687 , -0.90444624, -1.24217044,\n",
       "       -1.49218887, -1.6432188 , -1.7458434 , -1.58582058, -1.56547495,\n",
       "       -1.51925798, -1.44902444, -1.39545527, -1.35520903, -1.32176483,\n",
       "       -1.33646764, -1.31334192, -1.22103858, -0.55220401])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] # Looking at the new features after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 784)\n",
      "(1078,)\n",
      "(719, 784)\n",
      "(719,)\n"
     ]
    }
   ],
   "source": [
    "# Downsample the data\n",
    "\n",
    "# Add y_train back as an additional column to X_train\n",
    "y_train = y_train.reshape((-1,1))\n",
    "X_train = np.append(X_train, y_train, axis=1)\n",
    "\n",
    "# Add y_test back as an additional column to X_test\n",
    "y_test = y_test.reshape((-1,1))\n",
    "X_test = np.append(X_test, y_test, axis=1)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(X_train)\n",
    "np.random.shuffle(X_test)\n",
    "\n",
    "# Slice out only the first 1078 from X_train and 719 from X_test\n",
    "X_train = X_train[0:1078]\n",
    "X_test = X_test[0:719]\n",
    "\n",
    "# Remove the last columns of X_train and X_test and place them back into y_train and y_test\n",
    "y_train = X_train[:,-1]\n",
    "y_test = X_test[:,-1]\n",
    "X_train = X_train[:,0:X_train.shape[1]-1]\n",
    "X_test = X_test[:,0:X_test.shape[1]-1]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 8. 8. ... 1. 6. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the digits dataset:\n",
      "(1078, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Show that newly downsampled X_train still matches with y_train\n",
    "# Here we display a t-shirt with label of 0 which is correct\n",
    "\n",
    "X_train_test = np.zeros((28,28))\n",
    "for i in range(28):\n",
    "    X_train_test[i] = X_train[0][28*i:28*i+28]\n",
    "print(X_train_test.shape)\n",
    "\n",
    "print(\"The shape of the digits dataset:\") \n",
    "print(X_train.shape)\n",
    "plt.gray()\n",
    "plt.matshow(X_train_test)\n",
    "plt.show()\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "The shape of the digits dataset:\n",
      "(1078, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# Again we show that, as expected, an image of a sandal has a label of 5\n",
    "\n",
    "X_train_test2 = np.zeros((28,28))\n",
    "for i in range(28):\n",
    "    X_train_test2[i] = X_train[435][28*i:28*i+28]\n",
    "print(X_train_test2.shape)\n",
    "\n",
    "print(\"The shape of the digits dataset:\") \n",
    "print(X_train.shape)\n",
    "plt.gray()\n",
    "plt.matshow(X_train_test2)\n",
    "plt.show()\n",
    "print(y_train[435])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert digits to vectors (one hot encoding)\n",
    "y_v_train = convert_y_to_vect(y_train.astype(np.int))\n",
    "y_v_test = convert_y_to_vect(y_test.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(30, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "# Compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.1384 - accuracy: 0.1160\n",
      "Epoch 2/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0958 - accuracy: 0.1753\n",
      "Epoch 3/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0910 - accuracy: 0.2449\n",
      "Epoch 4/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0896 - accuracy: 0.2857\n",
      "Epoch 5/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0890 - accuracy: 0.3071\n",
      "Epoch 6/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0887 - accuracy: 0.3256\n",
      "Epoch 7/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0884 - accuracy: 0.3182\n",
      "Epoch 8/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0881 - accuracy: 0.3321\n",
      "Epoch 9/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0879 - accuracy: 0.3247\n",
      "Epoch 10/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0876 - accuracy: 0.3349\n",
      "Epoch 11/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0874 - accuracy: 0.3349\n",
      "Epoch 12/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0871 - accuracy: 0.3302\n",
      "Epoch 13/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0869 - accuracy: 0.3265\n",
      "Epoch 14/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0866 - accuracy: 0.3275\n",
      "Epoch 15/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0863 - accuracy: 0.3284\n",
      "Epoch 16/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0860 - accuracy: 0.3414\n",
      "Epoch 17/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0856 - accuracy: 0.3377\n",
      "Epoch 18/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0853 - accuracy: 0.3460\n",
      "Epoch 19/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0849 - accuracy: 0.3525\n",
      "Epoch 20/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0846 - accuracy: 0.3516\n",
      "Epoch 21/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0842 - accuracy: 0.3525\n",
      "Epoch 22/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0838 - accuracy: 0.3609\n",
      "Epoch 23/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0834 - accuracy: 0.3729\n",
      "Epoch 24/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0830 - accuracy: 0.3673\n",
      "Epoch 25/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0826 - accuracy: 0.3803\n",
      "Epoch 26/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0822 - accuracy: 0.3840\n",
      "Epoch 27/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0817 - accuracy: 0.3952\n",
      "Epoch 28/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0813 - accuracy: 0.3989\n",
      "Epoch 29/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0808 - accuracy: 0.4239\n",
      "Epoch 30/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0804 - accuracy: 0.4286\n",
      "Epoch 31/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0799 - accuracy: 0.4406\n",
      "Epoch 32/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0794 - accuracy: 0.4453\n",
      "Epoch 33/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0789 - accuracy: 0.4527\n",
      "Epoch 34/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0785 - accuracy: 0.4592\n",
      "Epoch 35/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0780 - accuracy: 0.4675\n",
      "Epoch 36/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0775 - accuracy: 0.4731\n",
      "Epoch 37/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0770 - accuracy: 0.4852\n",
      "Epoch 38/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0765 - accuracy: 0.4926\n",
      "Epoch 39/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0760 - accuracy: 0.5028\n",
      "Epoch 40/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0755 - accuracy: 0.5065\n",
      "Epoch 41/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0750 - accuracy: 0.5195\n",
      "Epoch 42/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0745 - accuracy: 0.5325\n",
      "Epoch 43/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0740 - accuracy: 0.5325\n",
      "Epoch 44/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0735 - accuracy: 0.5408\n",
      "Epoch 45/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0729 - accuracy: 0.5417\n",
      "Epoch 46/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0724 - accuracy: 0.5547\n",
      "Epoch 47/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0719 - accuracy: 0.5649\n",
      "Epoch 48/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0714 - accuracy: 0.5724\n",
      "Epoch 49/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0709 - accuracy: 0.5696\n",
      "Epoch 50/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0704 - accuracy: 0.5742\n",
      "Epoch 51/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0698 - accuracy: 0.5863\n",
      "Epoch 52/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0693 - accuracy: 0.5909\n",
      "Epoch 53/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0688 - accuracy: 0.5965\n",
      "Epoch 54/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0682 - accuracy: 0.6030\n",
      "Epoch 55/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0677 - accuracy: 0.6122\n",
      "Epoch 56/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0672 - accuracy: 0.6160\n",
      "Epoch 57/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0666 - accuracy: 0.6197\n",
      "Epoch 58/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0661 - accuracy: 0.6187\n",
      "Epoch 59/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0656 - accuracy: 0.6224\n",
      "Epoch 60/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0651 - accuracy: 0.6271\n",
      "Epoch 61/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0645 - accuracy: 0.6308\n",
      "Epoch 62/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0640 - accuracy: 0.6299\n",
      "Epoch 63/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0635 - accuracy: 0.6317\n",
      "Epoch 64/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0630 - accuracy: 0.6354\n",
      "Epoch 65/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0625 - accuracy: 0.6391\n",
      "Epoch 66/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0619 - accuracy: 0.6466\n",
      "Epoch 67/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0614 - accuracy: 0.6456\n",
      "Epoch 68/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0609 - accuracy: 0.6503\n",
      "Epoch 69/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0605 - accuracy: 0.6466\n",
      "Epoch 70/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0600 - accuracy: 0.6521\n",
      "Epoch 71/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0595 - accuracy: 0.6512\n",
      "Epoch 72/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0590 - accuracy: 0.6605\n",
      "Epoch 73/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0585 - accuracy: 0.6586\n",
      "Epoch 74/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0581 - accuracy: 0.6633\n",
      "Epoch 75/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0576 - accuracy: 0.6651\n",
      "Epoch 76/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0572 - accuracy: 0.6642\n",
      "Epoch 77/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0567 - accuracy: 0.6651\n",
      "Epoch 78/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0563 - accuracy: 0.6633\n",
      "Epoch 79/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0559 - accuracy: 0.6642\n",
      "Epoch 80/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0555 - accuracy: 0.6827\n",
      "Epoch 81/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0550 - accuracy: 0.6772\n",
      "Epoch 82/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0546 - accuracy: 0.6781\n",
      "Epoch 83/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0542 - accuracy: 0.6827\n",
      "Epoch 84/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0538 - accuracy: 0.6865\n",
      "Epoch 85/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0534 - accuracy: 0.6911\n",
      "Epoch 86/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0530 - accuracy: 0.6948\n",
      "Epoch 87/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0527 - accuracy: 0.6948\n",
      "Epoch 88/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0523 - accuracy: 0.7032\n",
      "Epoch 89/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0519 - accuracy: 0.6957\n",
      "Epoch 90/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0515 - accuracy: 0.7013\n",
      "Epoch 91/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0512 - accuracy: 0.7087\n",
      "Epoch 92/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0508 - accuracy: 0.7115\n",
      "Epoch 93/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0505 - accuracy: 0.7124\n",
      "Epoch 94/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0501 - accuracy: 0.7134\n",
      "Epoch 95/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0498 - accuracy: 0.7143\n",
      "Epoch 96/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0494 - accuracy: 0.7245\n",
      "Epoch 97/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0491 - accuracy: 0.7273\n",
      "Epoch 98/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0488 - accuracy: 0.7282\n",
      "Epoch 99/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0484 - accuracy: 0.7347\n",
      "Epoch 100/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0481 - accuracy: 0.7291\n",
      "Epoch 101/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0478 - accuracy: 0.7449\n",
      "Epoch 102/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0475 - accuracy: 0.7375\n",
      "Epoch 103/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0472 - accuracy: 0.7486\n",
      "Epoch 104/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0469 - accuracy: 0.7477\n",
      "Epoch 105/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0466 - accuracy: 0.7468\n",
      "Epoch 106/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0463 - accuracy: 0.7495\n",
      "Epoch 107/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0460 - accuracy: 0.7495\n",
      "Epoch 108/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0457 - accuracy: 0.7532\n",
      "Epoch 109/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0454 - accuracy: 0.7505\n",
      "Epoch 110/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0451 - accuracy: 0.7542\n",
      "Epoch 111/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0448 - accuracy: 0.7588\n",
      "Epoch 112/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0445 - accuracy: 0.7607\n",
      "Epoch 113/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0442 - accuracy: 0.7625\n",
      "Epoch 114/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0440 - accuracy: 0.7597\n",
      "Epoch 115/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0437 - accuracy: 0.7672\n",
      "Epoch 116/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0434 - accuracy: 0.7653\n",
      "Epoch 117/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0432 - accuracy: 0.7672\n",
      "Epoch 118/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0429 - accuracy: 0.7709\n",
      "Epoch 119/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0426 - accuracy: 0.7709\n",
      "Epoch 120/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0424 - accuracy: 0.7774\n",
      "Epoch 121/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0421 - accuracy: 0.7774\n",
      "Epoch 122/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0419 - accuracy: 0.7737\n",
      "Epoch 123/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0416 - accuracy: 0.7737\n",
      "Epoch 124/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0414 - accuracy: 0.7764\n",
      "Epoch 125/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0411 - accuracy: 0.7811\n",
      "Epoch 126/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0409 - accuracy: 0.7829\n",
      "Epoch 127/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0406 - accuracy: 0.7820\n",
      "Epoch 128/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0404 - accuracy: 0.7811\n",
      "Epoch 129/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0401 - accuracy: 0.7829\n",
      "Epoch 130/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0399 - accuracy: 0.7848\n",
      "Epoch 131/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0397 - accuracy: 0.7839\n",
      "Epoch 132/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0394 - accuracy: 0.7839\n",
      "Epoch 133/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0392 - accuracy: 0.7839\n",
      "Epoch 134/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0390 - accuracy: 0.7857\n",
      "Epoch 135/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0388 - accuracy: 0.7866\n",
      "Epoch 136/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0385 - accuracy: 0.7876\n",
      "Epoch 137/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0383 - accuracy: 0.7913\n",
      "Epoch 138/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0381 - accuracy: 0.7950\n",
      "Epoch 139/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0379 - accuracy: 0.7904\n",
      "Epoch 140/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0377 - accuracy: 0.7978\n",
      "Epoch 141/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0375 - accuracy: 0.7931\n",
      "Epoch 142/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0372 - accuracy: 0.7996\n",
      "Epoch 143/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0370 - accuracy: 0.7968\n",
      "Epoch 144/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0368 - accuracy: 0.7968\n",
      "Epoch 145/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0366 - accuracy: 0.7987\n",
      "Epoch 146/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0364 - accuracy: 0.7987\n",
      "Epoch 147/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0362 - accuracy: 0.7987\n",
      "Epoch 148/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0360 - accuracy: 0.7996\n",
      "Epoch 149/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0358 - accuracy: 0.7996\n",
      "Epoch 150/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0356 - accuracy: 0.8024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x157bf3690>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "model.fit(X_train, y_v_train, epochs = 150, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/719 [==============================] - 0s 214us/step\n",
      "Accuracy: 73.99\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the keras model\n",
    "accuracy = model.evaluate(X_test, y_v_test)\n",
    "print('Accuracy: %.2f' % (accuracy[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras implementation plus extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keras model with softmax outer layer\n",
    "model_plus_ext = Sequential()\n",
    "model_plus_ext.add(Dense(64, activation='sigmoid'))\n",
    "model_plus_ext.add(Dense(30, activation='sigmoid'))\n",
    "model_plus_ext.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the keras model\n",
    "model_plus_ext.compile(loss='mean_squared_error', optimizer='sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0929 - accuracy: 0.1104\n",
      "Epoch 2/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0916 - accuracy: 0.1187\n",
      "Epoch 3/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0907 - accuracy: 0.1865\n",
      "Epoch 4/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0900 - accuracy: 0.2486\n",
      "Epoch 5/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0895 - accuracy: 0.2774\n",
      "Epoch 6/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0889 - accuracy: 0.2885\n",
      "Epoch 7/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0884 - accuracy: 0.3098\n",
      "Epoch 8/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0879 - accuracy: 0.3024\n",
      "Epoch 9/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0875 - accuracy: 0.3117\n",
      "Epoch 10/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0870 - accuracy: 0.3117\n",
      "Epoch 11/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0866 - accuracy: 0.3089\n",
      "Epoch 12/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0861 - accuracy: 0.3043\n",
      "Epoch 13/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0856 - accuracy: 0.3043\n",
      "Epoch 14/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0851 - accuracy: 0.3043\n",
      "Epoch 15/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0846 - accuracy: 0.3071\n",
      "Epoch 16/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0841 - accuracy: 0.3006\n",
      "Epoch 17/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0836 - accuracy: 0.3061\n",
      "Epoch 18/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0830 - accuracy: 0.3052\n",
      "Epoch 19/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0825 - accuracy: 0.3033\n",
      "Epoch 20/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0819 - accuracy: 0.3061\n",
      "Epoch 21/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0813 - accuracy: 0.3117\n",
      "Epoch 22/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0808 - accuracy: 0.3163\n",
      "Epoch 23/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0802 - accuracy: 0.3293\n",
      "Epoch 24/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0796 - accuracy: 0.3423\n",
      "Epoch 25/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0790 - accuracy: 0.3571\n",
      "Epoch 26/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0785 - accuracy: 0.3683\n",
      "Epoch 27/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0779 - accuracy: 0.3905\n",
      "Epoch 28/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0773 - accuracy: 0.4035\n",
      "Epoch 29/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0768 - accuracy: 0.4147\n",
      "Epoch 30/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0762 - accuracy: 0.4276\n",
      "Epoch 31/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0757 - accuracy: 0.4490\n",
      "Epoch 32/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0751 - accuracy: 0.4527\n",
      "Epoch 33/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0746 - accuracy: 0.4573\n",
      "Epoch 34/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0740 - accuracy: 0.4629\n",
      "Epoch 35/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0735 - accuracy: 0.4759\n",
      "Epoch 36/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0729 - accuracy: 0.4796\n",
      "Epoch 37/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0724 - accuracy: 0.4889\n",
      "Epoch 38/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0718 - accuracy: 0.4852\n",
      "Epoch 39/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0713 - accuracy: 0.4954\n",
      "Epoch 40/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0707 - accuracy: 0.4944\n",
      "Epoch 41/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0702 - accuracy: 0.4991\n",
      "Epoch 42/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0696 - accuracy: 0.5037\n",
      "Epoch 43/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0690 - accuracy: 0.5176\n",
      "Epoch 44/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0684 - accuracy: 0.5250\n",
      "Epoch 45/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0679 - accuracy: 0.5204\n",
      "Epoch 46/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0673 - accuracy: 0.5399\n",
      "Epoch 47/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0667 - accuracy: 0.5399\n",
      "Epoch 48/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0661 - accuracy: 0.5455\n",
      "Epoch 49/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0655 - accuracy: 0.5501\n",
      "Epoch 50/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0649 - accuracy: 0.5529\n",
      "Epoch 51/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0644 - accuracy: 0.5566\n",
      "Epoch 52/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0638 - accuracy: 0.5640\n",
      "Epoch 53/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0632 - accuracy: 0.5659\n",
      "Epoch 54/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0627 - accuracy: 0.5686\n",
      "Epoch 55/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0621 - accuracy: 0.5714\n",
      "Epoch 56/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0616 - accuracy: 0.5770\n",
      "Epoch 57/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0610 - accuracy: 0.5761\n",
      "Epoch 58/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0605 - accuracy: 0.5798\n",
      "Epoch 59/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0600 - accuracy: 0.5844\n",
      "Epoch 60/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0595 - accuracy: 0.5955\n",
      "Epoch 61/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0589 - accuracy: 0.5937\n",
      "Epoch 62/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0584 - accuracy: 0.5965\n",
      "Epoch 63/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0580 - accuracy: 0.6039\n",
      "Epoch 64/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0575 - accuracy: 0.6058\n",
      "Epoch 65/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0570 - accuracy: 0.6085\n",
      "Epoch 66/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0565 - accuracy: 0.6113\n",
      "Epoch 67/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0561 - accuracy: 0.6150\n",
      "Epoch 68/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0556 - accuracy: 0.6197\n",
      "Epoch 69/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0551 - accuracy: 0.6224\n",
      "Epoch 70/150\n",
      "1078/1078 [==============================] - 280s 260ms/step - loss: 0.0547 - accuracy: 0.6280\n",
      "Epoch 71/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0543 - accuracy: 0.6289\n",
      "Epoch 72/150\n",
      "1078/1078 [==============================] - 9s 8ms/step - loss: 0.0538 - accuracy: 0.6299\n",
      "Epoch 73/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0534 - accuracy: 0.6299\n",
      "Epoch 74/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0529 - accuracy: 0.6364\n",
      "Epoch 75/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0525 - accuracy: 0.6419\n",
      "Epoch 76/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0521 - accuracy: 0.6401\n",
      "Epoch 77/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0517 - accuracy: 0.6466\n",
      "Epoch 78/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0512 - accuracy: 0.6521\n",
      "Epoch 79/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0508 - accuracy: 0.6577\n",
      "Epoch 80/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0504 - accuracy: 0.6531\n",
      "Epoch 81/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0500 - accuracy: 0.6670\n",
      "Epoch 82/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0495 - accuracy: 0.6763\n",
      "Epoch 83/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0491 - accuracy: 0.6883\n",
      "Epoch 84/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0487 - accuracy: 0.6985\n",
      "Epoch 85/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0483 - accuracy: 0.7069\n",
      "Epoch 86/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0479 - accuracy: 0.7152\n",
      "Epoch 87/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0474 - accuracy: 0.7254\n",
      "Epoch 88/150\n",
      "1078/1078 [==============================] - 525s 487ms/step - loss: 0.0470 - accuracy: 0.7328\n",
      "Epoch 89/150\n",
      "1078/1078 [==============================] - 16s 14ms/step - loss: 0.0466 - accuracy: 0.7384\n",
      "Epoch 90/150\n",
      "1078/1078 [==============================] - 7s 6ms/step - loss: 0.0462 - accuracy: 0.7449\n",
      "Epoch 91/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0458 - accuracy: 0.7477\n",
      "Epoch 92/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0453 - accuracy: 0.7551\n",
      "Epoch 93/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0449 - accuracy: 0.7532\n",
      "Epoch 94/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0445 - accuracy: 0.7635\n",
      "Epoch 95/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0441 - accuracy: 0.7672\n",
      "Epoch 96/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0436 - accuracy: 0.7737\n",
      "Epoch 97/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0432 - accuracy: 0.7727\n",
      "Epoch 98/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0428 - accuracy: 0.7820\n",
      "Epoch 99/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0424 - accuracy: 0.7801\n",
      "Epoch 100/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0419 - accuracy: 0.7811\n",
      "Epoch 101/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0415 - accuracy: 0.7801\n",
      "Epoch 102/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0411 - accuracy: 0.7848\n",
      "Epoch 103/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0407 - accuracy: 0.7857\n",
      "Epoch 104/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0402 - accuracy: 0.7894\n",
      "Epoch 105/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0399 - accuracy: 0.7922\n",
      "Epoch 106/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0394 - accuracy: 0.7894\n",
      "Epoch 107/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0390 - accuracy: 0.7950\n",
      "Epoch 108/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0386 - accuracy: 0.7968\n",
      "Epoch 109/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0382 - accuracy: 0.7987\n",
      "Epoch 110/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0378 - accuracy: 0.8033\n",
      "Epoch 111/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0374 - accuracy: 0.8033\n",
      "Epoch 112/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0370 - accuracy: 0.8033\n",
      "Epoch 113/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0366 - accuracy: 0.8061\n",
      "Epoch 114/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0362 - accuracy: 0.8098\n",
      "Epoch 115/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0359 - accuracy: 0.8080\n",
      "Epoch 116/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0355 - accuracy: 0.8061\n",
      "Epoch 117/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0351 - accuracy: 0.8154\n",
      "Epoch 118/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0348 - accuracy: 0.8117\n",
      "Epoch 119/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0344 - accuracy: 0.8154\n",
      "Epoch 120/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0341 - accuracy: 0.8173\n",
      "Epoch 121/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0337 - accuracy: 0.8173\n",
      "Epoch 122/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0334 - accuracy: 0.8173\n",
      "Epoch 123/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0330 - accuracy: 0.8191\n",
      "Epoch 124/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0327 - accuracy: 0.8200\n",
      "Epoch 125/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0324 - accuracy: 0.8265\n",
      "Epoch 126/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0320 - accuracy: 0.8256\n",
      "Epoch 127/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0317 - accuracy: 0.8275\n",
      "Epoch 128/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0314 - accuracy: 0.8312\n",
      "Epoch 129/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0311 - accuracy: 0.8293\n",
      "Epoch 130/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0308 - accuracy: 0.8358\n",
      "Epoch 131/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0305 - accuracy: 0.8321\n",
      "Epoch 132/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0302 - accuracy: 0.8414\n",
      "Epoch 133/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0299 - accuracy: 0.8386\n",
      "Epoch 134/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0296 - accuracy: 0.8414\n",
      "Epoch 135/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0293 - accuracy: 0.8460\n",
      "Epoch 136/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0290 - accuracy: 0.8469\n",
      "Epoch 137/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0288 - accuracy: 0.8469\n",
      "Epoch 138/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0285 - accuracy: 0.8525\n",
      "Epoch 139/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0282 - accuracy: 0.8506\n",
      "Epoch 140/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0280 - accuracy: 0.8534\n",
      "Epoch 141/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0277 - accuracy: 0.8534\n",
      "Epoch 142/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0275 - accuracy: 0.8544\n",
      "Epoch 143/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0272 - accuracy: 0.8544\n",
      "Epoch 144/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0270 - accuracy: 0.8553\n",
      "Epoch 145/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0268 - accuracy: 0.8590\n",
      "Epoch 146/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0265 - accuracy: 0.8581\n",
      "Epoch 147/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0263 - accuracy: 0.8609\n",
      "Epoch 148/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0261 - accuracy: 0.8627\n",
      "Epoch 149/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0258 - accuracy: 0.8646\n",
      "Epoch 150/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0256 - accuracy: 0.8655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x150bfd690>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "model_plus_ext.fit(X_train, y_v_train, epochs = 150, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/719 [==============================] - 0s 222us/step\n",
      "Accuracy: 75.38\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the keras model\n",
    "accuracy = model_plus_ext.evaluate(X_test, y_v_test)\n",
    "print('Accuracy: %.2f' % (accuracy[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
