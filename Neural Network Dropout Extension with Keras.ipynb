{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Dropout Extension with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits # The MNIST data set is in scikit learn data set\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import numpy as np\n",
    "import numpy.random as r # We will randomly initialize our weights\n",
    "import matplotlib.pyplot as plt \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits=load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Scale training features\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test set.  60% training and %40 test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert digits to vectors\n",
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.1002 - accuracy: 0.2171\n",
      "Epoch 2/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0734 - accuracy: 0.4091\n",
      "Epoch 3/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0553 - accuracy: 0.5955\n",
      "Epoch 4/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0450 - accuracy: 0.6781\n",
      "Epoch 5/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0402 - accuracy: 0.7217\n",
      "Epoch 6/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0361 - accuracy: 0.7560\n",
      "Epoch 7/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0307 - accuracy: 0.8098\n",
      "Epoch 8/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0264 - accuracy: 0.8321\n",
      "Epoch 9/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0241 - accuracy: 0.8451\n",
      "Epoch 10/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0225 - accuracy: 0.8553\n",
      "Epoch 11/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0211 - accuracy: 0.8664\n",
      "Epoch 12/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0201 - accuracy: 0.8692\n",
      "Epoch 13/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0192 - accuracy: 0.8711\n",
      "Epoch 14/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0186 - accuracy: 0.8720\n",
      "Epoch 15/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0180 - accuracy: 0.8757\n",
      "Epoch 16/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0176 - accuracy: 0.8766\n",
      "Epoch 17/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0171 - accuracy: 0.8803\n",
      "Epoch 18/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0167 - accuracy: 0.8803\n",
      "Epoch 19/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0163 - accuracy: 0.8813\n",
      "Epoch 20/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0160 - accuracy: 0.8831\n",
      "Epoch 21/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0157 - accuracy: 0.8859\n",
      "Epoch 22/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0154 - accuracy: 0.8850\n",
      "Epoch 23/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0152 - accuracy: 0.8859\n",
      "Epoch 24/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0149 - accuracy: 0.8878\n",
      "Epoch 25/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0147 - accuracy: 0.8887\n",
      "Epoch 26/150\n",
      "1078/1078 [==============================] - 16s 14ms/step - loss: 0.0145 - accuracy: 0.88786s - loss: 0.012 - ETA: 6s - loss: 0.0131 - accuracy: 0.90 - ETA: 6 - ETA: 13s - ETA: 3s - loss: 0.0140 \n",
      "Epoch 27/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0143 - accuracy: 0.8905\n",
      "Epoch 28/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0141 - accuracy: 0.8905\n",
      "Epoch 29/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0140 - accuracy: 0.8905\n",
      "Epoch 30/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0139 - accuracy: 0.8915\n",
      "Epoch 31/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0137 - accuracy: 0.8905\n",
      "Epoch 32/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0136 - accuracy: 0.8915\n",
      "Epoch 33/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0135 - accuracy: 0.8915\n",
      "Epoch 34/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0133 - accuracy: 0.8915\n",
      "Epoch 35/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0132 - accuracy: 0.8942\n",
      "Epoch 36/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0130 - accuracy: 0.8980\n",
      "Epoch 37/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0126 - accuracy: 0.9045\n",
      "Epoch 38/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0122 - accuracy: 0.9072\n",
      "Epoch 39/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0117 - accuracy: 0.9128\n",
      "Epoch 40/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9221\n",
      "Epoch 41/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0108 - accuracy: 0.9239\n",
      "Epoch 42/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0105 - accuracy: 0.9258\n",
      "Epoch 43/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0091 - accuracy: 0.9545\n",
      "Epoch 44/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0067 - accuracy: 0.9796\n",
      "Epoch 45/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0056 - accuracy: 0.9824\n",
      "Epoch 46/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0050 - accuracy: 0.9879\n",
      "Epoch 47/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0045 - accuracy: 0.9879\n",
      "Epoch 48/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0042 - accuracy: 0.9898\n",
      "Epoch 49/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0040 - accuracy: 0.9917\n",
      "Epoch 50/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0038 - accuracy: 0.9926\n",
      "Epoch 51/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0036 - accuracy: 0.9926\n",
      "Epoch 52/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0034 - accuracy: 0.9935\n",
      "Epoch 53/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0033 - accuracy: 0.9935\n",
      "Epoch 54/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0031 - accuracy: 0.9935\n",
      "Epoch 55/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0030 - accuracy: 0.9935\n",
      "Epoch 56/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0029 - accuracy: 0.9935\n",
      "Epoch 57/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 0.9944\n",
      "Epoch 58/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 0.9944\n",
      "Epoch 59/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0027 - accuracy: 0.9944\n",
      "Epoch 60/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0026 - accuracy: 0.9944\n",
      "Epoch 61/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0025 - accuracy: 0.9944\n",
      "Epoch 62/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0025 - accuracy: 0.9944\n",
      "Epoch 63/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0024 - accuracy: 0.9944\n",
      "Epoch 64/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0024 - accuracy: 0.9944\n",
      "Epoch 65/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0023 - accuracy: 0.9944\n",
      "Epoch 66/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0023 - accuracy: 0.9944\n",
      "Epoch 67/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0022 - accuracy: 0.9944\n",
      "Epoch 68/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0022 - accuracy: 0.9944\n",
      "Epoch 69/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0021 - accuracy: 0.9944\n",
      "Epoch 70/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0021 - accuracy: 0.9944\n",
      "Epoch 71/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9944\n",
      "Epoch 72/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9944\n",
      "Epoch 73/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9944\n",
      "Epoch 74/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 0.9944\n",
      "Epoch 75/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 0.9944\n",
      "Epoch 76/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 0.9944\n",
      "Epoch 77/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 0.9944\n",
      "Epoch 78/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 0.9944\n",
      "Epoch 79/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 0.9944\n",
      "Epoch 80/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 0.9944\n",
      "Epoch 81/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 0.9944\n",
      "Epoch 82/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 0.9944\n",
      "Epoch 83/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 0.9944\n",
      "Epoch 84/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 0.9944\n",
      "Epoch 85/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0016 - accuracy: 0.9944\n",
      "Epoch 86/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9944\n",
      "Epoch 87/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9944\n",
      "Epoch 88/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9944\n",
      "Epoch 89/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9944\n",
      "Epoch 90/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9944\n",
      "Epoch 91/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9944\n",
      "Epoch 92/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0015 - accuracy: 0.9944\n",
      "Epoch 93/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9944\n",
      "Epoch 94/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9944\n",
      "Epoch 95/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9944\n",
      "Epoch 96/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9944\n",
      "Epoch 97/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9944\n",
      "Epoch 98/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9944\n",
      "Epoch 99/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9944\n",
      "Epoch 100/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9944\n",
      "Epoch 101/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9944\n",
      "Epoch 102/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9944\n",
      "Epoch 103/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9944\n",
      "Epoch 104/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9944\n",
      "Epoch 105/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9944\n",
      "Epoch 106/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9944\n",
      "Epoch 107/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9944\n",
      "Epoch 108/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9944\n",
      "Epoch 109/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9944\n",
      "Epoch 110/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9944\n",
      "Epoch 111/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9944\n",
      "Epoch 112/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9944\n",
      "Epoch 113/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9944\n",
      "Epoch 114/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9944\n",
      "Epoch 115/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9944\n",
      "Epoch 116/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9944\n",
      "Epoch 117/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9944\n",
      "Epoch 118/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 119/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 120/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 121/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 122/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 123/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 124/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 125/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 126/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 127/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 128/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 129/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 130/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9944\n",
      "Epoch 131/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9944\n",
      "Epoch 132/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9944\n",
      "Epoch 133/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9944\n",
      "Epoch 134/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9944\n",
      "Epoch 135/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9944\n",
      "Epoch 136/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9944\n",
      "Epoch 137/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9944\n",
      "Epoch 138/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0010 - accuracy: 0.9944\n",
      "Epoch 139/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 9.9628e-04 - accuracy: 0.9944\n",
      "Epoch 140/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 9.9357e-04 - accuracy: 0.9944\n",
      "Epoch 141/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 9.8631e-04 - accuracy: 0.9944\n",
      "Epoch 142/150\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 9.8129e-04 - accuracy: 0.9944\n",
      "Epoch 143/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 9.7698e-04 - accuracy: 0.9944\n",
      "Epoch 144/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 9.6978e-04 - accuracy: 0.9944\n",
      "Epoch 145/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 9.6604e-04 - accuracy: 0.9944\n",
      "Epoch 146/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 9.6046e-04 - accuracy: 0.9944\n",
      "Epoch 147/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 9.5560e-04 - accuracy: 0.9944\n",
      "Epoch 148/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 9.5165e-04 - accuracy: 0.9944\n",
      "Epoch 149/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 9.3959e-04 - accuracy: 0.9944\n",
      "Epoch 150/150\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 9.3924e-04 - accuracy: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x153bc2e50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "model.fit(X_train, y_v_train, epochs = 150, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/719 [==============================] - 0s 177us/step\n",
      "Accuracy: 96.38\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the keras model\n",
    "accuracy = model.evaluate(X_test, y_v_test)\n",
    "print('Accuracy: %.2f' % (accuracy[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras implementation plus extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Define the keras model with softmax outer layer\n",
    "model_plus_ext = Sequential()\n",
    "model_plus_ext.add(Dense(64, activation='sigmoid', kernel_constraint = maxnorm(3)))\n",
    "model_plus_ext.add(Dropout(0.2))\n",
    "model_plus_ext.add(Dense(30, activation='sigmoid', kernel_constraint = maxnorm(3)))\n",
    "model_plus_ext.add(Dropout(0.2))\n",
    "model_plus_ext.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "# Compile the keras model\n",
    "sgd = SGD(lr=0.1, momentum=0.9)\n",
    "model_plus_ext.compile(loss='mean_squared_error', optimizer=sgd, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0894 - accuracy: 0.2199\n",
      "Epoch 2/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0681 - accuracy: 0.5046\n",
      "Epoch 3/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0456 - accuracy: 0.7180\n",
      "Epoch 4/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0350 - accuracy: 0.7950\n",
      "Epoch 5/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0278 - accuracy: 0.8599\n",
      "Epoch 6/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0231 - accuracy: 0.8794\n",
      "Epoch 7/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0186 - accuracy: 0.9174\n",
      "Epoch 8/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0159 - accuracy: 0.9230\n",
      "Epoch 9/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0150 - accuracy: 0.9286\n",
      "Epoch 10/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0132 - accuracy: 0.9443\n",
      "Epoch 11/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9518\n",
      "Epoch 12/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9471\n",
      "Epoch 13/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0096 - accuracy: 0.9508\n",
      "Epoch 14/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0099 - accuracy: 0.9518\n",
      "Epoch 15/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0087 - accuracy: 0.9638\n",
      "Epoch 16/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0079 - accuracy: 0.9675\n",
      "Epoch 17/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0079 - accuracy: 0.9610\n",
      "Epoch 18/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0088 - accuracy: 0.9620\n",
      "Epoch 19/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0084 - accuracy: 0.9592\n",
      "Epoch 20/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0065 - accuracy: 0.9777\n",
      "Epoch 21/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0068 - accuracy: 0.9731\n",
      "Epoch 22/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0070 - accuracy: 0.9722\n",
      "Epoch 23/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0061 - accuracy: 0.9777\n",
      "Epoch 24/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0060 - accuracy: 0.9722\n",
      "Epoch 25/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0052 - accuracy: 0.9759\n",
      "Epoch 26/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0054 - accuracy: 0.9805\n",
      "Epoch 27/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0062 - accuracy: 0.9731\n",
      "Epoch 28/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0054 - accuracy: 0.9824\n",
      "Epoch 29/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0060 - accuracy: 0.9768\n",
      "Epoch 30/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0056 - accuracy: 0.9787\n",
      "Epoch 31/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0048 - accuracy: 0.9787\n",
      "Epoch 32/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0051 - accuracy: 0.9777\n",
      "Epoch 33/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0046 - accuracy: 0.9768\n",
      "Epoch 34/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0050 - accuracy: 0.9796\n",
      "Epoch 35/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0046 - accuracy: 0.9842\n",
      "Epoch 36/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0046 - accuracy: 0.9796\n",
      "Epoch 37/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0043 - accuracy: 0.9852\n",
      "Epoch 38/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0050 - accuracy: 0.9805\n",
      "Epoch 39/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0046 - accuracy: 0.9796\n",
      "Epoch 40/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0038 - accuracy: 0.9879\n",
      "Epoch 41/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0044 - accuracy: 0.9796\n",
      "Epoch 42/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0044 - accuracy: 0.9842\n",
      "Epoch 43/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0042 - accuracy: 0.9824\n",
      "Epoch 44/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0048 - accuracy: 0.9768\n",
      "Epoch 45/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0039 - accuracy: 0.9824\n",
      "Epoch 46/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0041 - accuracy: 0.9824\n",
      "Epoch 47/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0035 - accuracy: 0.9852\n",
      "Epoch 48/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0038 - accuracy: 0.9833\n",
      "Epoch 49/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0036 - accuracy: 0.9861\n",
      "Epoch 50/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0039 - accuracy: 0.9842\n",
      "Epoch 51/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0042 - accuracy: 0.9814\n",
      "Epoch 52/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0037 - accuracy: 0.9861\n",
      "Epoch 53/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0035 - accuracy: 0.9861\n",
      "Epoch 54/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0039 - accuracy: 0.9842\n",
      "Epoch 55/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0039 - accuracy: 0.9861\n",
      "Epoch 56/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0033 - accuracy: 0.9870\n",
      "Epoch 57/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0036 - accuracy: 0.9852\n",
      "Epoch 58/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0036 - accuracy: 0.9870\n",
      "Epoch 59/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0034 - accuracy: 0.9870\n",
      "Epoch 60/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0032 - accuracy: 0.9861\n",
      "Epoch 61/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0030 - accuracy: 0.9907\n",
      "Epoch 62/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0030 - accuracy: 0.9898\n",
      "Epoch 63/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0035 - accuracy: 0.9842\n",
      "Epoch 64/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0034 - accuracy: 0.9898\n",
      "Epoch 65/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0034 - accuracy: 0.9879\n",
      "Epoch 66/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0031 - accuracy: 0.9889\n",
      "Epoch 67/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0034 - accuracy: 0.9852\n",
      "Epoch 68/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0030 - accuracy: 0.9907\n",
      "Epoch 69/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0038 - accuracy: 0.9842\n",
      "Epoch 70/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0033 - accuracy: 0.9879\n",
      "Epoch 71/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0028 - accuracy: 0.9898\n",
      "Epoch 72/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0033 - accuracy: 0.9879\n",
      "Epoch 73/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0032 - accuracy: 0.9852\n",
      "Epoch 74/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0031 - accuracy: 0.9879\n",
      "Epoch 75/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0029 - accuracy: 0.9898\n",
      "Epoch 76/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0029 - accuracy: 0.9917\n",
      "Epoch 77/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0031 - accuracy: 0.9852\n",
      "Epoch 78/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0028 - accuracy: 0.9926\n",
      "Epoch 79/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0039 - accuracy: 0.9842\n",
      "Epoch 80/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0031 - accuracy: 0.9926\n",
      "Epoch 81/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0033 - accuracy: 0.9870\n",
      "Epoch 82/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0033 - accuracy: 0.9842\n",
      "Epoch 83/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0034 - accuracy: 0.9852\n",
      "Epoch 84/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0029 - accuracy: 0.9889\n",
      "Epoch 85/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0029 - accuracy: 0.9879\n",
      "Epoch 86/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0031 - accuracy: 0.9879\n",
      "Epoch 87/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0025 - accuracy: 0.9917\n",
      "Epoch 88/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0029 - accuracy: 0.9917\n",
      "Epoch 89/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 0.9898\n",
      "Epoch 90/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0026 - accuracy: 0.9889\n",
      "Epoch 91/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0026 - accuracy: 0.9907\n",
      "Epoch 92/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0026 - accuracy: 0.9917\n",
      "Epoch 93/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 0.9935\n",
      "Epoch 94/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0031 - accuracy: 0.9898\n",
      "Epoch 95/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0028 - accuracy: 0.9917\n",
      "Epoch 96/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0033 - accuracy: 0.9870\n",
      "Epoch 97/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0026 - accuracy: 0.9907\n",
      "Epoch 98/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0025 - accuracy: 0.9889\n",
      "Epoch 99/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0027 - accuracy: 0.9879\n",
      "Epoch 100/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0029 - accuracy: 0.9898\n",
      "Epoch 101/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0025 - accuracy: 0.9907\n",
      "Epoch 102/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0028 - accuracy: 0.9889\n",
      "Epoch 103/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0026 - accuracy: 0.9907\n",
      "Epoch 104/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0022 - accuracy: 0.9926\n",
      "Epoch 105/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0027 - accuracy: 0.9889\n",
      "Epoch 106/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0023 - accuracy: 0.9926\n",
      "Epoch 107/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0026 - accuracy: 0.9889\n",
      "Epoch 108/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0023 - accuracy: 0.9926\n",
      "Epoch 109/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0027 - accuracy: 0.9898\n",
      "Epoch 110/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0026 - accuracy: 0.9898\n",
      "Epoch 111/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0027 - accuracy: 0.9907\n",
      "Epoch 112/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0027 - accuracy: 0.9898\n",
      "Epoch 113/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0025 - accuracy: 0.9898\n",
      "Epoch 114/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 0.9898\n",
      "Epoch 115/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 0.9926\n",
      "Epoch 116/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0025 - accuracy: 0.9926\n",
      "Epoch 117/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0018 - accuracy: 0.9944\n",
      "Epoch 118/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 0.9907\n",
      "Epoch 119/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0028 - accuracy: 0.9917\n",
      "Epoch 120/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0023 - accuracy: 0.9889\n",
      "Epoch 121/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0021 - accuracy: 0.9954\n",
      "Epoch 122/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0019 - accuracy: 0.9944\n",
      "Epoch 123/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0021 - accuracy: 0.9926\n",
      "Epoch 124/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0021 - accuracy: 0.9926\n",
      "Epoch 125/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0019 - accuracy: 0.9954\n",
      "Epoch 126/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0020 - accuracy: 0.9954\n",
      "Epoch 127/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0026 - accuracy: 0.9898\n",
      "Epoch 128/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0020 - accuracy: 0.9954\n",
      "Epoch 129/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 0.9917\n",
      "Epoch 130/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0021 - accuracy: 0.9944\n",
      "Epoch 131/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0021 - accuracy: 0.9944\n",
      "Epoch 132/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0025 - accuracy: 0.9879\n",
      "Epoch 133/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0024 - accuracy: 0.9926\n",
      "Epoch 134/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0023 - accuracy: 0.9907\n",
      "Epoch 135/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0021 - accuracy: 0.9917\n",
      "Epoch 136/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0017 - accuracy: 0.9935\n",
      "Epoch 137/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 0.9944\n",
      "Epoch 138/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0021 - accuracy: 0.9935\n",
      "Epoch 139/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0018 - accuracy: 0.9926\n",
      "Epoch 140/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0023 - accuracy: 0.9926\n",
      "Epoch 141/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0017 - accuracy: 0.9963\n",
      "Epoch 142/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 0.9907\n",
      "Epoch 143/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0027 - accuracy: 0.9898\n",
      "Epoch 144/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0021 - accuracy: 0.9935\n",
      "Epoch 145/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0023 - accuracy: 0.9907\n",
      "Epoch 146/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0026 - accuracy: 0.9926: 0s - l\n",
      "Epoch 147/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0018 - accuracy: 0.9954\n",
      "Epoch 148/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0022 - accuracy: 0.9917\n",
      "Epoch 149/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0020 - accuracy: 0.9944\n",
      "Epoch 150/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0025 - accuracy: 0.9926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1593aca90>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "model_plus_ext.fit(X_train, y_v_train, epochs = 150, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/719 [==============================] - 0s 228us/step\n",
      "Accuracy: 97.77\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the keras model\n",
    "accuracy = model_plus_ext.evaluate(X_test, y_v_test)\n",
    "print('Accuracy: %.2f' % (accuracy[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainX\n",
    "y_train = trainY\n",
    "X_test = testX\n",
    "y_test = testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the digits dataset:\n",
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
      "    0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
      "   54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
      "  144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
      "  107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
      "  216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
      "  223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
      "  235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
      "  180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
      "  169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
      "  198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
      "  232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
      "  222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
      "  211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
      "  224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
      "  255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
      "  188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
      "  168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
      "  239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
      "  199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
      "  195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
      "  210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
      "  182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of the digits dataset:\") \n",
    "print(X_train.shape)\n",
    "plt.gray()\n",
    "plt.matshow(X_train[0])\n",
    "plt.show()\n",
    "print(y_train[0:1])\n",
    "print(X_train[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset\n",
    "\n",
    "X_scale = StandardScaler()\n",
    "X_train = [X_scale.fit_transform(X_train[i]).flatten() for i in range(len(X_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [X_scale.fit_transform(X_test[i]).flatten() for i in range(len(X_test))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34062577, -0.48590849, -0.55368775, -0.57830472, -0.61724799,\n",
       "       -0.65473466, -0.66524031, -0.69031625, -0.71089238, -0.73827078,\n",
       "       -0.77682253, -0.7644687 , -0.90444624, -1.24217044, -1.49218887,\n",
       "       -1.6432188 , -1.7458434 , -1.58582058, -1.56547495, -1.51925798,\n",
       "       -1.44902444, -1.39545527, -1.35520903, -1.32176483, -1.33646764,\n",
       "       -1.31334192, -1.22103858, -0.55220401, -0.34062577, -0.48590849,\n",
       "       -0.55368775, -0.57830472, -0.61724799, -0.65473466, -0.66524031,\n",
       "       -0.69031625, -0.71089238, -0.73827078, -0.77682253, -0.7644687 ,\n",
       "       -0.90444624, -1.24217044, -1.49218887, -1.6432188 , -1.7458434 ,\n",
       "       -1.58582058, -1.56547495, -1.51925798, -1.44902444, -1.39545527,\n",
       "       -1.35520903, -1.32176483, -1.33646764, -1.31334192, -1.22103858,\n",
       "       -0.55220401, -0.34062577, -0.48590849, -0.55368775, -0.57830472,\n",
       "       -0.61724799, -0.65473466, -0.66524031, -0.69031625, -0.71089238,\n",
       "       -0.73827078, -0.77682253, -0.7644687 , -0.90444624, -1.24217044,\n",
       "       -1.49218887, -1.6432188 , -1.7458434 , -1.58582058, -1.56547495,\n",
       "       -1.51925798, -1.44902444, -1.39545527, -1.35520903, -1.32176483,\n",
       "       -1.33646764, -1.31334192, -1.22103858, -0.55220401, -0.34062577,\n",
       "       -0.48590849, -0.55368775, -0.57830472, -0.61724799, -0.65473466,\n",
       "       -0.66524031, -0.69031625, -0.71089238, -0.73827078, -0.77682253,\n",
       "       -0.7644687 , -0.89474708, -1.24217044, -1.49218887, -1.50501833,\n",
       "       -0.92869114, -1.58582058, -1.56547495, -1.50882914, -1.40684909,\n",
       "       -1.39545527, -1.35520903, -1.32176483, -1.33646764, -1.30350416,\n",
       "       -1.21065938, -0.55220401, -0.34062577, -0.48590849, -0.55368775,\n",
       "       -0.57830472, -0.61724799, -0.65473466, -0.66524031, -0.69031625,\n",
       "       -0.71089238, -0.73827078, -0.77682253, -0.7644687 , -0.87534877,\n",
       "       -1.24217044, -1.11784184, -0.19742925, -0.32422235, -0.92052707,\n",
       "       -0.98117524, -1.51925798, -1.44902444, -1.39545527, -1.34501949,\n",
       "       -1.29113635, -1.2957372 , -1.31334192, -1.22103858, -0.45849666,\n",
       "       -0.34062577, -0.48590849, -0.55368775, -0.57830472, -0.61724799,\n",
       "       -0.65473466, -0.66524031, -0.69031625, -0.71089238, -0.73827078,\n",
       "       -0.77682253, -0.7644687 , -0.8462513 , -1.24217044, -0.43153894,\n",
       "        0.52546553,  0.22427711, -0.14792816, -0.00734239, -0.23651111,\n",
       "       -1.20651619, -1.39545527, -1.35520903, -1.32176483, -1.33646764,\n",
       "       -1.19528872, -1.11724659, -0.55220401, -0.34062577, -0.48590849,\n",
       "       -0.55368775, -0.57830472, -0.61724799, -0.65473466, -0.66524031,\n",
       "       -0.69031625, -0.71089238, -0.73827078, -0.77682253, -0.7644687 ,\n",
       "       -0.90444624, -1.24217044,  0.11958308,  0.86565131,  0.57128697,\n",
       "        0.32421561, -0.40769589,  0.10764049,  0.2485333 , -0.26636474,\n",
       "       -0.70307837, -1.08694647, -0.55240663, -0.03443219, -0.47373628,\n",
       "       -0.08366727, -0.34062577, -0.48590849, -0.55368775, -0.57830472,\n",
       "       -0.61724799, -0.65473466, -0.66524031, -0.69031625, -0.71089238,\n",
       "       -0.73827078, -0.77682253, -0.75460914, -0.90444624, -0.56347324,\n",
       "        0.66030658,  0.72745084,  0.69441951,  0.73197615,  0.77172389,\n",
       "        0.18064234, -0.10995716, -0.14206119, -0.11208496,  0.16882127,\n",
       "        0.09928045, -0.44761841,  0.56418358,  1.50935763, -0.34062577,\n",
       "       -0.48590849, -0.55368775, -0.57830472, -0.61724799, -0.65473466,\n",
       "       -0.66524031, -0.69031625, -0.71089238, -0.72787783, -0.76645007,\n",
       "       -0.75460914, -0.90444624,  0.72506781,  0.92026979,  0.82312809,\n",
       "        0.8623275 ,  0.87147317,  0.84746644,  0.80637252,  0.81790049,\n",
       "        0.81093265,  0.31587579, -0.02515911, -0.08400654,  0.61486045,\n",
       "        1.1557979 , -0.55220401, -0.34062577, -0.48590849, -0.55368775,\n",
       "       -0.57830472, -0.61724799, -0.65473466, -0.66524031, -0.69031625,\n",
       "       -0.71089238, -0.73827078, -0.77682253, -0.7644687 , -0.90444624,\n",
       "        0.55785256,  0.84748009,  0.6530352 ,  0.75038884,  0.86074263,\n",
       "        0.97731082,  0.84808787,  0.91279502,  0.90416031,  0.92724828,\n",
       "        0.93453332,  0.93425452,  1.09691105,  0.57456278, -0.55220401,\n",
       "       -0.34062577, -0.48590849, -0.55368775, -0.57830472, -0.61724799,\n",
       "       -0.65473466, -0.66524031, -0.69031625, -0.71089238, -0.73827078,\n",
       "       -0.77682253, -0.7644687 , -0.90444624,  0.65621447,  0.87867568,\n",
       "        0.67429681,  0.63845017,  0.53882642,  0.38219075,  0.69165532,\n",
       "        0.7651813 ,  0.79021539,  0.81516333,  0.9549523 ,  0.90370669,\n",
       "        1.07723551,  0.87555954, -0.55220401, -0.34062577, -0.48590849,\n",
       "       -0.55368775, -0.57830472, -0.61724799, -0.65473466, -0.66524031,\n",
       "       -0.69031625, -0.71089238, -0.72787783, -0.74570517, -0.7644687 ,\n",
       "       -0.78805637,  0.91195545,  0.79548745,  0.61051198,  0.69441951,\n",
       "        0.47444318,  0.26316673,  0.84808787,  0.74409363,  0.86272579,\n",
       "        0.92724828,  0.84264787,  0.96480235,  0.62469822,  0.94821393,\n",
       "        1.07205668, -0.34062577, -0.48590849, -0.55368775, -0.57830472,\n",
       "       -0.61724799, -0.65473466, -0.66524031, -0.69031625, -0.71089238,\n",
       "       -0.73827078, -0.71458782, -0.7644687 ,  0.05577014,  1.15786023,\n",
       "        0.81628451,  0.69555842,  0.69441951,  0.59247912,  0.57695732,\n",
       "        0.78551485,  0.81790049,  0.81093265,  0.9068692 ,  0.92432382,\n",
       "        1.15827196, -0.14264762,  0.51228759,  1.19699981, -0.34062577,\n",
       "       -0.48590849, -0.55368775, -0.57830472, -0.61724799, -0.65473466,\n",
       "       -0.66524031, -0.69031625, -0.71089238, -0.69669897, -0.77682253,\n",
       "       -0.7644687 , -0.37099269,  1.0791707 ,  0.87867568,  0.80186648,\n",
       "        0.80635817,  0.98950912,  0.94484973,  0.70208416,  0.849532  ,\n",
       "        0.91451894,  1.0291437 ,  0.89369534,  0.87315886,  0.74275143,\n",
       "       -0.26615231, -0.55220401, -0.34062577, -0.48590849, -0.54178048,\n",
       "       -0.53288387, -0.55051848, -0.57816664, -0.64363157, -0.69031625,\n",
       "       -0.71089238, -0.73827078, -0.77682253, -0.7644687 ,  1.39425358,\n",
       "        0.98080879,  0.76429186,  0.72745084,  0.73919497,  0.76416777,\n",
       "        0.83664608,  0.78551485,  0.82844432,  0.91451894,  0.97819599,\n",
       "        0.87327635,  0.88334147,  1.19528872, -0.42184029, -0.55220401,\n",
       "       -0.34062577, -0.44604881, -0.55368775, -0.57830472, -0.61724799,\n",
       "       -0.65473466, -0.66524031, -0.69031625, -0.71089238, -0.09390775,\n",
       "        0.72718294,  1.24688332,  1.30696118,  0.79392115,  0.72269775,\n",
       "        0.70618923,  0.69441951,  0.64613183,  0.71762206,  0.75422834,\n",
       "        0.91279502,  0.91451894,  0.87630058,  0.87327635,  0.94443713,\n",
       "        1.08707328,  0.429254  , -0.55220401, -0.34062577, -0.48590849,\n",
       "       -0.55368775, -0.57830472, -0.41705945, -0.17344999,  0.22071778,\n",
       "        0.44418045,  1.23231138,  1.63132231,  1.5051168 ,  1.42435555,\n",
       "        1.20027047,  0.98080879,  0.58751687,  0.53609634,  0.61606244,\n",
       "        0.88220372,  0.85828681,  0.92108972,  0.40669085,  0.55196693,\n",
       "        1.19217637,  1.21018965,  1.03608063,  1.02804668,  1.01048912,\n",
       "       -0.55220401, -0.34062577,  0.27142544,  1.6729705 ,  1.78357935,\n",
       "        1.87398715,  1.76262695,  1.75493789,  1.51506051,  1.38653391,\n",
       "        1.48582098,  1.38064738,  1.29618116,  1.03538483,  0.32178397,\n",
       "        1.05545067,  0.40852667,  0.56009311,  0.80708993,  1.19371812,\n",
       "        1.14009528,  0.88116351,  1.02846385,  0.89667966,  0.83243838,\n",
       "        0.90370669,  0.96902007,  1.33224427, -0.55220401, -0.2140215 ,\n",
       "        2.19797667,  2.1611683 ,  1.96526274,  1.84062239,  1.65324407,\n",
       "        1.61448112,  1.57867714,  1.39681541,  1.39228441,  1.34953003,\n",
       "        1.40463642,  1.42335105, -0.45527514,  0.06759044,  1.06763661,\n",
       "        0.81755204,  0.78562885,  0.46875367,  0.08678282,  0.5648484 ,\n",
       "        0.77985676,  0.72345745,  0.81201939,  0.92407191,  0.929669  ,\n",
       "        1.1142811 , -0.55220401,  3.79511366,  2.60986004,  1.8039504 ,\n",
       "        1.80628977,  1.85174398,  1.85013325,  1.80895972,  1.7907326 ,\n",
       "        1.84920147,  1.5481787 ,  1.23543306,  1.35533857,  1.20027047,\n",
       "        1.12835166, -0.81628451, -0.86716999, -0.55929355, -0.33034735,\n",
       "        0.25234637,  0.76465718,  0.88116351,  0.83164991,  0.85592149,\n",
       "        0.9549523 ,  0.93425452,  0.89031793,  1.1557979 ,  0.35363368,\n",
       "        2.82448094,  2.22454979,  1.97065208,  1.7381585 ,  1.529218  ,\n",
       "        1.58761435,  1.61448112,  1.69530764,  1.50991192,  1.18442537,\n",
       "        1.26655041,  1.26660245,  1.01598652,  0.8529383 ,  1.00345802,\n",
       "        0.42978828,  0.79516431,  1.04316182,  1.02059228,  0.80637252,\n",
       "        0.849532  ,  0.80057402,  0.77440516,  0.94474281,  0.90370669,\n",
       "        0.86080463,  1.1661771 ,  1.54059341,  1.68504252,  2.21126323,\n",
       "        1.62534145,  1.62460638,  1.75164971,  1.50010804,  1.33356758,\n",
       "        1.32421059,  1.28371889,  1.25717604,  1.31841267,  1.345479  ,\n",
       "        1.21966878,  0.93162783,  0.79548745,  0.86565131,  0.77277657,\n",
       "        0.73197615,  0.58777768,  0.6290823 ,  0.51212922,  0.47945653,\n",
       "        0.44833983,  0.43426812,  0.50658488,  0.70340036,  0.91707633,\n",
       "        3.03991097, -0.34062577,  1.13505186,  2.05400293,  1.61325117,\n",
       "        1.3735158 ,  1.21571256,  1.31195884,  1.38782723,  1.38653391,\n",
       "        1.44424917,  1.43250964,  1.27646202,  1.14207554,  0.82342973,\n",
       "        0.58751687,  0.44041909,  0.42576671,  0.46371264,  0.54449622,\n",
       "        0.47264976,  0.63865526,  0.59340145,  0.43815029,  0.27091621,\n",
       "        0.36402833,  0.42794287,  0.95859313,  2.32148797, -0.34062577,\n",
       "       -0.48590849,  0.32744974,  1.56783032,  1.74052812,  1.43447832,\n",
       "        1.22552391,  1.13336068,  1.08837036,  1.14285356,  1.142081  ,\n",
       "        1.08913022,  0.92869412,  0.60703352,  0.51472717,  0.4616807 ,\n",
       "        0.53770538,  0.65686237,  0.7068017 ,  0.67079765,  0.77572514,\n",
       "        0.55196693,  0.56042479,  0.65887698,  0.61859359,  0.8116158 ,\n",
       "        0.54342518, -0.55220401, -0.25622292, -0.48590849, -0.55368775,\n",
       "       -0.57830472,  0.11677665,  1.53292291,  1.73332915,  1.82254092,\n",
       "        1.74638646,  1.77682364,  1.77480054,  1.6314065 ,  1.46214767,\n",
       "        0.93162783,  0.79548745,  0.40852667,  0.39218511,  0.33494615,\n",
       "        0.40383148,  0.37879023,  0.45941003,  0.42766339,  0.33625487,\n",
       "        0.39343014, -0.32838919, -0.74275143, -1.22103858, -0.55220401,\n",
       "       -0.34062577, -0.48590849, -0.55368775, -0.57830472, -0.61724799,\n",
       "       -0.65473466, -0.66524031, -0.26620533, -0.0837208 , -0.28098089,\n",
       "       -0.03000602, -0.36022639, -0.5649758 , -1.24217044, -1.49218887,\n",
       "       -1.6432188 , -1.7458434 , -1.58582058, -1.56547495, -1.51925798,\n",
       "       -1.44902444, -1.39545527, -1.35520903, -1.32176483, -1.33646764,\n",
       "       -1.31334192, -1.22103858, -0.55220401, -0.34062577, -0.48590849,\n",
       "       -0.55368775, -0.57830472, -0.61724799, -0.65473466, -0.66524031,\n",
       "       -0.69031625, -0.71089238, -0.73827078, -0.77682253, -0.7644687 ,\n",
       "       -0.90444624, -1.24217044, -1.49218887, -1.6432188 , -1.7458434 ,\n",
       "       -1.58582058, -1.56547495, -1.51925798, -1.44902444, -1.39545527,\n",
       "       -1.35520903, -1.32176483, -1.33646764, -1.31334192, -1.22103858,\n",
       "       -0.55220401, -0.34062577, -0.48590849, -0.55368775, -0.57830472,\n",
       "       -0.61724799, -0.65473466, -0.66524031, -0.69031625, -0.71089238,\n",
       "       -0.73827078, -0.77682253, -0.7644687 , -0.90444624, -1.24217044,\n",
       "       -1.49218887, -1.6432188 , -1.7458434 , -1.58582058, -1.56547495,\n",
       "       -1.51925798, -1.44902444, -1.39545527, -1.35520903, -1.32176483,\n",
       "       -1.33646764, -1.31334192, -1.22103858, -0.55220401])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] # Looking at the new features after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 784)\n",
      "(1078,)\n",
      "(719, 784)\n",
      "(719,)\n"
     ]
    }
   ],
   "source": [
    "# Downsample the data\n",
    "\n",
    "# Add y_train back as an additional column to X_train\n",
    "y_train = y_train.reshape((-1,1))\n",
    "X_train = np.append(X_train, y_train, axis=1)\n",
    "\n",
    "# Add y_test back as an additional column to X_test\n",
    "y_test = y_test.reshape((-1,1))\n",
    "X_test = np.append(X_test, y_test, axis=1)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(X_train)\n",
    "np.random.shuffle(X_test)\n",
    "\n",
    "# Slice out only the first 1078 from X_train and 719 from X_test\n",
    "X_train = X_train[0:1078]\n",
    "X_test = X_test[0:719]\n",
    "\n",
    "# Remove the last columns of X_train and X_test and place them back into y_train and y_test\n",
    "y_train = X_train[:,-1]\n",
    "y_test = X_test[:,-1]\n",
    "X_train = X_train[:,0:X_train.shape[1]-1]\n",
    "X_test = X_test[:,0:X_test.shape[1]-1]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(int(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert digits to vectors\n",
    "y_v_train = convert_y_to_vect(y_train.astype(np.int))\n",
    "y_v_test = convert_y_to_vect(y_test.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "The shape of the digits dataset:\n",
      "(1078, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# Show that newly downsampled X_train still matches with y_train\n",
    "# Here we display a pullover with label of 2 which is correct\n",
    "\n",
    "X_train_test = np.zeros((28,28))\n",
    "for i in range(28):\n",
    "    X_train_test[i] = X_train[3][28*i:28*i+28]\n",
    "print(X_train_test.shape)\n",
    "\n",
    "print(\"The shape of the digits dataset:\") \n",
    "print(X_train.shape)\n",
    "plt.gray()\n",
    "plt.matshow(X_train_test)\n",
    "plt.show()\n",
    "print(y_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "The shape of the digits dataset:\n",
      "(1078, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAECCAYAAAD+eGJTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASHUlEQVR4nO3dW2xdZXoG4PfF8SGOg2MnOAk5TGhOSgVqEixUQVVRjTqic8NJVOVilCLUcAHSIM1FETcgoUpRNcl0LiqkUA4ZiaEgEQoXqJ0IjUQnF9E4EQJD2hKSNMnE2EE5OdiJT18vvKCbYP+f7bX3Xsv53kdC9t7fXt6f13Ze1t7/v/5FM4OIxHVT0Q2ISLEUAiLBKQREglMIiASnEBAJTiEgElwhIUDyPpL/TfIYyWeK6CGF5EmSn5D8iGRPCfp5heQAyd6K+zpJHiD5efa1o2T9PU/yD9k+/Ijkjwvsbw3J35I8SvJTkj/N7i/FPkz0V5d9yHrPEyDZAOB/APwlgDMAfg/gUTP7rK6NJJA8CaDbzL4quhcAIPnnAK4A+JWZ3Z7d948AzpvZrixIO8zs70vU3/MArpjZz4voqRLJlQBWmtkRkosBHAbwAIC/RQn2YaK/v0Yd9mERRwJ3AThmZsfNbATAvwK4v4A+5g0z+xDA+evuvh/Avuz7fZj8oynENP2Vhpn1mdmR7PtBAEcBrEJJ9mGiv7ooIgRWAThdcfsM6vgLz5AB+A3JwyR3Ft3MNJabWR8w+UcEoKvgfqbyFMmPs7cLhb1dqURyHYBtAA6hhPvwuv6AOuzDIkKAU9xXtrnL95jZdgB/BeDJ7HBXZudFAOsBbAXQB2B3se0AJNsAvA3gaTO7XHQ/15uiv7rswyJC4AyANRW3VwM4W0Af0zKzs9nXAQDvYPItTNn0Z+8lv3lPOVBwP99hZv1mNm5mEwBeQsH7kGQjJv+BvW5m+7O7S7MPp+qvXvuwiBD4PYCNJG8j2QTgbwC8V0AfUyK5KPtwBiQXAfgRgN70VoV4D8CO7PsdAN4tsJfv+eYfV+ZBFLgPSRLAywCOmtmeilIp9uF0/dVrH9Z9dAAAsqGOfwLQAOAVM/uHujcxDZJ/hMn/+wPAAgC/Lro/km8AuBfAMgD9AJ4D8G8A3gKwFsApAI+YWSEfzk3T372YPIw1ACcBPPHN++8C+vszAP8J4BMAE9ndz2LyfXfh+zDR36Oowz4sJAREpDw0Y1AkOIWASHAKAZHgFAIiwSkERIIrNARKPCUXgPrLq8z9lbk3oL79FX0kUOoXAuovrzL3V+begDr2V3QIiEjBck0WInkfgF9icubfv5jZrtTjW1tbrb29/dvbQ0NDaG1tnfPzV9vw8PB3bo+MjKCpqenb20NDQ8ntx8bGkvWFCxfOvTkAExMT37k9Pj6OhoaGb283NjYmt6987FRuuin9/4TJ2a0z7+/atWtobm5OblNpfHw8V937W67sf3R09Hv76/rX/3re/vV+19n8rVf738alS5cwNDQ05Qu4YK4/NFsc5J9RsTgIyfdSi4O0t7fjsccem+tT1lxvb3pq9uHDh5P18+fTM05vv/32ZN37I7969Wqy3tWVPhN2yZIlyXpbW1uy7oWA94/IMzg4mKs+OjqarHsh573+K1asSNY3bNiQrG/bti1Zr6VXX3112lqetwNaHETkBpAnBObD4iAi4sgTAjNaHITkTpI9JHu899QiUn95QmBGi4OY2V4z6zaz7jJ9CCgik/KEQKkXBxGRmZnz6ICZjZF8CsB/4P8XB/m0ap0FtGtXcoQVp06dStYXLEi/nJXDs1PxhtiuHwKc7fN7ox8XLlxI1r0hukOHDiXr3uhOVHMOAQAws/cBvF+lXkSkAJoxKBKcQkAkOIWASHAKAZHgFAIiwSkERILLNUQo1TUwkO8qWN5ZdP39/cm6dyq0N0+gpaUlWc97ll/lad1SPToSEAlOISASnEJAJDiFgEhwCgGR4BQCIsEpBESC0zyBOtq+fXuy7o2De6sZ5x1H99YD8M7393hLrl+7di1Z95anu+OOO5J1bz0Bbx6Ex1uNuax0JCASnEJAJDiFgEhwCgGR4BQCIsEpBESCUwiIBKd5AlW0efPmZP2hhx5K1r3z/b1x7JGRkWTd410hqrOzM1n3rivgrVfgjbNfunQpWfeu+rx79+5k/eGHH07Wb1Q6EhAJTiEgEpxCQCQ4hYBIcAoBkeAUAiLBKQREgtM8gSp68sknk3XvfHmPt/3SpUuTde98/itXruTa3jvf//Lly8m6d90Bb70D7/m9n//4448n62+++WaybmbJelnlCgGSJwEMAhgHMGZm3dVoSkTqpxpHAn9hZl9V4eeISAH0mYBIcHlDwAD8huRhkjur0ZCI1FfetwP3mNlZkl0ADpD8LzP7sPIBWTjsBICbb74559OJSLXlOhIws7PZ1wEA7wC4a4rH7DWzbjPr9s5SE5H6m3MIkFxEcvE33wP4EYDeajUmIvWR5+3AcgDvZOeALwDwazP796p0VVIrV65M1r3z6T0tLS3J+sGDB5P1L774Illfs2ZNst7Y2Jise+Pwp0+fTtbvvvvuZN07UvTmKWzcuDFZ99ZbWLx4cbJ+o5pzCJjZcQB/UsVeRKQAGiIUCU4hIBKcQkAkOIWASHAKAZHgFAIiwWk9gVlYt25dsu6dr+7xzpdfu3Ztsn7nnXcm6+fOnUvW29vbk/WOjo5k3VsvwBuHb25uTtYvXryYrLe1tSXr3nULvGntS5YsSdbnKx0JiASnEBAJTiEgEpxCQCQ4hYBIcAoBkeAUAiLBaZ7ALHjn49d6nsCGDRuS9a6urmS9qakpWffWQ1i1alWyfv78+WR9YmIiWW9oaEjWFy1alKx7v5/3+njrKXiv//DwcLJeVjoSEAlOISASnEJAJDiFgEhwCgGR4BQCIsEpBESC0zyBWfDOJ/fGwb1x6P7+/mTdG+ceHBxM1js7O5N1b57AhQsXkvXVq1cn6948CG//efUjR44k6948C8/69euT9d7e+XntHR0JiASnEBAJTiEgEpxCQCQ4hYBIcAoBkeAUAiLBaZ7ALHjr0nvj2N66/vv370/Wt2/fnqx78wi8/rztR0ZGkvWFCxcm62NjY8m6N4/C6++zzz5L1r395+2fFStWJOs37DwBkq+QHCDZW3FfJ8kDJD/PvqavSiEipTWTtwOvAbjvuvueAfCBmW0E8EF2W0TmITcEzOxDANevG3U/gH3Z9/sAPFDlvkSkTub6weByM+sDgOxrenE7ESmtmo8OkNxJsodkz9DQUK2fTkRmaa4h0E9yJQBkXweme6CZ7TWzbjPrbm1tnePTiUitzDUE3gOwI/t+B4B3q9OOiNSbO0+A5BsA7gWwjOQZAM8B2AXgLZKPAzgF4JFaNlkWHR3pkdCrV68m6944+blz55J1b939vG+3vHHyvNt7dZLJuvf7e+sdeLz+li9fnuvnl5UbAmb26DSlH1a5FxEpgKYNiwSnEBAJTiEgEpxCQCQ4hYBIcAoBkeC0nsAseOPw3rr63vnyx48fT9YbGhpy1b3rCnjn63s/3xvn9+qevPvPY2bJ+vDwcK6fX1Y6EhAJTiEgEpxCQCQ4hYBIcAoBkeAUAiLBKQREgtM8gVloampK1vOeT+/JO86eV97n97b39o+3//Py5kk0Nzcn6951KcpKRwIiwSkERIJTCIgEpxAQCU4hIBKcQkAkOIWASHCaJzAL3noB3jj3mTNncj2/N47tXdfAG6f3zqfPy3v+kZGRZL2trS3X83vXdfDWS/D2/6233jrrnspARwIiwSkERIJTCIgEpxAQCU4hIBKcQkAkOIWASHCaJ1Bh2bJlybo3TuzVvXXxb7vttmTdMzo6mqy3tLQk63nXO/B48wS8eQ6eVatWJesnTpxI1js6OpJ17/VdunRpsu5dt6Io7pEAyVdIDpDsrbjveZJ/IPlR9t+Pa9umiNTKTN4OvAbgvinu/4WZbc3+e7+6bYlIvbghYGYfAjhfh15EpAB5Phh8iuTH2duF9JspESmtuYbAiwDWA9gKoA/A7ukeSHInyR6SPWX9YEQksjmFgJn1m9m4mU0AeAnAXYnH7jWzbjPrbm1tnWufIlIjcwoBkisrbj4IoHe6x4pIubnzBEi+AeBeAMtIngHwHIB7SW4FYABOAniihj3WzS233JJre28c2bu+/ebNm3M9v7fegcfr35tH4G0/Pj6erHvn83tWr16drJ8/n/5825sn4v3+3nUHyvp22P2rMbNHp7j75Rr0IiIF0LRhkeAUAiLBKQREglMIiASnEBAJTiEgEpzWE6iwfPnyZD3vOPrly5eT9S1btiTrV69eTda9/jze+f55eT/fm+dw4cKFZH3Tpk3J+smTJ3M9v/f6tre3J+tffvllsl4UHQmIBKcQEAlOISASnEJAJDiFgEhwCgGR4BQCIsFpnkCFrq6uXNs3NjYm6yMjI8l6W1tbsv71118n63mvi+DxxvnzzjMws2S9qakpWffmeRw+fHjWPVXyfr/5unKWjgREglMIiASnEBAJTiEgEpxCQCQ4hYBIcAoBkeA0T6CCdz744OBgrp+/aNGiXPWLFy8m6948Be98eI+3vVf35gF41x3w1lNYsmRJrrpn4cKFyXreeRhFmZ9di0jVKAREglMIiASnEBAJTiEgEpxCQCQ4hYBIcJonMAt5rzvgzTPIOw7tjbPnHcf3zqf3nt/j/X7edQGam5uTdW89Bk/e9SDKyj0SILmG5G9JHiX5KcmfZvd3kjxA8vPsa0ft2xWRapvJ24ExAD8zsy0A/hTAkyT/GMAzAD4ws40APshui8g844aAmfWZ2ZHs+0EARwGsAnA/gH3Zw/YBeKBWTYpI7czqg0GS6wBsA3AIwHIz6wMmgwJAvgX6RKQQMw4Bkm0A3gbwtJmlr6z53e12kuwh2TM0NDSXHkWkhmYUAiQbMRkAr5vZ/uzufpIrs/pKAANTbWtme82s28y65+tqrCI3spmMDhDAywCOmtmeitJ7AHZk3+8A8G712xORWpvJPIF7APwEwCckP8ruexbALgBvkXwcwCkAj9SmxfrJu67++Ph4sn7ixIlk3Runz8vrP+/z551n4G3vzXPw5hn09/cn6948gJaWlmTdW8+hrNwQMLPfAZju1fthddsRkXrTtGGR4BQCIsEpBESCUwiIBKcQEAlOISASnNYTqOCNE3u8adFXrlxJ1r3z8b31BpqampJ1bxzdq3vzILztx8bGkvW88zTyrnfgvT7edSHyrqdQFB0JiASnEBAJTiEgEpxCQCQ4hYBIcAoBkeAUAiLBaZ5AhbVr1ybr3noA3vnu3jyEF154IVk/ffp0su6N03u8ce688wS8df+9/ec9/6ZNm5J1b56Cd90Ar78b9roDInJjUwiIBKcQEAlOISASnEJAJDiFgEhwCgGR4DRPoII3zuuNgw8ODuZ6/osXLybrfX19ybq3bv/o6Giy7o3De+fre+sZXLt2LdfP9+YxePMwvNfP295bT8D7/ctKRwIiwSkERIJTCIgEpxAQCU4hIBKcQkAkOIWASHDuPAGSawD8CsAKABMA9prZL0k+D+DvAJzLHvqsmb1fq0brobOzM1n3zic/e/Zsruf3xsnzrhfgzQPw5hnkHcfPuy5/3t/fc+zYsWS9u7s7WV+xYkU126mbmUwWGgPwMzM7QnIxgMMkD2S1X5jZz2vXnojUmhsCZtYHoC/7fpDkUQCrat2YiNTHrI6vSK4DsA3Aoeyup0h+TPIVkh1V7k1E6mDGIUCyDcDbAJ42s8sAXgSwHsBWTB4p7J5mu50ke0j2eNfqE5H6m1EIkGzEZAC8bmb7AcDM+s1s3MwmALwE4K6ptjWzvWbWbWbdra2t1epbRKrEDQFOfiT8MoCjZran4v6VFQ97EEBv9dsTkVqbyejAPQB+AuATkh9l9z0L4FGSWwEYgJMAnqhJhyJSUzMZHfgdgKkGiOf1nICpdHV1JesLFqR318DAQDXb+R5vnoI3zu9t79W9eQLePIS86xV49byGh4eT9cbGxmTdu65BWWnGoEhwCgGR4BQCIsEpBESCUwiIBKcQEAlOISASnK47UGHPnj3J+qVLl5L1gwcPJuvz9XzzKPbv35+se9eVeO2115L1LVu2zLalutCRgEhwCgGR4BQCIsEpBESCUwiIBKcQEAlOISASHL1z0Kv6ZOQ5AP9bcdcyAF/VrYHZU3/5lLm/MvcGVL+/H5jZLVMV6hoC33tyssfM0ld0KJD6y6fM/ZW5N6C+/entgEhwCgGR4IoOgb0FP79H/eVT5v7K3BtQx/4K/UxARIpX9JGAiBRMISASnEJAJDiFgEhwCgGR4P4Pkpd9tXke9wYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Again we show that, as expected, an image of an ankle boot has a label of 9\n",
    "\n",
    "X_train_test2 = np.zeros((28,28))\n",
    "for i in range(28):\n",
    "    X_train_test2[i] = X_train[435][28*i:28*i+28]\n",
    "print(X_train_test2.shape)\n",
    "\n",
    "print(\"The shape of the digits dataset:\") \n",
    "print(X_train.shape)\n",
    "plt.gray()\n",
    "plt.matshow(X_train_test2)\n",
    "plt.show()\n",
    "print(y_train[435])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert digits to vectors (one hot encoding)\n",
    "y_v_train = convert_y_to_vect(y_train.astype(np.int))\n",
    "y_v_test = convert_y_to_vect(y_test.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0940 - accuracy: 0.3256\n",
      "Epoch 2/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0640 - accuracy: 0.5009\n",
      "Epoch 3/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0574 - accuracy: 0.5510\n",
      "Epoch 4/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0516 - accuracy: 0.5918\n",
      "Epoch 5/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0475 - accuracy: 0.6113\n",
      "Epoch 6/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0451 - accuracy: 0.6187\n",
      "Epoch 7/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0434 - accuracy: 0.6262\n",
      "Epoch 8/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0413 - accuracy: 0.6531\n",
      "Epoch 9/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0380 - accuracy: 0.6883\n",
      "Epoch 10/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0349 - accuracy: 0.7143\n",
      "Epoch 11/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0328 - accuracy: 0.7310\n",
      "Epoch 12/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0312 - accuracy: 0.7403\n",
      "Epoch 13/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0299 - accuracy: 0.7449\n",
      "Epoch 14/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0290 - accuracy: 0.7514\n",
      "Epoch 15/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0281 - accuracy: 0.7579\n",
      "Epoch 16/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0274 - accuracy: 0.7616\n",
      "Epoch 17/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0267 - accuracy: 0.7662\n",
      "Epoch 18/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0263 - accuracy: 0.7699\n",
      "Epoch 19/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0257 - accuracy: 0.7727\n",
      "Epoch 20/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0252 - accuracy: 0.7755\n",
      "Epoch 21/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0249 - accuracy: 0.7746\n",
      "Epoch 22/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0246 - accuracy: 0.7746\n",
      "Epoch 23/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0243 - accuracy: 0.7783\n",
      "Epoch 24/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0241 - accuracy: 0.7774\n",
      "Epoch 25/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0238 - accuracy: 0.7811\n",
      "Epoch 26/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0237 - accuracy: 0.7801\n",
      "Epoch 27/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0235 - accuracy: 0.7820\n",
      "Epoch 28/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0234 - accuracy: 0.7801\n",
      "Epoch 29/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0234 - accuracy: 0.7829\n",
      "Epoch 30/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0233 - accuracy: 0.7792\n",
      "Epoch 31/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0233 - accuracy: 0.7801\n",
      "Epoch 32/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0232 - accuracy: 0.7820\n",
      "Epoch 33/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0232 - accuracy: 0.7811\n",
      "Epoch 34/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0231 - accuracy: 0.7811\n",
      "Epoch 35/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0231 - accuracy: 0.7811\n",
      "Epoch 36/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0231 - accuracy: 0.7820\n",
      "Epoch 37/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0230 - accuracy: 0.7829\n",
      "Epoch 38/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0230 - accuracy: 0.7839\n",
      "Epoch 39/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0228 - accuracy: 0.7848\n",
      "Epoch 40/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0228 - accuracy: 0.7839\n",
      "Epoch 41/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0228 - accuracy: 0.7848\n",
      "Epoch 42/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0228 - accuracy: 0.7848\n",
      "Epoch 43/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.7848\n",
      "Epoch 44/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7857\n",
      "Epoch 45/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7839\n",
      "Epoch 46/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7848\n",
      "Epoch 47/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.7857\n",
      "Epoch 48/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.7848\n",
      "Epoch 49/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.7857\n",
      "Epoch 50/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.7857\n",
      "Epoch 51/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.7866\n",
      "Epoch 52/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.7866\n",
      "Epoch 53/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7866\n",
      "Epoch 54/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7866\n",
      "Epoch 55/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7848\n",
      "Epoch 56/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7857\n",
      "Epoch 57/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0227 - accuracy: 0.7866\n",
      "Epoch 58/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7857\n",
      "Epoch 59/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.7866\n",
      "Epoch 60/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7866\n",
      "Epoch 61/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7857\n",
      "Epoch 62/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.7857\n",
      "Epoch 63/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.7857\n",
      "Epoch 64/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.7848\n",
      "Epoch 65/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 66/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 67/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 68/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 69/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 70/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 71/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 72/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 73/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7848\n",
      "Epoch 74/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7839\n",
      "Epoch 75/150\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 76/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7848\n",
      "Epoch 77/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7839\n",
      "Epoch 78/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 79/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 80/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 81/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 82/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7839\n",
      "Epoch 83/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7848\n",
      "Epoch 84/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 85/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 86/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 87/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7839\n",
      "Epoch 88/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7848\n",
      "Epoch 89/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7848\n",
      "Epoch 90/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7848\n",
      "Epoch 91/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 92/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 93/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 94/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 95/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 96/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 97/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 98/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 99/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 100/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 101/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 102/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 103/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 104/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 105/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 106/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 107/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 108/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 109/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 110/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 111/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 112/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 113/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 114/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 115/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 116/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 117/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 118/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 119/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7857\n",
      "Epoch 120/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 121/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 122/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 123/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 124/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 125/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 126/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 127/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 128/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 129/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 130/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 131/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 132/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 133/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 134/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 135/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 136/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 137/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 138/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 139/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 140/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 141/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 142/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 143/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 144/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 145/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 146/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7876\n",
      "Epoch 147/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 148/150\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 149/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7866\n",
      "Epoch 150/150\n",
      "1078/1078 [==============================] - 3s 2ms/step - loss: 0.0226 - accuracy: 0.7876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14046b250>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "model.fit(X_train, y_v_train, epochs = 150, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/719 [==============================] - 0s 245us/step\n",
      "Accuracy: 62.73\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the keras model\n",
    "accuracy = model.evaluate(X_test, y_v_test)\n",
    "print('Accuracy: %.2f' % (accuracy[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras implementation plus extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Define the keras model with softmax outer layer\n",
    "model_plus_ext = Sequential()\n",
    "model_plus_ext.add(Dense(64, activation='sigmoid', kernel_constraint = maxnorm(3)))\n",
    "model_plus_ext.add(Dropout(0.2))\n",
    "model_plus_ext.add(Dense(30, activation='sigmoid', kernel_constraint = maxnorm(3)))\n",
    "model_plus_ext.add(Dropout(0.2))\n",
    "model_plus_ext.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "# Compile the keras model\n",
    "sgd = SGD(lr=0.1, momentum=0.9)\n",
    "model_plus_ext.compile(loss='mean_squared_error', optimizer=sgd, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0816 - accuracy: 0.3442\n",
      "Epoch 2/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0566 - accuracy: 0.5909\n",
      "Epoch 3/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0486 - accuracy: 0.6531\n",
      "Epoch 4/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0402 - accuracy: 0.7384\n",
      "Epoch 5/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0354 - accuracy: 0.7811\n",
      "Epoch 6/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0303 - accuracy: 0.8182\n",
      "Epoch 7/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0266 - accuracy: 0.8432\n",
      "Epoch 8/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0226 - accuracy: 0.8673\n",
      "Epoch 9/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0214 - accuracy: 0.8785\n",
      "Epoch 10/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0200 - accuracy: 0.8850\n",
      "Epoch 11/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0178 - accuracy: 0.9035\n",
      "Epoch 12/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0176 - accuracy: 0.8989\n",
      "Epoch 13/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0154 - accuracy: 0.9082\n",
      "Epoch 14/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0147 - accuracy: 0.9137\n",
      "Epoch 15/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0134 - accuracy: 0.9202\n",
      "Epoch 16/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0134 - accuracy: 0.9212\n",
      "Epoch 17/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0119 - accuracy: 0.9369\n",
      "Epoch 18/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0132 - accuracy: 0.9239\n",
      "Epoch 19/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0114 - accuracy: 0.9416\n",
      "Epoch 20/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0102 - accuracy: 0.9481\n",
      "Epoch 21/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0112 - accuracy: 0.9406\n",
      "Epoch 22/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0099 - accuracy: 0.9490\n",
      "Epoch 23/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0101 - accuracy: 0.9434\n",
      "Epoch 24/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0099 - accuracy: 0.9453\n",
      "Epoch 25/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0092 - accuracy: 0.9518\n",
      "Epoch 26/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0098 - accuracy: 0.9406\n",
      "Epoch 27/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0078 - accuracy: 0.9638\n",
      "Epoch 28/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0082 - accuracy: 0.9536\n",
      "Epoch 29/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0089 - accuracy: 0.9462\n",
      "Epoch 30/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0084 - accuracy: 0.9555\n",
      "Epoch 31/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0074 - accuracy: 0.9685\n",
      "Epoch 32/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0098 - accuracy: 0.9462\n",
      "Epoch 33/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0085 - accuracy: 0.9555\n",
      "Epoch 34/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0077 - accuracy: 0.9620\n",
      "Epoch 35/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0072 - accuracy: 0.9647\n",
      "Epoch 36/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0067 - accuracy: 0.9629\n",
      "Epoch 37/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0074 - accuracy: 0.9657\n",
      "Epoch 38/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0060 - accuracy: 0.9731\n",
      "Epoch 39/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0071 - accuracy: 0.9647\n",
      "Epoch 40/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0066 - accuracy: 0.9647\n",
      "Epoch 41/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0067 - accuracy: 0.9666\n",
      "Epoch 42/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0077 - accuracy: 0.9564\n",
      "Epoch 43/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0079 - accuracy: 0.9573\n",
      "Epoch 44/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0067 - accuracy: 0.9629\n",
      "Epoch 45/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0074 - accuracy: 0.9620\n",
      "Epoch 46/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0064 - accuracy: 0.9675\n",
      "Epoch 47/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0054 - accuracy: 0.9722\n",
      "Epoch 48/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0073 - accuracy: 0.9657\n",
      "Epoch 49/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0073 - accuracy: 0.9647\n",
      "Epoch 50/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0066 - accuracy: 0.9629\n",
      "Epoch 51/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0077 - accuracy: 0.9545\n",
      "Epoch 52/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0077 - accuracy: 0.9629\n",
      "Epoch 53/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0085 - accuracy: 0.9601\n",
      "Epoch 54/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0076 - accuracy: 0.9620\n",
      "Epoch 55/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0066 - accuracy: 0.9629\n",
      "Epoch 56/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0054 - accuracy: 0.9759\n",
      "Epoch 57/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0072 - accuracy: 0.9564\n",
      "Epoch 58/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0065 - accuracy: 0.9694\n",
      "Epoch 59/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0059 - accuracy: 0.9750\n",
      "Epoch 60/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0077 - accuracy: 0.9527\n",
      "Epoch 61/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0064 - accuracy: 0.9666\n",
      "Epoch 62/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0069 - accuracy: 0.9638\n",
      "Epoch 63/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0076 - accuracy: 0.9545\n",
      "Epoch 64/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0045 - accuracy: 0.9824\n",
      "Epoch 65/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0066 - accuracy: 0.9638\n",
      "Epoch 66/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0060 - accuracy: 0.9657\n",
      "Epoch 67/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0075 - accuracy: 0.9583\n",
      "Epoch 68/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0064 - accuracy: 0.9647\n",
      "Epoch 69/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0051 - accuracy: 0.9722\n",
      "Epoch 70/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0057 - accuracy: 0.9657\n",
      "Epoch 71/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0077 - accuracy: 0.9583\n",
      "Epoch 72/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0061 - accuracy: 0.9712\n",
      "Epoch 73/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0068 - accuracy: 0.9685\n",
      "Epoch 74/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0077 - accuracy: 0.9555\n",
      "Epoch 75/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0069 - accuracy: 0.9647\n",
      "Epoch 76/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0067 - accuracy: 0.9712\n",
      "Epoch 77/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0057 - accuracy: 0.9722\n",
      "Epoch 78/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0062 - accuracy: 0.9740\n",
      "Epoch 79/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0063 - accuracy: 0.9657\n",
      "Epoch 80/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0067 - accuracy: 0.9620\n",
      "Epoch 81/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0071 - accuracy: 0.9647\n",
      "Epoch 82/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0060 - accuracy: 0.9657\n",
      "Epoch 83/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0071 - accuracy: 0.9601\n",
      "Epoch 84/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0055 - accuracy: 0.9685\n",
      "Epoch 85/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0058 - accuracy: 0.9712\n",
      "Epoch 86/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0048 - accuracy: 0.9740\n",
      "Epoch 87/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0056 - accuracy: 0.9685\n",
      "Epoch 88/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0066 - accuracy: 0.9647\n",
      "Epoch 89/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0051 - accuracy: 0.9750\n",
      "Epoch 90/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0059 - accuracy: 0.9685\n",
      "Epoch 91/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0075 - accuracy: 0.9564\n",
      "Epoch 92/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0074 - accuracy: 0.9592\n",
      "Epoch 93/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0057 - accuracy: 0.9703\n",
      "Epoch 94/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0055 - accuracy: 0.9722\n",
      "Epoch 95/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0070 - accuracy: 0.9629\n",
      "Epoch 96/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0064 - accuracy: 0.9675\n",
      "Epoch 97/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0065 - accuracy: 0.9666\n",
      "Epoch 98/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0055 - accuracy: 0.9750\n",
      "Epoch 99/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0061 - accuracy: 0.9657\n",
      "Epoch 100/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0072 - accuracy: 0.9583\n",
      "Epoch 101/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0077 - accuracy: 0.9555\n",
      "Epoch 102/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0070 - accuracy: 0.9610\n",
      "Epoch 103/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0080 - accuracy: 0.9601\n",
      "Epoch 104/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0082 - accuracy: 0.9555\n",
      "Epoch 105/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0087 - accuracy: 0.9518\n",
      "Epoch 106/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0060 - accuracy: 0.9694\n",
      "Epoch 107/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0065 - accuracy: 0.9712\n",
      "Epoch 108/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0069 - accuracy: 0.9638\n",
      "Epoch 109/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0056 - accuracy: 0.9740\n",
      "Epoch 110/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0050 - accuracy: 0.9740\n",
      "Epoch 111/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0068 - accuracy: 0.9610\n",
      "Epoch 112/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0066 - accuracy: 0.9657\n",
      "Epoch 113/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0074 - accuracy: 0.9573\n",
      "Epoch 114/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0059 - accuracy: 0.9675\n",
      "Epoch 115/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0060 - accuracy: 0.9722\n",
      "Epoch 116/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0058 - accuracy: 0.9703\n",
      "Epoch 117/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0082 - accuracy: 0.9564\n",
      "Epoch 118/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0064 - accuracy: 0.9685\n",
      "Epoch 119/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0075 - accuracy: 0.9620\n",
      "Epoch 120/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0053 - accuracy: 0.9722\n",
      "Epoch 121/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0078 - accuracy: 0.9536\n",
      "Epoch 122/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0080 - accuracy: 0.9592\n",
      "Epoch 123/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0070 - accuracy: 0.9620\n",
      "Epoch 124/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0064 - accuracy: 0.9638\n",
      "Epoch 125/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0068 - accuracy: 0.9601\n",
      "Epoch 126/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0069 - accuracy: 0.9620\n",
      "Epoch 127/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0061 - accuracy: 0.9657\n",
      "Epoch 128/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0089 - accuracy: 0.9443\n",
      "Epoch 129/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0075 - accuracy: 0.9601\n",
      "Epoch 130/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0070 - accuracy: 0.9583\n",
      "Epoch 131/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0059 - accuracy: 0.9712\n",
      "Epoch 132/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0066 - accuracy: 0.9657\n",
      "Epoch 133/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0069 - accuracy: 0.9564\n",
      "Epoch 134/150\n",
      "1078/1078 [==============================] - 4s 3ms/step - loss: 0.0069 - accuracy: 0.9647\n",
      "Epoch 135/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0053 - accuracy: 0.9703\n",
      "Epoch 136/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0085 - accuracy: 0.9536\n",
      "Epoch 137/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0075 - accuracy: 0.9610\n",
      "Epoch 138/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0070 - accuracy: 0.9629\n",
      "Epoch 139/150\n",
      "1078/1078 [==============================] - 6s 6ms/step - loss: 0.0061 - accuracy: 0.9694\n",
      "Epoch 140/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0063 - accuracy: 0.9657\n",
      "Epoch 141/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0072 - accuracy: 0.9610\n",
      "Epoch 142/150\n",
      "1078/1078 [==============================] - 5s 5ms/step - loss: 0.0064 - accuracy: 0.9657\n",
      "Epoch 143/150\n",
      "1078/1078 [==============================] - 5s 4ms/step - loss: 0.0065 - accuracy: 0.9666\n",
      "Epoch 144/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0068 - accuracy: 0.9638\n",
      "Epoch 145/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0049 - accuracy: 0.9731\n",
      "Epoch 146/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0076 - accuracy: 0.9564\n",
      "Epoch 147/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0066 - accuracy: 0.9620\n",
      "Epoch 148/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0082 - accuracy: 0.9536\n",
      "Epoch 149/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0067 - accuracy: 0.9647\n",
      "Epoch 150/150\n",
      "1078/1078 [==============================] - 4s 4ms/step - loss: 0.0079 - accuracy: 0.9527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15ef8f6d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "model_plus_ext.fit(X_train, y_v_train, epochs = 150, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/719 [==============================] - 0s 300us/step\n",
      "Accuracy: 77.19\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the keras model\n",
    "accuracy = model_plus_ext.evaluate(X_test, y_v_test)\n",
    "print('Accuracy: %.2f' % (accuracy[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
