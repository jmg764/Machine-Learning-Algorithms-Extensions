{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Soft Margin Extension with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from numpy import genfromtxt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from cvxopt import matrix, solvers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits=load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Scale training features\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X and y the subset of data that describe the numbers 8 and 9\n",
    "\n",
    "new_X = []\n",
    "new_y = []\n",
    "for i in range(len(X)):\n",
    "    if y[i] == 8:\n",
    "        new_X.append(X[i])\n",
    "        new_y.append(y[i])\n",
    "    elif y[i] == 9:\n",
    "        new_X.append(X[i])\n",
    "        new_y.append(y[i])\n",
    "new_X = np.array(new_X)\n",
    "new_y = np.array(new_y)\n",
    "\n",
    "X = new_X\n",
    "y = new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 64)\n",
      "(141,)\n",
      "(213, 64)\n",
      "(213,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8262e-02 -4.0188e-02  2e+02  1e+01  1e+00\n",
      " 1: -3.0905e-04 -1.1130e-04  2e+00  1e-01  1e-02\n",
      " 2: -1.8827e-05 -6.7504e-05  3e-02  2e-03  2e-04\n",
      " 3: -6.8873e-06 -1.1057e-05  1e-03  8e-05  7e-06\n",
      " 4: -1.2441e-06 -1.0286e-07  3e-05  3e-06  2e-07\n",
      " 5: -1.3608e-08 -1.0581e-11  4e-07  3e-08  2e-09\n",
      " 6: -1.3618e-10 -1.0581e-15  4e-09  3e-10  2e-11\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "def kernel_svm(X, y): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    G = matrix(-np.eye(m))\n",
    "    h = matrix(np.zeros(m))\n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_svm(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8262e-02 -4.0188e-02  2e+02  1e+01  1e+00\n",
      " 1: -3.0905e-04 -1.1130e-04  2e+00  1e-01  1e-02\n",
      " 2: -1.8827e-05 -6.7504e-05  3e-02  2e-03  2e-04\n",
      " 3: -6.8873e-06 -1.1057e-05  1e-03  8e-05  7e-06\n",
      " 4: -1.2441e-06 -1.0286e-07  3e-05  3e-06  2e-07\n",
      " 5: -1.3608e-08 -1.0581e-11  4e-07  3e-08  2e-09\n",
      " 6: -1.3618e-10 -1.0581e-15  4e-09  3e-10  2e-11\n",
      "Optimal solution found.\n",
      "[ 1.10008300e-11  2.09343171e-11 -1.52888491e-11 -1.67481751e-11\n",
      " -1.66363190e-11 -1.37071990e-11 -1.49454228e-11 -4.22863306e-12\n",
      " -1.65478139e-11  3.43433702e-11  2.60853523e-11  2.11618856e-11\n",
      " -1.54137825e-11 -1.85374511e-11 -1.44943188e-11  2.11612318e-11\n",
      "  2.58213534e-11 -1.36190907e-11 -1.72111703e-11 -1.74237986e-11\n",
      "  1.34051841e-11  8.87109839e-12 -1.49446968e-11 -1.58828641e-11\n",
      " -1.29457904e-11  1.39840867e-11  2.43037776e-11  2.68312148e-11\n",
      "  1.28495891e-11 -9.40838869e-12 -1.56115124e-11  2.20192537e-11\n",
      "  2.57802252e-11 -1.64524268e-11  1.20248647e-11  8.71851732e-12\n",
      " -1.78152256e-11 -1.54681937e-11 -1.69369660e-11 -1.56928018e-11\n",
      "  2.03017181e-11 -1.24168815e-11 -1.43429212e-11 -1.74064062e-11\n",
      " -1.23124636e-11  1.58690658e-11  1.82075663e-11 -1.62947767e-11\n",
      " -1.34383917e-11 -1.52778117e-11  7.37365738e-12 -1.66344752e-11\n",
      " -8.22997012e-12  1.97973577e-11  2.69518566e-11  6.40280902e-12\n",
      " -1.71901933e-11  1.28503643e-11  1.63929985e-11 -1.57618011e-11\n",
      "  2.71303860e-11  1.99909544e-11 -1.73503978e-11  2.52904980e-11\n",
      "  1.99579878e-11  1.22635226e-11 -1.34214430e-11  1.15973612e-11\n",
      " -1.54818477e-11  7.68398663e-12 -1.33721134e-11 -1.68019612e-11\n",
      " -1.48943797e-11  2.17831208e-11 -1.54976821e-11  3.13349112e-11\n",
      "  2.56203874e-11 -6.28125277e-12 -1.63494742e-11  1.15461943e-11\n",
      " -1.67916025e-11 -1.28148013e-11 -1.35935249e-11  1.57732604e-11\n",
      " -1.45889966e-11 -1.63632483e-11  2.02736469e-11 -1.26380473e-11\n",
      " -1.59980004e-11  1.11674308e-11 -1.81081316e-11  8.90566068e-12\n",
      " -1.72148103e-11 -1.49202229e-11 -1.20928445e-11  1.79762620e-11\n",
      "  5.02318619e-12 -1.52595436e-11 -1.51287373e-11 -1.71038656e-11\n",
      " -1.49003651e-11 -1.67085679e-11 -1.16996227e-11  2.26865624e-12\n",
      "  1.42072441e-11 -1.45200759e-11 -1.31442462e-11  2.26392871e-11\n",
      "  1.02259334e-11 -1.68209265e-11  3.50351526e-11 -1.57652280e-11\n",
      " -1.51828468e-11 -1.28634052e-11  2.97131310e-11 -1.23449569e-11\n",
      " -1.67385582e-11  1.90029100e-11  1.96035027e-11 -1.65791575e-11\n",
      "  1.70545745e-11  1.40021414e-11 -1.50971030e-11 -1.72352585e-11\n",
      " -1.48421173e-11  3.00695515e-11  1.27773135e-11  2.52269326e-11\n",
      "  1.17253568e-11  1.23490463e-12  1.18373556e-11  1.03972402e-11\n",
      " -1.76934452e-11  2.25759292e-11  2.24887951e-11  3.15339638e-11\n",
      "  1.00249723e-11  2.07633857e-11  1.14852360e-11  2.19285586e-11\n",
      "  3.10691849e-11]\n"
     ]
    }
   ],
   "source": [
    "def kernel_svm(X, y): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)*1.\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    G = matrix(-np.eye(m))\n",
    "    h = matrix(np.zeros(m))\n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_svm(X_train, y_train)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] > 1e-12]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] > 1e-12:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 9, 10, 11, 15, 16, 20, 21, 25, 26, 27, 28, 31, 32, 34, 35, 40, 45, 46, 50, 53, 54, 55, 57, 58, 60, 61, 63, 64, 65, 67, 69, 73, 75, 76, 79, 83, 86, 89, 91, 95, 96, 103, 104, 107, 108, 110, 114, 117, 118, 120, 121, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 8\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 79.81220657276995%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended implementation using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.1056e-02 -1.4523e+01  3e+02  1e+01  3e-14\n",
      " 1: -3.6794e-03 -1.2104e+01  2e+01  2e-01  3e-14\n",
      " 2: -1.3539e-05 -4.4261e-01  5e-01  4e-03  5e-15\n",
      " 3: -6.7445e-06 -1.3705e-02  1e-02  1e-04  6e-16\n",
      " 4: -1.4392e-06 -5.3602e-04  6e-04  4e-06  5e-16\n",
      " 5: -1.6065e-08 -5.4504e-06  6e-06  4e-08  8e-16\n",
      " 6: -1.6080e-10 -5.4504e-08  6e-08  4e-10  8e-16\n",
      "Optimal solution found.\n",
      "[ 1.36585483e-11  2.27506384e-11 -1.92366639e-11 -1.98397047e-11\n",
      " -2.00293069e-11 -1.61368576e-11 -1.81284643e-11  2.00850294e-12\n",
      " -2.03122376e-11  4.36445838e-11  2.90944943e-11  2.33602023e-11\n",
      " -1.95539561e-11 -2.12733573e-11 -1.69055886e-11  2.44005157e-11\n",
      "  2.84644718e-11 -1.60393180e-11 -2.08773643e-11 -2.05332723e-11\n",
      "  1.58030199e-11  1.03761550e-11 -1.75146329e-11 -1.81462117e-11\n",
      " -1.64446170e-11  1.71012765e-11  2.81641735e-11  3.21843997e-11\n",
      "  1.55842114e-11 -1.12318390e-11 -1.92758127e-11  2.37963252e-11\n",
      "  3.03680369e-11 -1.87172034e-11  1.57760173e-11  1.12448786e-11\n",
      " -2.04423767e-11 -1.73569720e-11 -2.05143741e-11 -1.88305274e-11\n",
      "  2.32417801e-11 -1.36254299e-11 -1.68648253e-11 -2.06368812e-11\n",
      " -1.46642733e-11  1.76430957e-11  2.20579040e-11 -1.91555891e-11\n",
      " -1.67150386e-11 -1.85446111e-11  1.14276482e-11 -2.02973481e-11\n",
      " -1.04092602e-11  2.27294890e-11  3.06090172e-11  7.51381618e-12\n",
      " -2.00792956e-11  1.47062924e-11  1.82162155e-11 -1.79397805e-11\n",
      "  3.02387562e-11  2.31590826e-11 -1.96106676e-11  2.99690264e-11\n",
      "  2.22345368e-11  1.58957239e-11 -1.46630007e-11  1.42105056e-11\n",
      " -1.74947983e-11  1.00897340e-11 -1.67350293e-11 -2.00352152e-11\n",
      " -1.80280702e-11  2.39573502e-11 -2.07912410e-11  3.50969011e-11\n",
      "  2.96728686e-11 -2.11425550e-12 -1.91888546e-11  1.36596447e-11\n",
      " -1.92825942e-11 -1.59292843e-11 -1.48221877e-11  1.95061577e-11\n",
      " -1.92316687e-11 -1.89591861e-11  2.31532210e-11 -1.48034991e-11\n",
      " -1.85255847e-11  1.42081963e-11 -2.03485768e-11  1.15096549e-11\n",
      " -2.00665389e-11 -1.77601953e-11 -1.34362701e-11  2.10130073e-11\n",
      "  1.08203332e-11 -1.88292712e-11 -1.84357208e-11 -1.99928904e-11\n",
      " -2.01295362e-11 -1.97805404e-11 -1.30103116e-11  6.47181383e-12\n",
      "  1.76123335e-11 -1.74546177e-11 -1.54810668e-11  2.55128360e-11\n",
      "  1.28479430e-11 -2.01878655e-11  4.50790265e-11 -1.86688790e-11\n",
      " -1.77719404e-11 -1.50658512e-11  3.18806982e-11 -1.52544232e-11\n",
      " -2.06396561e-11  2.30251137e-11  2.27318519e-11 -1.94400649e-11\n",
      "  1.86190120e-11  1.60561534e-11 -1.88091354e-11 -2.24295898e-11\n",
      " -1.83511192e-11  3.62328088e-11  1.48028060e-11  2.89711618e-11\n",
      "  1.43372286e-11  3.40252615e-12  1.46374955e-11  1.21663176e-11\n",
      " -2.05571950e-11  2.61102893e-11  2.64814029e-11  3.64437308e-11\n",
      "  1.26919657e-11  2.35231116e-11  1.29948496e-11  2.65616088e-11\n",
      "  3.56724993e-11]\n"
     ]
    }
   ],
   "source": [
    "def kernel_soft_margin_svm(X, y, C): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    \n",
    "    # Changed G and h\n",
    "    G = matrix(np.vstack((np.diag(np.ones(m))*-1, np.identity(m))))\n",
    "    h = matrix(np.hstack((np.zeros(m), np.ones(m)*C)))\n",
    "    \n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_soft_margin_svm(X_train, y_train, 0.1)\n",
    "\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] > 1e-12]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] > 1e-12:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])\n",
    "\n",
    "# print(\"The following are support vectors: \")\n",
    "# for i in range(len(support_vectors)):\n",
    "#     print(support_vectors[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 7, 9, 10, 11, 15, 16, 20, 21, 25, 26, 27, 28, 31, 32, 34, 35, 40, 45, 46, 50, 53, 54, 55, 57, 58, 60, 61, 63, 64, 65, 67, 69, 73, 75, 76, 79, 83, 86, 89, 91, 95, 96, 103, 104, 107, 108, 110, 114, 117, 118, 120, 121, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 8\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 80.28169014084507%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainX\n",
    "y_train = trainY\n",
    "X_test = testX\n",
    "y_test = testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X_train and y_train the subset of data that describe the labels 0 and 2 (T-shirts and pullovers, respectively)\n",
    "\n",
    "new_X_train = []\n",
    "new_y_train = []\n",
    "for i in range(len(X_train)):\n",
    "    if y_train[i] == 0:\n",
    "        new_X_train.append(X_train[i])\n",
    "        new_y_train.append(y_train[i])\n",
    "    elif y_train[i] == 2:\n",
    "        new_X_train.append(X_train[i])\n",
    "        new_y_train.append(y_train[i])\n",
    "new_X_train = np.array(new_X_train)\n",
    "new_y_train = np.array(new_y_train)\n",
    "\n",
    "X_train = new_X_train\n",
    "y_train = new_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X_test and y_test the subset of data that describe the labels 0 and 2 (T-shirts and pullovers, respectively)\n",
    "\n",
    "new_X_test = []\n",
    "new_y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    if y_test[i] == 0:\n",
    "        new_X_test.append(X_test[i])\n",
    "        new_y_test.append(y_test[i])\n",
    "    elif y_test[i] == 2:\n",
    "        new_X_test.append(X_test[i])\n",
    "        new_y_test.append(y_test[i])\n",
    "new_X_test = np.array(new_X_test)\n",
    "new_y_test = np.array(new_y_test)\n",
    "\n",
    "X_test = new_X_test\n",
    "y_test = new_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784)\n",
      "(12000,)\n",
      "(2000, 784)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([X_train[i].flatten() for i in range(len(X_train))])\n",
    "X_test = np.array([X_test[i].flatten() for i in range(len(X_test))])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 784)\n",
      "(141,)\n",
      "(213, 784)\n",
      "(213,)\n"
     ]
    }
   ],
   "source": [
    "# Downsample the data\n",
    "\n",
    "# Add y_train back as an additional column to X_train\n",
    "y_train = y_train.reshape((-1,1))\n",
    "X_train = np.append(X_train, y_train, axis=1)\n",
    "\n",
    "# Add y_test back as an additional column to X_test\n",
    "y_test = y_test.reshape((-1,1))\n",
    "X_test = np.append(X_test, y_test, axis=1)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(X_train)\n",
    "np.random.shuffle(X_test)\n",
    "\n",
    "# Slice out only the first 141 from X_train and 213 from X_test\n",
    "X_train = X_train[0:141]\n",
    "X_test = X_test[0:213]\n",
    "\n",
    "# Remove the last columns of X_train and X_test and place them back into y_train and y_test\n",
    "y_train = X_train[:,-1]\n",
    "y_test = X_test[:,-1]\n",
    "X_train = X_train[:,0:X_train.shape[1]-1]\n",
    "X_test = X_test[:,0:X_test.shape[1]-1]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset\n",
    "\n",
    "X_scale = StandardScaler()\n",
    "X_train = X_scale.fit_transform(X_train) \n",
    "X_test = X_scale.fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.5000e+01 -1.5000e+02  3e+02  1e+01  2e+00\n",
      " 1: -3.3583e+02 -3.3919e+02  7e+01  6e+00  1e+00\n",
      " 2: -3.7729e+04 -3.7733e+04  7e+01  6e+00  1e+00\n",
      " 3: -3.7397e+08 -3.7397e+08  4e+02  6e+00  1e+00\n",
      " 4: -3.7023e+14 -3.7023e+14  4e+06  6e+00  1e+00\n",
      " 5: -3.6653e+22 -3.6653e+22  4e+12  4e+00  1e+00\n",
      " 6: -3.6286e+32 -3.6286e+32  4e+20  5e+15  1e+00\n",
      " 7: -3.5918e+44 -3.5918e+44  4e+30  5e+27  1e+00\n",
      " 8: -3.5881e+58 -3.5881e+58  4e+42  8e+41  1e+00\n",
      " 9: -2.0543e+74 -2.0543e+74  2e+56  4e+00  1e+00\n",
      "10: -7.5601e+91 -7.5601e+91  8e+71  4e+75  1e+00\n",
      "11: -1.8306e+108 -1.8306e+108  2e+86  4e+91  1e+00\n",
      "12: -1.8123e+152 -3.9306e+157  4e+157 6e+135  2e+05\n",
      "13: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+03\n",
      "14: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+03\n",
      "15: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+03\n",
      "16: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+03\n",
      "17: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+03\n",
      "18: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+03\n",
      "19: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+03\n",
      "20: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+03\n",
      "21: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+03\n",
      "22: -1.8123e+152 -3.9324e+155  4e+155 3e+135  5e+03\n",
      "23: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+04\n",
      "24: -1.8123e+152 -3.9324e+155  4e+155 3e+135  9e+04\n",
      "25: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+05\n",
      "26: -1.8123e+152 -3.9324e+155  4e+155 3e+135  8e+05\n",
      "27: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+06\n",
      "28: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+07\n",
      "29: -1.8123e+152 -3.9324e+155  4e+155 3e+135  4e+07\n",
      "30: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+08\n",
      "31: -1.8123e+152 -3.9324e+155  4e+155 3e+135  7e+08\n",
      "32: -1.8123e+152 -3.9324e+155  4e+155 3e+135  4e+09\n",
      "33: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+10\n",
      "34: -1.8123e+152 -3.9324e+155  4e+155 3e+135  6e+10\n",
      "35: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+11\n",
      "36: -1.8123e+152 -3.9324e+155  4e+155 3e+135  7e+11\n",
      "37: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+12\n",
      "38: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+13\n",
      "39: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+13\n",
      "40: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+14\n",
      "41: -1.8123e+152 -3.9324e+155  4e+155 3e+135  5e+14\n",
      "42: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+15\n",
      "43: -1.8123e+152 -3.9324e+155  4e+155 3e+135  8e+15\n",
      "44: -1.8123e+152 -3.9324e+155  4e+155 3e+135  4e+16\n",
      "45: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+17\n",
      "46: -1.8123e+152 -3.9324e+155  4e+155 3e+135  5e+17\n",
      "47: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+18\n",
      "48: -1.8123e+152 -3.9324e+155  4e+155 3e+135  7e+18\n",
      "49: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+19\n",
      "50: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+20\n",
      "51: -1.8123e+152 -3.9324e+155  4e+155 3e+135  5e+20\n",
      "52: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+21\n",
      "53: -1.8123e+152 -3.9324e+155  4e+155 3e+135  4e+21\n",
      "54: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+22\n",
      "55: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+23\n",
      "56: -1.8123e+152 -3.9324e+155  4e+155 3e+135  4e+23\n",
      "57: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+24\n",
      "58: -1.8123e+152 -3.9324e+155  4e+155 3e+135  4e+24\n",
      "59: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+25\n",
      "60: -1.8123e+152 -3.9324e+155  4e+155 3e+135  7e+25\n",
      "61: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+26\n",
      "62: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+27\n",
      "63: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+27\n",
      "64: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+28\n",
      "65: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+28\n",
      "66: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+29\n",
      "67: -1.8123e+152 -3.9324e+155  4e+155 3e+135  4e+29\n",
      "68: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+30\n",
      "69: -1.8123e+152 -3.9324e+155  4e+155 3e+135  5e+30\n",
      "70: -1.8123e+152 -3.9324e+155  4e+155 3e+135  6e+31\n",
      "71: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+32\n",
      "72: -1.8123e+152 -3.9324e+155  4e+155 3e+135  8e+32\n",
      "73: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+33\n",
      "74: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+34\n",
      "75: -1.8123e+152 -3.9324e+155  4e+155 3e+135  4e+34\n",
      "76: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+35\n",
      "77: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+35\n",
      "78: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+36\n",
      "79: -1.8123e+152 -3.9324e+155  4e+155 3e+135  9e+36\n",
      "80: -1.8123e+152 -3.9324e+155  4e+155 3e+135  7e+37\n",
      "81: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+38\n",
      "82: -1.8123e+152 -3.9324e+155  4e+155 3e+135  5e+38\n",
      "83: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+39\n",
      "84: -1.8123e+152 -3.9324e+155  4e+155 3e+135  5e+39\n",
      "85: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+40\n",
      "86: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+41\n",
      "87: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+41\n",
      "88: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+42\n",
      "89: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+42\n",
      "90: -1.8123e+152 -3.9324e+155  4e+155 3e+135  3e+43\n",
      "91: -1.8123e+152 -3.9324e+155  4e+155 3e+135  8e+43\n",
      "92: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+44\n",
      "93: -1.8123e+152 -3.9324e+155  4e+155 3e+135  9e+44\n",
      "94: -1.8123e+152 -3.9324e+155  4e+155 3e+135  5e+45\n",
      "95: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+46\n",
      "96: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+47\n",
      "97: -1.8123e+152 -3.9324e+155  4e+155 3e+135  2e+47\n",
      "98: -1.8123e+152 -3.9324e+155  4e+155 3e+135  7e+47\n",
      "99: -1.8123e+152 -3.9324e+155  4e+155 3e+135  6e+48\n",
      "100: -1.8123e+152 -3.9324e+155  4e+155 3e+135  1e+49\n",
      "Terminated (maximum number of iterations reached).\n",
      "[ 2.41636415e+150  2.41636415e+150 -4.40620984e-005  2.41636415e+150\n",
      "  2.41636415e+150  2.41636415e+150  2.41636415e+150  2.41636415e+150\n",
      "  2.41636415e+150 -3.61650782e-005  2.41636415e+150  2.22381929e-004\n",
      "  2.41636415e+150  2.41636415e+150  2.41636415e+150 -4.34241544e-005\n",
      " -4.21205706e-005  2.41636415e+150  2.41636415e+150 -4.58340755e-005\n",
      "  2.41636415e+150  2.41636415e+150  2.41636415e+150  2.41636415e+150\n",
      "  2.41636415e+150 -4.01189247e-005  2.41636415e+150  2.41636415e+150\n",
      " -4.16221853e-005  2.41636415e+150 -3.22968276e-005  2.41636415e+150\n",
      " -3.03884721e-005  2.41636415e+150 -4.45609788e-005  2.41636415e+150\n",
      " -4.68620832e-005 -2.97446263e-005  2.41636415e+150  2.41636415e+150\n",
      "  2.41636415e+150  2.41636415e+150  2.24583711e-004 -4.15318331e-005\n",
      " -3.96264893e-005  2.41636415e+150 -4.65391690e-005  2.41636415e+150\n",
      " -3.38319397e-005 -3.61850454e-005  2.41636415e+150 -3.96571902e-005\n",
      "  2.41636415e+150 -4.57248950e-005  2.41636415e+150  2.41636415e+150\n",
      " -4.48161919e-005 -3.35144122e-005 -5.03538679e-005  2.41636415e+150\n",
      "  2.13278359e-004  2.41636415e+150  2.41636415e+150  2.41636415e+150\n",
      "  2.26856742e-004  2.41636415e+150 -3.40924024e-005  2.41636415e+150\n",
      " -2.65833565e-005  2.41636415e+150  2.41636415e+150  2.41636415e+150\n",
      "  2.41636415e+150  2.41636415e+150 -3.60071477e-005 -3.52984912e-005\n",
      "  2.41636415e+150 -3.56787024e-005  2.41636415e+150 -2.46402324e-005\n",
      " -4.19382043e-005  2.41636415e+150  2.41636415e+150 -3.65462624e-005\n",
      "  2.22880545e-004 -2.87115115e-005  2.41636415e+150 -3.57679163e-005\n",
      "  2.41636415e+150  2.41636415e+150  2.41636415e+150  2.11309479e-004\n",
      "  2.41636415e+150 -3.79019788e-005 -4.48223739e-005 -2.84230781e-005\n",
      " -2.74327189e-005  2.41636415e+150  2.41636415e+150  2.41636415e+150\n",
      " -3.87954203e-005 -4.08546309e-005 -3.36287733e-005 -3.96887366e-005\n",
      "  2.41636415e+150  2.16879253e-004 -3.86258925e-005  2.41636415e+150\n",
      "  2.12523542e-004  2.41636415e+150 -3.99652087e-005 -2.59148889e-005\n",
      "  2.41636415e+150  2.41636415e+150  2.41636415e+150 -3.61062325e-005\n",
      "  2.41636415e+150  2.41636415e+150  2.41636415e+150  2.16165595e-004\n",
      " -4.01236762e-005  2.41636415e+150 -4.18982542e-005 -4.36935131e-005\n",
      " -4.09636357e-005 -3.45989748e-005  2.41636415e+150 -3.88939963e-005\n",
      "  2.41636415e+150 -3.86740905e-005 -3.70031767e-005  2.41636415e+150\n",
      " -3.62329087e-005 -3.58910076e-005 -4.79530718e-005  2.41636415e+150\n",
      " -2.75173273e-005  2.41636415e+150  2.41636415e+150  2.23668344e-004\n",
      "  2.41636415e+150]\n"
     ]
    }
   ],
   "source": [
    "def kernel_svm(X, y): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)*1.\n",
    "    X_y = y*X\n",
    "    H = np.dot(X_y, X_y.T)*1.\n",
    "\n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    G = matrix(-np.eye(m))\n",
    "    h = matrix(np.zeros(m))\n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_svm(X_train, y_train)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] < 1e-7]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] < 1e-7:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 9, 15, 16, 19, 25, 28, 30, 32, 34, 36, 37, 43, 44, 46, 48, 49, 51, 53, 56, 57, 58, 66, 68, 74, 75, 77, 79, 80, 83, 85, 87, 93, 94, 95, 96, 100, 101, 102, 103, 106, 110, 111, 115, 120, 122, 123, 124, 125, 127, 129, 130, 132, 133, 134, 136]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 94.83568075117371%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended implementation using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.7875e+01 -2.4867e+00  6e+02  3e+01  3e-16\n",
      " 1: -9.7299e-01 -2.4678e+00  8e+00  3e-01  2e-16\n",
      " 2: -6.3553e-01 -1.4803e+00  9e-01  3e-03  3e-16\n",
      " 3: -7.3085e-01 -7.7093e-01  4e-02  1e-04  5e-16\n",
      " 4: -7.4981e-01 -7.5021e-01  4e-04  1e-06  3e-16\n",
      " 5: -7.5000e-01 -7.5000e-01  4e-06  1e-08  3e-16\n",
      " 6: -7.5000e-01 -7.5000e-01  4e-08  1e-10  2e-16\n",
      "Optimal solution found.\n",
      "[ 9.99999974e-03  9.99999974e-03  5.68122494e-26  9.99999974e-03\n",
      "  9.99999974e-03  9.99999974e-03  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03 -1.98842391e-26  9.99999974e-03 -3.45416816e-26\n",
      "  9.99999974e-03  9.99999974e-03  9.99999974e-03 -1.35688632e-25\n",
      " -2.25866111e-26  9.99999974e-03  9.99999974e-03 -2.09652939e-25\n",
      "  9.99999974e-03  9.99999974e-03  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03 -5.44459569e-26  9.99999974e-03  9.99999974e-03\n",
      " -8.67804406e-26  9.99999974e-03  4.29290755e-26  9.99999974e-03\n",
      "  2.85915634e-25  9.99999974e-03  2.21012742e-25  9.99999974e-03\n",
      " -2.04295425e-25  3.75861660e-26  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03  9.99999974e-03  8.88722352e-27 -2.52995371e-26\n",
      " -4.71086310e-26  9.99999974e-03 -2.51036315e-25  9.99999974e-03\n",
      " -2.90118650e-25 -1.85445170e-25  9.99999974e-03 -1.27536001e-25\n",
      "  9.99999974e-03 -1.62410655e-25  9.99999974e-03  9.99999974e-03\n",
      " -4.88046408e-26 -6.79078987e-27 -3.31369673e-26  9.99999974e-03\n",
      "  6.05100544e-26  9.99999974e-03  9.99999974e-03  9.99999974e-03\n",
      "  1.70759984e-25  9.99999974e-03  1.98035666e-25  9.99999974e-03\n",
      " -6.97244060e-26  9.99999974e-03  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03  9.99999974e-03 -5.33418763e-26  1.08721241e-25\n",
      "  9.99999974e-03 -7.36321611e-26  9.99999974e-03 -4.30186720e-26\n",
      " -3.10156885e-25  9.99999974e-03  9.99999974e-03 -3.86321532e-27\n",
      "  2.25375546e-26  4.35507352e-26  9.99999974e-03  1.11762353e-25\n",
      "  9.99999974e-03  9.99999974e-03  9.99999974e-03 -2.54955005e-26\n",
      "  9.99999974e-03  3.78011567e-26  2.24916860e-26 -9.17640408e-26\n",
      "  2.91046358e-25  9.99999974e-03  9.99999974e-03  9.99999974e-03\n",
      "  9.82474135e-26 -1.71227141e-25  4.94103146e-26 -4.63166701e-26\n",
      "  9.99999974e-03 -1.37104446e-25 -2.02831667e-25  9.99999974e-03\n",
      "  1.02577422e-25  9.99999974e-03  2.24029284e-25 -1.22227407e-25\n",
      "  9.99999974e-03  9.99999974e-03  9.99999974e-03  2.14157858e-26\n",
      "  9.99999974e-03  9.99999974e-03  9.99999974e-03  1.65871630e-25\n",
      "  8.76513668e-26  9.99999974e-03  2.40136285e-26  4.16365897e-27\n",
      " -1.65863780e-26  1.15689000e-25  9.99999974e-03 -9.12956334e-26\n",
      "  9.99999974e-03  3.15936018e-26 -2.15356559e-26  9.99999974e-03\n",
      " -4.04641083e-26  1.22891064e-25  1.09486830e-25  9.99999974e-03\n",
      "  1.75247564e-25  9.99999974e-03  9.99999974e-03  2.40707611e-25\n",
      "  9.99999974e-03]\n"
     ]
    }
   ],
   "source": [
    "def kernel_soft_margin_svm(X, y, C): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)*1.\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)*1.\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    \n",
    "    # Changed G and h\n",
    "    G = matrix(np.vstack((np.diag(np.ones(m))*-1, np.identity(m))))\n",
    "    h = matrix(np.hstack((np.zeros(m), np.ones(m)*C)))\n",
    "    \n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_soft_margin_svm(X_train, y_train, 0.01)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] < 0]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] < 0:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])\n",
    "\n",
    "# print(\"The following are support vectors: \")\n",
    "# for i in range(len(support_vectors)):\n",
    "#     print(support_vectors[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 11, 15, 16, 19, 25, 28, 36, 43, 44, 46, 48, 49, 51, 53, 56, 57, 58, 68, 74, 77, 79, 80, 83, 91, 95, 101, 103, 105, 106, 111, 124, 127, 130, 132]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 93.42723004694837%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
