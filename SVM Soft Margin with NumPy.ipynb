{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Soft Margin Extension with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from numpy import genfromtxt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from cvxopt import matrix, solvers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits=load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Scale training features\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X and y the subset of data that describe the numbers 8 and 9\n",
    "\n",
    "new_X = []\n",
    "new_y = []\n",
    "for i in range(len(X)):\n",
    "    if y[i] == 8:\n",
    "        new_X.append(X[i])\n",
    "        new_y.append(y[i])\n",
    "    elif y[i] == 9:\n",
    "        new_X.append(X[i])\n",
    "        new_y.append(y[i])\n",
    "new_X = np.array(new_X)\n",
    "new_y = np.array(new_y)\n",
    "\n",
    "X = new_X\n",
    "y = new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 64)\n",
      "(141,)\n",
      "(213, 64)\n",
      "(213,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8262e-02 -4.0188e-02  2e+02  1e+01  1e+00\n",
      " 1: -3.0905e-04 -1.1130e-04  2e+00  1e-01  1e-02\n",
      " 2: -1.8827e-05 -6.7504e-05  3e-02  2e-03  2e-04\n",
      " 3: -6.8873e-06 -1.1057e-05  1e-03  8e-05  7e-06\n",
      " 4: -1.2441e-06 -1.0286e-07  3e-05  3e-06  2e-07\n",
      " 5: -1.3608e-08 -1.0581e-11  4e-07  3e-08  2e-09\n",
      " 6: -1.3618e-10 -1.0581e-15  4e-09  3e-10  2e-11\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "def kernel_svm(X, y): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    G = matrix(-np.eye(m))\n",
    "    h = matrix(np.zeros(m))\n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_svm(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8262e-02 -4.0188e-02  2e+02  1e+01  1e+00\n",
      " 1: -3.0905e-04 -1.1130e-04  2e+00  1e-01  1e-02\n",
      " 2: -1.8827e-05 -6.7504e-05  3e-02  2e-03  2e-04\n",
      " 3: -6.8873e-06 -1.1057e-05  1e-03  8e-05  7e-06\n",
      " 4: -1.2441e-06 -1.0286e-07  3e-05  3e-06  2e-07\n",
      " 5: -1.3608e-08 -1.0581e-11  4e-07  3e-08  2e-09\n",
      " 6: -1.3618e-10 -1.0581e-15  4e-09  3e-10  2e-11\n",
      "Optimal solution found.\n",
      "[ 1.10008300e-11  2.09343171e-11 -1.52888491e-11 -1.67481751e-11\n",
      " -1.66363190e-11 -1.37071990e-11 -1.49454228e-11 -4.22863306e-12\n",
      " -1.65478139e-11  3.43433702e-11  2.60853523e-11  2.11618856e-11\n",
      " -1.54137825e-11 -1.85374511e-11 -1.44943188e-11  2.11612318e-11\n",
      "  2.58213534e-11 -1.36190907e-11 -1.72111703e-11 -1.74237986e-11\n",
      "  1.34051841e-11  8.87109839e-12 -1.49446968e-11 -1.58828641e-11\n",
      " -1.29457904e-11  1.39840867e-11  2.43037776e-11  2.68312148e-11\n",
      "  1.28495891e-11 -9.40838869e-12 -1.56115124e-11  2.20192537e-11\n",
      "  2.57802252e-11 -1.64524268e-11  1.20248647e-11  8.71851732e-12\n",
      " -1.78152256e-11 -1.54681937e-11 -1.69369660e-11 -1.56928018e-11\n",
      "  2.03017181e-11 -1.24168815e-11 -1.43429212e-11 -1.74064062e-11\n",
      " -1.23124636e-11  1.58690658e-11  1.82075663e-11 -1.62947767e-11\n",
      " -1.34383917e-11 -1.52778117e-11  7.37365738e-12 -1.66344752e-11\n",
      " -8.22997012e-12  1.97973577e-11  2.69518566e-11  6.40280902e-12\n",
      " -1.71901933e-11  1.28503643e-11  1.63929985e-11 -1.57618011e-11\n",
      "  2.71303860e-11  1.99909544e-11 -1.73503978e-11  2.52904980e-11\n",
      "  1.99579878e-11  1.22635226e-11 -1.34214430e-11  1.15973612e-11\n",
      " -1.54818477e-11  7.68398663e-12 -1.33721134e-11 -1.68019612e-11\n",
      " -1.48943797e-11  2.17831208e-11 -1.54976821e-11  3.13349112e-11\n",
      "  2.56203874e-11 -6.28125277e-12 -1.63494742e-11  1.15461943e-11\n",
      " -1.67916025e-11 -1.28148013e-11 -1.35935249e-11  1.57732604e-11\n",
      " -1.45889966e-11 -1.63632483e-11  2.02736469e-11 -1.26380473e-11\n",
      " -1.59980004e-11  1.11674308e-11 -1.81081316e-11  8.90566068e-12\n",
      " -1.72148103e-11 -1.49202229e-11 -1.20928445e-11  1.79762620e-11\n",
      "  5.02318619e-12 -1.52595436e-11 -1.51287373e-11 -1.71038656e-11\n",
      " -1.49003651e-11 -1.67085679e-11 -1.16996227e-11  2.26865624e-12\n",
      "  1.42072441e-11 -1.45200759e-11 -1.31442462e-11  2.26392871e-11\n",
      "  1.02259334e-11 -1.68209265e-11  3.50351526e-11 -1.57652280e-11\n",
      " -1.51828468e-11 -1.28634052e-11  2.97131310e-11 -1.23449569e-11\n",
      " -1.67385582e-11  1.90029100e-11  1.96035027e-11 -1.65791575e-11\n",
      "  1.70545745e-11  1.40021414e-11 -1.50971030e-11 -1.72352585e-11\n",
      " -1.48421173e-11  3.00695515e-11  1.27773135e-11  2.52269326e-11\n",
      "  1.17253568e-11  1.23490463e-12  1.18373556e-11  1.03972402e-11\n",
      " -1.76934452e-11  2.25759292e-11  2.24887951e-11  3.15339638e-11\n",
      "  1.00249723e-11  2.07633857e-11  1.14852360e-11  2.19285586e-11\n",
      "  3.10691849e-11]\n"
     ]
    }
   ],
   "source": [
    "def kernel_svm(X, y): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)*1.\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    G = matrix(-np.eye(m))\n",
    "    h = matrix(np.zeros(m))\n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_svm(X_train, y_train)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] > 1e-12]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] > 1e-12:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 9, 10, 11, 15, 16, 20, 21, 25, 26, 27, 28, 31, 32, 34, 35, 40, 45, 46, 50, 53, 54, 55, 57, 58, 60, 61, 63, 64, 65, 67, 69, 73, 75, 76, 79, 83, 86, 89, 91, 95, 96, 103, 104, 107, 108, 110, 114, 117, 118, 120, 121, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 8\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 79.81220657276995%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended implementation using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.9163e-02 -1.4487e-01  3e+02  2e+01  2e-14\n",
      " 1: -2.5634e-04 -1.4459e-01  3e+00  2e-01  3e-14\n",
      " 2: -6.1243e-05 -1.2071e-01  2e-01  3e-03  6e-16\n",
      " 3: -6.9198e-06 -7.9138e-03  9e-03  1e-04  5e-16\n",
      " 4: -1.2140e-06 -2.9272e-04  3e-04  4e-06  5e-16\n",
      " 5: -1.3375e-08 -2.9688e-06  3e-06  4e-08  8e-16\n",
      " 6: -1.3386e-10 -2.9688e-08  3e-08  4e-10  6e-16\n",
      "Optimal solution found.\n",
      "[ 1.24120257e-11  1.41890516e-11 -1.60729212e-11 -1.64779020e-11\n",
      " -1.68332696e-11 -1.38694933e-11 -1.63472005e-11  1.16480750e-12\n",
      " -1.73180058e-11  2.89400710e-11  1.99031890e-11  1.40888734e-11\n",
      " -1.68585770e-11 -1.64294551e-11 -1.37141364e-11  2.23473821e-11\n",
      "  1.71598390e-11 -1.41454501e-11 -1.79452898e-11 -1.69506435e-11\n",
      "  1.31399772e-11  2.09989902e-11 -1.51620537e-11 -1.34305108e-11\n",
      " -1.57734870e-11  1.53280444e-11  2.22223175e-11  2.63242526e-11\n",
      "  1.81660315e-11 -1.14676542e-11 -1.68604848e-11  1.69591847e-11\n",
      "  2.17654599e-11 -1.43833269e-11  1.00316635e-11  1.28877874e-11\n",
      " -1.59884621e-11 -9.63671357e-12 -1.74611271e-11 -1.54938521e-11\n",
      "  1.53312823e-11 -1.15140361e-11 -1.43263183e-11 -1.73706892e-11\n",
      " -1.37074448e-11  1.59091183e-11  2.23782688e-11 -1.57104332e-11\n",
      " -1.51715670e-11 -1.60040961e-11  8.74232572e-12 -1.73637269e-11\n",
      " -1.37595912e-11  1.90241551e-11  2.06728313e-11  1.18520582e-11\n",
      " -1.60854878e-11  1.68399101e-11  1.10334009e-11 -1.27176316e-11\n",
      "  1.79442399e-11  1.99035747e-11 -1.46206142e-11  2.68412295e-11\n",
      "  1.43790949e-11  1.84942540e-11 -7.28013118e-12  9.91118230e-12\n",
      " -1.23464212e-11  1.45607668e-11 -1.46925189e-11 -1.67054114e-11\n",
      " -1.63250300e-11  1.66355663e-11 -1.80375351e-11  2.32564412e-11\n",
      "  2.44237126e-11  2.12203497e-13 -1.55682210e-11  1.15551962e-11\n",
      " -1.47228204e-11 -1.49228194e-11 -8.83571207e-12  2.09944131e-11\n",
      " -1.62993547e-11 -1.41219308e-11  1.91293483e-11 -1.33859903e-11\n",
      " -1.32418260e-11  1.06775489e-11 -1.69380129e-11  1.96025769e-11\n",
      " -1.60564785e-11 -1.52149685e-11 -1.21452826e-11  1.99397904e-11\n",
      "  9.49245463e-12 -1.59865878e-11 -1.60839841e-11 -1.60822148e-11\n",
      " -1.87029978e-11 -1.59132767e-11 -1.02730199e-11  7.09328136e-12\n",
      "  1.57897805e-11 -1.52217500e-11 -1.36115580e-11  2.08840707e-11\n",
      "  9.14588174e-12 -1.72683632e-11  3.16808857e-11 -1.62764886e-11\n",
      " -1.46348273e-11 -1.32206605e-11  1.79433688e-11 -1.54019243e-11\n",
      " -1.73207021e-11  2.75079244e-11  2.26857054e-11 -1.59187491e-11\n",
      "  2.11492793e-11  1.36684788e-11 -1.65482048e-11 -1.80698645e-11\n",
      " -1.46765160e-11  2.67066334e-11  1.83201579e-11  2.02896952e-11\n",
      "  1.17458222e-11  3.48511115e-12  1.51631825e-11  1.59480380e-11\n",
      " -1.72260369e-11  1.98831117e-11  2.21420761e-11  2.16272664e-11\n",
      "  9.21545646e-12  1.62418903e-11  1.79111906e-11  2.99245162e-11\n",
      "  2.21902509e-11]\n"
     ]
    }
   ],
   "source": [
    "def kernel_soft_margin_svm(X, y, C): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    \n",
    "    # Changed G and h\n",
    "    G = matrix(np.vstack((np.diag(np.ones(m))*-1, np.identity(m))))\n",
    "    h = matrix(np.hstack((np.zeros(m), np.ones(m)*C)))\n",
    "    \n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_soft_margin_svm(X_train, y_train, 0.001)\n",
    "\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] > 1e-12]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] > 1e-12:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])\n",
    "\n",
    "# print(\"The following are support vectors: \")\n",
    "# for i in range(len(support_vectors)):\n",
    "#     print(support_vectors[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 7, 9, 10, 11, 15, 16, 20, 21, 25, 26, 27, 28, 31, 32, 34, 35, 40, 45, 46, 50, 53, 54, 55, 57, 58, 60, 61, 63, 64, 65, 67, 69, 73, 75, 76, 79, 83, 86, 89, 91, 95, 96, 103, 104, 107, 108, 110, 114, 117, 118, 120, 121, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 8\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 84.03755868544602%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainX\n",
    "y_train = trainY\n",
    "X_test = testX\n",
    "y_test = testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X_train and y_train the subset of data that describe the labels 0 and 2 (T-shirts and pullovers, respectively)\n",
    "\n",
    "new_X_train = []\n",
    "new_y_train = []\n",
    "for i in range(len(X_train)):\n",
    "    if y_train[i] == 0:\n",
    "        new_X_train.append(X_train[i])\n",
    "        new_y_train.append(y_train[i])\n",
    "    elif y_train[i] == 2:\n",
    "        new_X_train.append(X_train[i])\n",
    "        new_y_train.append(y_train[i])\n",
    "new_X_train = np.array(new_X_train)\n",
    "new_y_train = np.array(new_y_train)\n",
    "\n",
    "X_train = new_X_train\n",
    "y_train = new_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X_test and y_test the subset of data that describe the labels 0 and 2 (T-shirts and pullovers, respectively)\n",
    "\n",
    "new_X_test = []\n",
    "new_y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    if y_test[i] == 0:\n",
    "        new_X_test.append(X_test[i])\n",
    "        new_y_test.append(y_test[i])\n",
    "    elif y_test[i] == 2:\n",
    "        new_X_test.append(X_test[i])\n",
    "        new_y_test.append(y_test[i])\n",
    "new_X_test = np.array(new_X_test)\n",
    "new_y_test = np.array(new_y_test)\n",
    "\n",
    "X_test = new_X_test\n",
    "y_test = new_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784)\n",
      "(12000,)\n",
      "(2000, 784)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([X_train[i].flatten() for i in range(len(X_train))])\n",
    "X_test = np.array([X_test[i].flatten() for i in range(len(X_test))])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 784)\n",
      "(141,)\n",
      "(213, 784)\n",
      "(213,)\n"
     ]
    }
   ],
   "source": [
    "# Downsample the data\n",
    "\n",
    "# Add y_train back as an additional column to X_train\n",
    "y_train = y_train.reshape((-1,1))\n",
    "X_train = np.append(X_train, y_train, axis=1)\n",
    "\n",
    "# Add y_test back as an additional column to X_test\n",
    "y_test = y_test.reshape((-1,1))\n",
    "X_test = np.append(X_test, y_test, axis=1)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(X_train)\n",
    "np.random.shuffle(X_test)\n",
    "\n",
    "# Slice out only the first 141 from X_train and 213 from X_test\n",
    "X_train = X_train[0:141]\n",
    "X_test = X_test[0:213]\n",
    "\n",
    "# Remove the last columns of X_train and X_test and place them back into y_train and y_test\n",
    "y_train = X_train[:,-1]\n",
    "y_test = X_test[:,-1]\n",
    "X_train = X_train[:,0:X_train.shape[1]-1]\n",
    "X_test = X_test[:,0:X_test.shape[1]-1]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset\n",
    "\n",
    "X_scale = StandardScaler()\n",
    "X_train = X_scale.fit_transform(X_train) \n",
    "X_test = X_scale.fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.8000e+01 -1.3600e+02  3e+02  1e+01  2e+00\n",
      " 1: -3.0479e+02 -3.0784e+02  8e+01  6e+00  1e+00\n",
      " 2: -3.4242e+04 -3.4246e+04  8e+01  6e+00  1e+00\n",
      " 3: -3.3940e+08 -3.3940e+08  4e+02  6e+00  1e+00\n",
      " 4: -3.3601e+14 -3.3601e+14  3e+06  6e+00  1e+00\n",
      " 5: -3.3265e+22 -3.3265e+22  3e+12  1e+06  1e+00\n",
      " 6: -3.2932e+32 -3.2932e+32  3e+20  9e+15  1e+00\n",
      " 7: -3.2603e+44 -3.2603e+44  3e+30  4e+00  1e+00\n",
      " 8: -3.2126e+58 -3.2126e+58  3e+42  4e+00  1e+00\n",
      " 9: -5.4955e+74 -5.4955e+74  5e+56  4e+00  1e+00\n",
      "10: -1.0166e+91 -1.0166e+91  1e+71  4e+00  1e+00\n",
      "11: -4.4480e+106 -4.4480e+106  4e+84  4e+00  1e+00\n",
      "12: -4.4035e+150 -2.7145e+156  3e+156 9e+133  4e+05\n",
      "13: -4.4035e+150 -2.7150e+154  3e+154 9e+133  4e+03\n",
      "14: -4.4049e+150 -2.7595e+152  3e+152 9e+133  4e+01\n",
      "15: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+01\n",
      "16: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+01\n",
      "17: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+01\n",
      "18: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+01\n",
      "19: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+01\n",
      "20: -4.4049e+150 -2.7595e+152  3e+152 2e+134  5e+01\n",
      "21: -4.4049e+150 -2.7595e+152  3e+152 2e+134  5e+01\n",
      "22: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+02\n",
      "23: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+02\n",
      "24: -4.4049e+150 -2.7595e+152  3e+152 2e+134  7e+02\n",
      "25: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+03\n",
      "26: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+04\n",
      "27: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+04\n",
      "28: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+05\n",
      "29: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+05\n",
      "30: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+06\n",
      "31: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+06\n",
      "32: -4.4049e+150 -2.7595e+152  3e+152 2e+134  7e+06\n",
      "33: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+07\n",
      "34: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+08\n",
      "35: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+08\n",
      "36: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+09\n",
      "37: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+09\n",
      "38: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+09\n",
      "39: -4.4049e+150 -2.7595e+152  3e+152 2e+134  9e+09\n",
      "40: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+11\n",
      "41: -4.4049e+150 -2.7595e+152  3e+152 2e+134  9e+10\n",
      "42: -4.4049e+150 -2.7595e+152  3e+152 2e+134  8e+11\n",
      "43: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+12\n",
      "44: -4.4049e+150 -2.7595e+152  3e+152 2e+134  5e+12\n",
      "45: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+13\n",
      "46: -4.4049e+150 -2.7595e+152  3e+152 2e+134  5e+13\n",
      "47: -4.4049e+150 -2.7595e+152  3e+152 2e+134  9e+13\n",
      "48: -4.4049e+150 -2.7595e+152  3e+152 2e+134  5e+14\n",
      "49: -4.4049e+150 -2.7595e+152  3e+152 2e+134  8e+14\n",
      "50: -4.4049e+150 -2.7595e+152  3e+152 2e+134  7e+15\n",
      "51: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+16\n",
      "52: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+16\n",
      "53: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+17\n",
      "54: -4.4049e+150 -2.7595e+152  3e+152 2e+134  9e+17\n",
      "55: -4.4049e+150 -2.7595e+152  3e+152 2e+134  9e+17\n",
      "56: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+18\n",
      "57: -4.4049e+150 -2.7595e+152  3e+152 2e+134  5e+18\n",
      "58: -4.4049e+150 -2.7595e+152  3e+152 2e+134  5e+19\n",
      "59: -4.4049e+150 -2.7595e+152  3e+152 2e+134  6e+19\n",
      "60: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+20\n",
      "61: -4.4049e+150 -2.7595e+152  3e+152 2e+134  6e+20\n",
      "62: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+21\n",
      "63: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+21\n",
      "64: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+22\n",
      "65: -4.4049e+150 -2.7595e+152  3e+152 2e+134  6e+22\n",
      "66: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+23\n",
      "67: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+23\n",
      "68: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+24\n",
      "69: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+24\n",
      "70: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+25\n",
      "71: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+25\n",
      "72: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+26\n",
      "73: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+26\n",
      "74: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+27\n",
      "75: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+27\n",
      "76: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+28\n",
      "77: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+28\n",
      "78: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+29\n",
      "79: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+29\n",
      "80: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+30\n",
      "81: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+30\n",
      "82: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+31\n",
      "83: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+31\n",
      "84: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+32\n",
      "85: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+32\n",
      "86: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+33\n",
      "87: -4.4049e+150 -2.7595e+152  3e+152 2e+134  3e+33\n",
      "88: -4.4049e+150 -2.7595e+152  3e+152 2e+134  6e+33\n",
      "89: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+34\n",
      "90: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+35\n",
      "91: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+35\n",
      "92: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+36\n",
      "93: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+36\n",
      "94: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+37\n",
      "95: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+37\n",
      "96: -4.4049e+150 -2.7595e+152  3e+152 2e+134  1e+38\n",
      "97: -4.4049e+150 -2.7595e+152  3e+152 2e+134  2e+38\n",
      "98: -4.4049e+150 -2.7595e+152  3e+152 2e+134  4e+38\n",
      "99: -4.4049e+150 -2.7595e+152  3e+152 2e+134  8e+38\n",
      "100: -4.4049e+150 -2.7595e+152  3e+152 2e+134  8e+39\n",
      "Terminated (maximum number of iterations reached).\n",
      "[-8.11965148e-008  6.47783180e+148  6.47783180e+148 -8.17739255e-008\n",
      "  6.47783180e+148 -8.28595318e-008  6.47783180e+148  6.47783180e+148\n",
      "  6.47783180e+148  6.47783180e+148  6.47783180e+148  6.47783180e+148\n",
      "  3.24557997e-007 -7.95181403e-008  6.47783180e+148  6.47783180e+148\n",
      " -8.10590798e-008 -8.88553095e-008 -8.24167382e-008  3.29507345e-007\n",
      " -8.27346545e-008 -7.94860581e-008  6.47783180e+148  6.47783180e+148\n",
      " -7.90862480e-008  6.47783180e+148  6.47783180e+148 -7.97981118e-008\n",
      " -7.88087687e-008 -7.45687391e-008  6.47783180e+148  6.47783180e+148\n",
      "  6.47783180e+148 -8.66891037e-008  6.47783180e+148  6.47783180e+148\n",
      " -8.03978814e-008  6.47783180e+148  3.35047052e-007  3.33179954e-007\n",
      "  3.20097676e-007 -8.87911286e-008 -8.19228534e-008  6.47783180e+148\n",
      "  6.47783180e+148 -7.74929497e-008  3.34351260e-007  6.47783180e+148\n",
      "  6.47783180e+148  6.47783180e+148  6.47783180e+148  6.47783180e+148\n",
      "  6.47783180e+148  3.25076779e-007  6.47783180e+148 -8.44735173e-008\n",
      "  6.47783180e+148  6.47783180e+148 -9.42986080e-008  6.47783180e+148\n",
      "  6.47783180e+148 -8.28074957e-008 -8.23357555e-008 -6.80794478e-008\n",
      " -7.64868535e-008  3.33730505e-007  6.47783180e+148  6.47783180e+148\n",
      " -7.99676133e-008  6.47783180e+148 -7.36618595e-008 -4.93734116e-007\n",
      "  6.47783180e+148 -7.08757593e-008 -8.69176901e-008 -8.24764626e-008\n",
      "  6.47783180e+148  3.30609422e-007 -8.70546761e-008  6.47783180e+148\n",
      " -4.81056585e-007 -8.73140621e-008  6.47783180e+148  6.47783180e+148\n",
      "  6.47783180e+148 -8.34021682e-008  6.47783180e+148  6.47783180e+148\n",
      "  6.47783180e+148 -6.98636828e-008  6.47783180e+148  6.47783180e+148\n",
      " -8.94702349e-008 -8.58136283e-008 -8.93993046e-008  6.47783180e+148\n",
      "  6.47783180e+148 -8.34914257e-008 -9.19250183e-008  6.47783180e+148\n",
      " -8.67955223e-008 -8.54384968e-008 -8.67266089e-008 -7.93406525e-008\n",
      "  6.47783180e+148 -8.24223833e-008  6.47783180e+148  6.47783180e+148\n",
      "  6.47783180e+148 -9.00797743e-008  6.47783180e+148 -7.43646976e-008\n",
      "  6.47783180e+148 -8.06625838e-008  6.47783180e+148  3.29269769e-007\n",
      "  6.47783180e+148 -6.82946679e-008  6.47783180e+148  6.47783180e+148\n",
      "  6.47783180e+148  6.47783180e+148  6.47783180e+148  6.47783180e+148\n",
      " -8.44862774e-008 -9.04235472e-008  1.35347217e-007 -7.89082823e-008\n",
      "  3.28710102e-007  6.47783180e+148 -6.75711397e-008 -9.59077917e-008\n",
      "  3.25175299e-007  1.27425741e-007  6.47783180e+148  1.24666447e-007\n",
      "  3.15687340e-007  6.47783180e+148 -8.17244304e-008 -8.21698981e-008\n",
      "  3.25727132e-007]\n"
     ]
    }
   ],
   "source": [
    "def kernel_svm(X, y): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)*1.\n",
    "    X_y = y*X\n",
    "    H = np.dot(X_y, X_y.T)*1.\n",
    "\n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    G = matrix(-np.eye(m))\n",
    "    h = matrix(np.zeros(m))\n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_svm(X_train, y_train)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] < 1e-7]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] < 1e-7:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 5, 13, 16, 17, 18, 20, 21, 24, 27, 28, 29, 33, 36, 41, 42, 45, 55, 58, 61, 62, 63, 64, 68, 70, 71, 73, 74, 75, 78, 80, 81, 85, 89, 92, 93, 94, 97, 98, 100, 101, 102, 103, 105, 109, 111, 113, 117, 124, 125, 127, 130, 131, 138, 139]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 87.32394366197182%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended implementation using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.4340e+01 -2.4513e+00  6e+02  3e+01  3e-16\n",
      " 1: -8.8217e-01 -2.4317e+00  8e+00  3e-01  5e-16\n",
      " 2: -5.7624e-01 -1.4136e+00  9e-01  3e-03  4e-16\n",
      " 3: -6.6265e-01 -7.0241e-01  4e-02  1e-04  3e-16\n",
      " 4: -6.7983e-01 -6.8023e-01  4e-04  1e-06  2e-16\n",
      " 5: -6.8000e-01 -6.8000e-01  4e-06  1e-08  3e-16\n",
      " 6: -6.8000e-01 -6.8000e-01  4e-08  1e-10  3e-16\n",
      "Optimal solution found.\n",
      "[-8.56293230e-28  9.99999974e-03  9.99999974e-03  6.50534043e-27\n",
      "  9.99999974e-03  8.48243603e-26  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03  9.99999974e-03  9.99999974e-03  9.99999974e-03\n",
      "  5.27602331e-26 -5.19956758e-27  9.99999974e-03  9.99999974e-03\n",
      "  2.69652899e-26  3.73005360e-26 -8.61825824e-26  1.90149523e-26\n",
      "  3.89815599e-26  9.86331063e-26  9.99999974e-03  9.99999974e-03\n",
      " -1.46910059e-25  9.99999974e-03  9.99999974e-03  8.50029001e-26\n",
      " -1.67609057e-25  2.63621268e-26  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03 -1.13665882e-25  9.99999974e-03  9.99999974e-03\n",
      " -8.79968451e-26  9.99999974e-03 -8.30789932e-26 -5.66148942e-26\n",
      " -4.63937830e-26  3.48697481e-26 -8.45658234e-26  9.99999974e-03\n",
      "  9.99999974e-03  3.48702984e-26 -1.12356445e-25  9.99999974e-03\n",
      "  9.99999974e-03  9.99999974e-03  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03  1.05375464e-25  9.99999974e-03 -4.16300749e-27\n",
      "  9.99999974e-03  9.99999974e-03 -2.60094731e-25  9.99999974e-03\n",
      "  9.99999974e-03  6.33250265e-26 -4.18617909e-26  6.33412405e-26\n",
      "  5.39350995e-26  1.71960906e-25  9.99999974e-03  9.99999974e-03\n",
      "  1.32509896e-26  9.99999974e-03  6.54092008e-26 -1.11357480e-26\n",
      "  9.99999974e-03  7.84809392e-26 -1.02046958e-26 -1.51824078e-25\n",
      "  9.99999974e-03  1.48215793e-25 -1.96942959e-25  9.99999974e-03\n",
      "  1.31425160e-25  6.27659941e-27  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03 -4.56544556e-26  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03 -6.05774757e-27  9.99999974e-03  9.99999974e-03\n",
      "  5.13999186e-26  1.16404603e-26 -6.49630910e-26  9.99999974e-03\n",
      "  9.99999974e-03  2.50376541e-25  7.62364576e-26  9.99999974e-03\n",
      "  3.05962292e-26 -6.07498815e-26 -7.19289816e-26 -2.92219453e-26\n",
      "  9.99999974e-03  6.47576764e-26  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03  1.06632373e-25  9.99999974e-03 -7.53290802e-26\n",
      "  9.99999974e-03 -4.90216918e-26  9.99999974e-03  7.41324151e-26\n",
      "  9.99999974e-03  4.66509752e-26  9.99999974e-03  9.99999974e-03\n",
      "  9.99999974e-03  9.99999974e-03  9.99999974e-03  9.99999974e-03\n",
      "  9.59716108e-26 -1.23103949e-25  1.63056476e-26 -2.66597475e-26\n",
      "  8.52013479e-26  9.99999974e-03 -7.45078966e-26  6.73242834e-26\n",
      " -6.48330388e-26 -6.34464579e-27  9.99999974e-03 -8.54211156e-26\n",
      "  1.54801358e-25  9.99999974e-03  7.02906370e-27  7.87676613e-26\n",
      " -4.33388591e-25]\n"
     ]
    }
   ],
   "source": [
    "def kernel_soft_margin_svm(X, y, C): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)*1.\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)*1.\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    \n",
    "    # Changed G and h\n",
    "    G = matrix(np.vstack((np.diag(np.ones(m))*-1, np.identity(m))))\n",
    "    h = matrix(np.hstack((np.zeros(m), np.ones(m)*C)))\n",
    "    \n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_soft_margin_svm(X_train, y_train, 0.01)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] < 0]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] < 0:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])\n",
    "\n",
    "# print(\"The following are support vectors: \")\n",
    "# for i in range(len(support_vectors)):\n",
    "#     print(support_vectors[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 13, 18, 24, 28, 33, 36, 38, 39, 40, 42, 46, 55, 58, 62, 71, 74, 75, 78, 85, 89, 94, 101, 102, 103, 111, 113, 125, 127, 130, 132, 133, 135, 140]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 89.67136150234741%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
