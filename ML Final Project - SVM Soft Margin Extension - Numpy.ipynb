{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - SVM Soft Margin Extension - Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from numpy import genfromtxt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from cvxopt import matrix, solvers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits=load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Scale training features\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X and y the subset of data that describe the numbers 8 and 9\n",
    "\n",
    "new_X = []\n",
    "new_y = []\n",
    "for i in range(len(X)):\n",
    "    if y[i] == 8:\n",
    "        new_X.append(X[i])\n",
    "        new_y.append(y[i])\n",
    "    elif y[i] == 9:\n",
    "        new_X.append(X[i])\n",
    "        new_y.append(y[i])\n",
    "new_X = np.array(new_X)\n",
    "new_y = np.array(new_y)\n",
    "\n",
    "X = new_X\n",
    "y = new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 64)\n",
      "(141,)\n",
      "(213, 64)\n",
      "(213,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8262e-02 -4.0188e-02  2e+02  1e+01  1e+00\n",
      " 1: -3.0905e-04 -1.1130e-04  2e+00  1e-01  1e-02\n",
      " 2: -1.8827e-05 -6.7504e-05  3e-02  2e-03  2e-04\n",
      " 3: -6.8873e-06 -1.1057e-05  1e-03  8e-05  7e-06\n",
      " 4: -1.2441e-06 -1.0286e-07  3e-05  3e-06  2e-07\n",
      " 5: -1.3608e-08 -1.0581e-11  4e-07  3e-08  2e-09\n",
      " 6: -1.3618e-10 -1.0581e-15  4e-09  3e-10  2e-11\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "def kernel_svm(X, y): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    G = matrix(-np.eye(m))\n",
    "    h = matrix(np.zeros(m))\n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_svm(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8262e-02 -4.0188e-02  2e+02  1e+01  1e+00\n",
      " 1: -3.0905e-04 -1.1130e-04  2e+00  1e-01  1e-02\n",
      " 2: -1.8827e-05 -6.7504e-05  3e-02  2e-03  2e-04\n",
      " 3: -6.8873e-06 -1.1057e-05  1e-03  8e-05  7e-06\n",
      " 4: -1.2441e-06 -1.0286e-07  3e-05  3e-06  2e-07\n",
      " 5: -1.3608e-08 -1.0581e-11  4e-07  3e-08  2e-09\n",
      " 6: -1.3618e-10 -1.0581e-15  4e-09  3e-10  2e-11\n",
      "Optimal solution found.\n",
      "[ 1.10008300e-11  2.09343171e-11 -1.52888491e-11 -1.67481751e-11\n",
      " -1.66363190e-11 -1.37071990e-11 -1.49454228e-11 -4.22863306e-12\n",
      " -1.65478139e-11  3.43433702e-11  2.60853523e-11  2.11618856e-11\n",
      " -1.54137825e-11 -1.85374511e-11 -1.44943188e-11  2.11612318e-11\n",
      "  2.58213534e-11 -1.36190907e-11 -1.72111703e-11 -1.74237986e-11\n",
      "  1.34051841e-11  8.87109839e-12 -1.49446968e-11 -1.58828641e-11\n",
      " -1.29457904e-11  1.39840867e-11  2.43037776e-11  2.68312148e-11\n",
      "  1.28495891e-11 -9.40838869e-12 -1.56115124e-11  2.20192537e-11\n",
      "  2.57802252e-11 -1.64524268e-11  1.20248647e-11  8.71851732e-12\n",
      " -1.78152256e-11 -1.54681937e-11 -1.69369660e-11 -1.56928018e-11\n",
      "  2.03017181e-11 -1.24168815e-11 -1.43429212e-11 -1.74064062e-11\n",
      " -1.23124636e-11  1.58690658e-11  1.82075663e-11 -1.62947767e-11\n",
      " -1.34383917e-11 -1.52778117e-11  7.37365738e-12 -1.66344752e-11\n",
      " -8.22997012e-12  1.97973577e-11  2.69518566e-11  6.40280902e-12\n",
      " -1.71901933e-11  1.28503643e-11  1.63929985e-11 -1.57618011e-11\n",
      "  2.71303860e-11  1.99909544e-11 -1.73503978e-11  2.52904980e-11\n",
      "  1.99579878e-11  1.22635226e-11 -1.34214430e-11  1.15973612e-11\n",
      " -1.54818477e-11  7.68398663e-12 -1.33721134e-11 -1.68019612e-11\n",
      " -1.48943797e-11  2.17831208e-11 -1.54976821e-11  3.13349112e-11\n",
      "  2.56203874e-11 -6.28125277e-12 -1.63494742e-11  1.15461943e-11\n",
      " -1.67916025e-11 -1.28148013e-11 -1.35935249e-11  1.57732604e-11\n",
      " -1.45889966e-11 -1.63632483e-11  2.02736469e-11 -1.26380473e-11\n",
      " -1.59980004e-11  1.11674308e-11 -1.81081316e-11  8.90566068e-12\n",
      " -1.72148103e-11 -1.49202229e-11 -1.20928445e-11  1.79762620e-11\n",
      "  5.02318619e-12 -1.52595436e-11 -1.51287373e-11 -1.71038656e-11\n",
      " -1.49003651e-11 -1.67085679e-11 -1.16996227e-11  2.26865624e-12\n",
      "  1.42072441e-11 -1.45200759e-11 -1.31442462e-11  2.26392871e-11\n",
      "  1.02259334e-11 -1.68209265e-11  3.50351526e-11 -1.57652280e-11\n",
      " -1.51828468e-11 -1.28634052e-11  2.97131310e-11 -1.23449569e-11\n",
      " -1.67385582e-11  1.90029100e-11  1.96035027e-11 -1.65791575e-11\n",
      "  1.70545745e-11  1.40021414e-11 -1.50971030e-11 -1.72352585e-11\n",
      " -1.48421173e-11  3.00695515e-11  1.27773135e-11  2.52269326e-11\n",
      "  1.17253568e-11  1.23490463e-12  1.18373556e-11  1.03972402e-11\n",
      " -1.76934452e-11  2.25759292e-11  2.24887951e-11  3.15339638e-11\n",
      "  1.00249723e-11  2.07633857e-11  1.14852360e-11  2.19285586e-11\n",
      "  3.10691849e-11]\n"
     ]
    }
   ],
   "source": [
    "def kernel_svm(X, y): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)*1.\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    G = matrix(-np.eye(m))\n",
    "    h = matrix(np.zeros(m))\n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_svm(X_train, y_train)\n",
    "\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] > 1e-12]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] > 1e-12:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 9, 10, 11, 15, 16, 20, 21, 25, 26, 27, 28, 31, 32, 34, 35, 40, 45, 46, 50, 53, 54, 55, 57, 58, 60, 61, 63, 64, 65, 67, 69, 73, 75, 76, 79, 83, 86, 89, 91, 95, 96, 103, 104, 107, 108, 110, 114, 117, 118, 120, 121, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 8\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 79.81220657276995%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended implementation using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.1056e-02 -1.4523e+01  3e+02  1e+01  3e-14\n",
      " 1: -3.6794e-03 -1.2104e+01  2e+01  2e-01  3e-14\n",
      " 2: -1.3539e-05 -4.4261e-01  5e-01  4e-03  5e-15\n",
      " 3: -6.7445e-06 -1.3705e-02  1e-02  1e-04  6e-16\n",
      " 4: -1.4392e-06 -5.3602e-04  6e-04  4e-06  5e-16\n",
      " 5: -1.6065e-08 -5.4504e-06  6e-06  4e-08  8e-16\n",
      " 6: -1.6080e-10 -5.4504e-08  6e-08  4e-10  8e-16\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "def kernel_soft_margin_svm(X, y, C): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    \n",
    "    # Changed G and h\n",
    "    G = matrix(np.vstack((np.diag(np.ones(m))*-1, np.identity(m))))\n",
    "    h = matrix(np.hstack((np.zeros(m), np.ones(m)*C)))\n",
    "    \n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_soft_margin_svm(X_train, y_train, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.36585483e-11  2.27506384e-11 -1.92366639e-11 -1.98397047e-11\n",
      " -2.00293069e-11 -1.61368576e-11 -1.81284643e-11  2.00850294e-12\n",
      " -2.03122376e-11  4.36445838e-11  2.90944943e-11  2.33602023e-11\n",
      " -1.95539561e-11 -2.12733573e-11 -1.69055886e-11  2.44005157e-11\n",
      "  2.84644718e-11 -1.60393180e-11 -2.08773643e-11 -2.05332723e-11\n",
      "  1.58030199e-11  1.03761550e-11 -1.75146329e-11 -1.81462117e-11\n",
      " -1.64446170e-11  1.71012765e-11  2.81641735e-11  3.21843997e-11\n",
      "  1.55842114e-11 -1.12318390e-11 -1.92758127e-11  2.37963252e-11\n",
      "  3.03680369e-11 -1.87172034e-11  1.57760173e-11  1.12448786e-11\n",
      " -2.04423767e-11 -1.73569720e-11 -2.05143741e-11 -1.88305274e-11\n",
      "  2.32417801e-11 -1.36254299e-11 -1.68648253e-11 -2.06368812e-11\n",
      " -1.46642733e-11  1.76430957e-11  2.20579040e-11 -1.91555891e-11\n",
      " -1.67150386e-11 -1.85446111e-11  1.14276482e-11 -2.02973481e-11\n",
      " -1.04092602e-11  2.27294890e-11  3.06090172e-11  7.51381618e-12\n",
      " -2.00792956e-11  1.47062924e-11  1.82162155e-11 -1.79397805e-11\n",
      "  3.02387562e-11  2.31590826e-11 -1.96106676e-11  2.99690264e-11\n",
      "  2.22345368e-11  1.58957239e-11 -1.46630007e-11  1.42105056e-11\n",
      " -1.74947983e-11  1.00897340e-11 -1.67350293e-11 -2.00352152e-11\n",
      " -1.80280702e-11  2.39573502e-11 -2.07912410e-11  3.50969011e-11\n",
      "  2.96728686e-11 -2.11425550e-12 -1.91888546e-11  1.36596447e-11\n",
      " -1.92825942e-11 -1.59292843e-11 -1.48221877e-11  1.95061577e-11\n",
      " -1.92316687e-11 -1.89591861e-11  2.31532210e-11 -1.48034991e-11\n",
      " -1.85255847e-11  1.42081963e-11 -2.03485768e-11  1.15096549e-11\n",
      " -2.00665389e-11 -1.77601953e-11 -1.34362701e-11  2.10130073e-11\n",
      "  1.08203332e-11 -1.88292712e-11 -1.84357208e-11 -1.99928904e-11\n",
      " -2.01295362e-11 -1.97805404e-11 -1.30103116e-11  6.47181383e-12\n",
      "  1.76123335e-11 -1.74546177e-11 -1.54810668e-11  2.55128360e-11\n",
      "  1.28479430e-11 -2.01878655e-11  4.50790265e-11 -1.86688790e-11\n",
      " -1.77719404e-11 -1.50658512e-11  3.18806982e-11 -1.52544232e-11\n",
      " -2.06396561e-11  2.30251137e-11  2.27318519e-11 -1.94400649e-11\n",
      "  1.86190120e-11  1.60561534e-11 -1.88091354e-11 -2.24295898e-11\n",
      " -1.83511192e-11  3.62328088e-11  1.48028060e-11  2.89711618e-11\n",
      "  1.43372286e-11  3.40252615e-12  1.46374955e-11  1.21663176e-11\n",
      " -2.05571950e-11  2.61102893e-11  2.64814029e-11  3.64437308e-11\n",
      "  1.26919657e-11  2.35231116e-11  1.29948496e-11  2.65616088e-11\n",
      "  3.56724993e-11]\n"
     ]
    }
   ],
   "source": [
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] > 1e-12]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] > 1e-12:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])\n",
    "\n",
    "# print(\"The following are support vectors: \")\n",
    "# for i in range(len(support_vectors)):\n",
    "#     print(support_vectors[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 7, 9, 10, 11, 15, 16, 20, 21, 25, 26, 27, 28, 31, 32, 34, 35, 40, 45, 46, 50, 53, 54, 55, 57, 58, 60, 61, 63, 64, 65, 67, 69, 73, 75, 76, 79, 83, 86, 89, 91, 95, 96, 103, 104, 107, 108, 110, 114, 117, 118, 120, 121, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 8\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 80.28169014084507%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainX\n",
    "y_train = trainY\n",
    "X_test = testX\n",
    "y_test = testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X_train and y_train the subset of data that describe the labels 0 and 2 (T-shirts and pullovers, respectively)\n",
    "\n",
    "new_X_train = []\n",
    "new_y_train = []\n",
    "for i in range(len(X_train)):\n",
    "    if y_train[i] == 0:\n",
    "        new_X_train.append(X_train[i])\n",
    "        new_y_train.append(y_train[i])\n",
    "    elif y_train[i] == 2:\n",
    "        new_X_train.append(X_train[i])\n",
    "        new_y_train.append(y_train[i])\n",
    "new_X_train = np.array(new_X_train)\n",
    "new_y_train = np.array(new_y_train)\n",
    "\n",
    "X_train = new_X_train\n",
    "y_train = new_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X_test and y_test the subset of data that describe the labels 0 and 2 (T-shirts and pullovers, respectively)\n",
    "\n",
    "new_X_test = []\n",
    "new_y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    if y_test[i] == 0:\n",
    "        new_X_test.append(X_test[i])\n",
    "        new_y_test.append(y_test[i])\n",
    "    elif y_test[i] == 2:\n",
    "        new_X_test.append(X_test[i])\n",
    "        new_y_test.append(y_test[i])\n",
    "new_X_test = np.array(new_X_test)\n",
    "new_y_test = np.array(new_y_test)\n",
    "\n",
    "X_test = new_X_test\n",
    "y_test = new_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784)\n",
      "(12000,)\n",
      "(2000, 784)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([X_train[i].flatten() for i in range(len(X_train))])\n",
    "X_test = np.array([X_test[i].flatten() for i in range(len(X_test))])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 784)\n",
      "(141,)\n",
      "(213, 784)\n",
      "(213,)\n"
     ]
    }
   ],
   "source": [
    "# Downsample the data\n",
    "\n",
    "# Add y_train back as an additional column to X_train\n",
    "y_train = y_train.reshape((-1,1))\n",
    "X_train = np.append(X_train, y_train, axis=1)\n",
    "\n",
    "# Add y_test back as an additional column to X_test\n",
    "y_test = y_test.reshape((-1,1))\n",
    "X_test = np.append(X_test, y_test, axis=1)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(X_train)\n",
    "np.random.shuffle(X_test)\n",
    "\n",
    "# Slice out only the first 141 from X_train and 213 from X_test\n",
    "X_train = X_train[0:141]\n",
    "X_test = X_test[0:213]\n",
    "\n",
    "# Remove the last columns of X_train and X_test and place them back into y_train and y_test\n",
    "y_train = X_train[:,-1]\n",
    "y_test = X_test[:,-1]\n",
    "X_train = X_train[:,0:X_train.shape[1]-1]\n",
    "X_test = X_test[:,0:X_test.shape[1]-1]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset\n",
    "\n",
    "X_scale = StandardScaler()\n",
    "X_train = X_scale.fit_transform(X_train) \n",
    "X_test = X_scale.fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.9000e+01 -1.3800e+02  3e+02  1e+01  2e+00\n",
      " 1: -3.0922e+02 -3.1231e+02  8e+01  6e+00  1e+00\n",
      " 2: -3.4741e+04 -3.4744e+04  8e+01  6e+00  1e+00\n",
      " 3: -3.4434e+08 -3.4434e+08  4e+02  6e+00  1e+00\n",
      " 4: -3.4090e+14 -3.4090e+14  3e+06  6e+00  1e+00\n",
      " 5: -3.3749e+22 -3.3749e+22  3e+12  4e+00  1e+00\n",
      " 6: -3.3412e+32 -3.3412e+32  3e+20  9e+15  1e+00\n",
      " 7: -3.3075e+44 -3.3075e+44  3e+30  5e+27  1e+00\n",
      " 8: -3.3278e+58 -3.3278e+58  3e+42  1e+42  1e+00\n",
      " 9: -6.4522e+74 -6.4522e+74  6e+56  1e+58  1e+00\n",
      "10: -6.3877e+110 -2.0609e+112  2e+112  9e+93  2e+01\n",
      "11: -6.7860e+110 -8.9075e+110  2e+110  2e+94  9e-01\n",
      "12: -6.7860e+110 -8.9075e+110  2e+110  2e+94  9e-01\n",
      "13: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+00\n",
      "14: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+00\n",
      "15: -6.7860e+110 -8.9075e+110  2e+110  2e+94  6e+00\n",
      "16: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+01\n",
      "17: -6.7860e+110 -8.9075e+110  2e+110  2e+94  6e+01\n",
      "18: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+02\n",
      "19: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+02\n",
      "20: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+03\n",
      "21: -6.7860e+110 -8.9075e+110  2e+110  2e+94  4e+03\n",
      "22: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+04\n",
      "23: -6.7860e+110 -8.9075e+110  2e+110  2e+94  4e+04\n",
      "24: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+05\n",
      "25: -6.7860e+110 -8.9075e+110  2e+110  2e+94  4e+05\n",
      "26: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+06\n",
      "27: -6.7860e+110 -8.9075e+110  2e+110  2e+94  5e+06\n",
      "28: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+07\n",
      "29: -6.7860e+110 -8.9075e+110  2e+110  2e+94  4e+07\n",
      "30: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+08\n",
      "31: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+08\n",
      "32: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+09\n",
      "33: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+09\n",
      "34: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+10\n",
      "35: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+10\n",
      "36: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+11\n",
      "37: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+11\n",
      "38: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+12\n",
      "39: -6.7860e+110 -8.9075e+110  2e+110  2e+94  4e+12\n",
      "40: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+13\n",
      "41: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+13\n",
      "42: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+14\n",
      "43: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+14\n",
      "44: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+15\n",
      "45: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+15\n",
      "46: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+16\n",
      "47: -6.7860e+110 -8.9075e+110  2e+110  2e+94  6e+16\n",
      "48: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+17\n",
      "49: -6.7860e+110 -8.9075e+110  2e+110  2e+94  4e+17\n",
      "50: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+18\n",
      "51: -6.7860e+110 -8.9075e+110  2e+110  2e+94  4e+18\n",
      "52: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+19\n",
      "53: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+19\n",
      "54: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+20\n",
      "55: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+20\n",
      "56: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+21\n",
      "57: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+21\n",
      "58: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+22\n",
      "59: -6.7860e+110 -8.9075e+110  2e+110  2e+94  6e+22\n",
      "60: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+23\n",
      "61: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+23\n",
      "62: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+24\n",
      "63: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+24\n",
      "64: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+25\n",
      "65: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+25\n",
      "66: -6.7860e+110 -8.9075e+110  2e+110  2e+94  8e+25\n",
      "67: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+26\n",
      "68: -6.7860e+110 -8.9075e+110  2e+110  2e+94  5e+26\n",
      "69: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+27\n",
      "70: -6.7860e+110 -8.9075e+110  2e+110  2e+94  9e+27\n",
      "71: -6.7860e+110 -8.9075e+110  2e+110  2e+94  4e+28\n",
      "72: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+29\n",
      "73: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+29\n",
      "74: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+30\n",
      "75: -6.7860e+110 -8.9075e+110  2e+110  2e+94  2e+30\n",
      "76: -6.7860e+110 -8.9075e+110  2e+110  2e+94  1e+31\n",
      "77: -6.7860e+110 -8.9075e+110  2e+110  2e+94  3e+31\n",
      "78: -6.7860e+110 -8.9075e+110  2e+110  2e-03  9e+31\n",
      "79: -3.3393e+111 -3.3497e+111  1e+109  7e+94  7e+31\n",
      "80: -1.0641e+114 -1.0641e+114  3e+109  2e+97  9e+31\n",
      "81: -1.0641e+114 -1.0641e+114  3e+109  1e-03  8e+32\n",
      "82: -1.0641e+114 -1.0641e+114  3e+109  2e+97  1e+33\n",
      "83: -1.0641e+114 -1.0641e+114  3e+109  4e+97  2e+33\n",
      "84: -3.3697e+118 -3.3697e+118  1e+112 6e+101  2e+33\n",
      "85: -1.0670e+125 -1.0670e+125  3e+116 2e+108  2e+33\n",
      "86: -1.0671e+125 -1.0671e+125  3e+116 2e+108  9e+33\n",
      "87: -3.3790e+133 -3.3790e+133  1e+123  1e-03  1e+34\n",
      "88: -1.0700e+144 -1.0700e+144  3e+131 2e+127  1e+34\n",
      "89: -3.3901e+156 -3.3901e+156  1e+142  1e-03  1e+34\n",
      "90: -1.1049e+171 -1.1049e+171  3e+154  1e-03  1e+34\n",
      "91: -1.1049e+171 -1.1049e+171  3e+154  1e-03  2e+34\n",
      "92: -1.1049e+171 -1.1049e+171  3e+154  1e-03  9e+34\n",
      "93: -1.1049e+171 -1.1049e+171  3e+154  1e-03  5e+35\n",
      "94: -1.1049e+171 -1.1049e+171  3e+154  1e-03  8e+35\n",
      "95: -1.1049e+171 -1.1049e+171  3e+154  1e-03  1e+36\n",
      "96: -1.1049e+171 -1.1049e+171  3e+154  1e-03  9e+36\n",
      "97: -1.1049e+171 -1.1049e+171  3e+154  1e-03  4e+37\n",
      "98: -1.1049e+171 -1.1049e+171  3e+154  1e-03  1e+38\n",
      "99: -1.1049e+171 -1.1049e+171  3e+154  1e-03  3e+38\n",
      "100: -1.1049e+171 -1.1049e+171  3e+154  1e-03  1e+39\n",
      "Terminated (maximum number of iterations reached).\n",
      "[-4.36832463e-005  1.60126991e+169  1.60126991e+169  1.60126991e+169\n",
      "  1.60126991e+169 -2.50635393e-005  1.60126991e+169 -2.52940103e-005\n",
      " -3.42457291e-005 -3.82874653e-005  1.60126991e+169 -4.30658252e-005\n",
      "  1.60126991e+169  1.60126991e+169 -7.12594231e-006 -2.35382060e-005\n",
      "  1.60126991e+169 -4.37280782e-005 -1.42499712e-004  1.60126991e+169\n",
      " -1.93066767e-005  1.60126991e+169 -3.64045063e-005 -5.43628844e-005\n",
      "  1.60126991e+169  1.60126991e+169 -2.38517534e-005 -3.28450568e-005\n",
      " -2.92677489e-005 -2.39794707e-005  1.60126991e+169  1.60126991e+169\n",
      " -3.61889185e-005 -3.56961741e-005  1.60126991e+169  1.60126991e+169\n",
      " -4.99420259e-005  1.60126991e+169  1.60126991e+169  1.60126991e+169\n",
      " -3.19557371e-005  1.60126991e+169  2.52890458e-004  1.60126991e+169\n",
      "  1.60126991e+169  1.60126991e+169  2.09274046e-004  1.60126991e+169\n",
      "  1.60126991e+169  2.36110630e-004 -4.94395929e-005  1.60126991e+169\n",
      "  2.36225007e-004  1.60126991e+169  1.60126991e+169  1.60126991e+169\n",
      "  1.60126991e+169  1.60126991e+169 -6.13180772e-005 -2.68253058e-005\n",
      " -4.23972366e-005  3.16422420e-005  1.60126991e+169  1.60126991e+169\n",
      "  1.60126991e+169 -3.36585096e-005  1.60126991e+169  1.60126991e+169\n",
      " -1.92896261e-005 -4.67887821e-005  1.60126991e+169  1.60126991e+169\n",
      " -3.22730679e-005 -2.84468646e-005 -3.91821629e-005 -1.52418067e-004\n",
      " -1.14536395e-005  1.60126991e+169  1.60126991e+169  1.60126991e+169\n",
      "  1.60126991e+169  1.60126991e+169 -2.02933422e-005  1.60126991e+169\n",
      " -8.49006538e-005 -5.28916798e-005 -3.20501595e-005  1.60126991e+169\n",
      "  1.60126991e+169  1.21736347e-005  2.48605604e-004  1.60126991e+169\n",
      " -6.15455858e-005  1.60126991e+169 -3.99073196e-005  1.60126991e+169\n",
      "  1.60126991e+169 -4.04070939e-005 -3.52077176e-005  2.50894022e-004\n",
      " -8.14552659e-006  1.60126991e+169  2.42511289e-004 -6.99037732e-005\n",
      "  1.60126991e+169  1.60126991e+169  1.60126991e+169 -2.35685351e-005\n",
      "  1.60126991e+169 -2.15473322e-005 -1.48472505e-005  1.60126991e+169\n",
      "  1.60126991e+169 -7.21147894e-005  1.60126991e+169  1.60126991e+169\n",
      "  1.60126991e+169 -8.17787575e-005  1.60126991e+169 -7.76424146e-005\n",
      " -3.05649048e-005 -5.99569382e-005  1.60126991e+169  1.60126991e+169\n",
      " -2.04345571e-005  1.60126991e+169  1.60126991e+169 -5.65418710e-005\n",
      " -4.38314038e-005  2.61282437e-004 -1.13264675e-005  1.60126991e+169\n",
      " -3.67307712e-005  1.60126991e+169 -2.70057353e-005 -4.83978270e-005\n",
      " -4.11541430e-005 -3.63171399e-005 -3.80665999e-005 -4.07218815e-005\n",
      "  1.60126991e+169]\n"
     ]
    }
   ],
   "source": [
    "def kernel_svm(X, y): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)*1.\n",
    "    X_y = y*X\n",
    "    H = np.dot(X_y, X_y.T)*1.\n",
    "\n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    G = matrix(-np.eye(m))\n",
    "    h = matrix(np.zeros(m))\n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_svm(X_train, y_train)\n",
    "\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] < 1e-7]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] < 1e-7:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 7, 8, 9, 11, 14, 15, 17, 18, 20, 22, 23, 26, 27, 28, 29, 32, 33, 36, 40, 50, 58, 59, 60, 65, 68, 69, 72, 73, 74, 75, 76, 82, 84, 85, 86, 92, 94, 97, 98, 100, 103, 107, 109, 110, 113, 117, 119, 120, 121, 124, 127, 128, 130, 132, 134, 135, 136, 137, 138, 139]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 94.83568075117371%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended implementation using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.7950e+01 -2.4240e+01  6e+02  2e+01  2e-16\n",
      " 1: -5.8654e+00 -2.2413e+01  2e+01  2e-01  5e-16\n",
      " 2: -6.3600e+00 -7.4989e+00  1e+00  9e-03  4e-16\n",
      " 3: -6.8946e+00 -6.9069e+00  1e-02  9e-05  3e-16\n",
      " 4: -6.8999e+00 -6.9001e+00  1e-04  9e-07  2e-16\n",
      " 5: -6.9000e+00 -6.9000e+00  1e-06  9e-09  3e-16\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "def kernel_soft_margin_svm(X, y, C): \n",
    "\n",
    "    m,n = X.shape\n",
    "    y = y.reshape(-1,1)*1.\n",
    "    X_y = X*y\n",
    "    H = np.dot(X_y, X_y.T)*1.\n",
    "    \n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    \n",
    "    # Changed G and h\n",
    "    G = matrix(np.vstack((np.diag(np.ones(m))*-1, np.identity(m))))\n",
    "    h = matrix(np.hstack((np.zeros(m), np.ones(m)*C)))\n",
    "    \n",
    "    A = matrix(y.reshape(1,-1))\n",
    "    A = matrix(A, (1, m), 'd')\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    sol = solvers.qp(P,q,G,h,A,b) \n",
    "    \n",
    "    alphas = np.array(sol['x'])[:,0]\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "# fit svm dual classifier\n",
    "alphas = kernel_soft_margin_svm(X_train, y_train, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.49642095e-24  9.99999922e-02  9.99999922e-02  9.99999922e-02\n",
      "  9.99999922e-02  4.54330640e-24  9.99999922e-02  1.94066816e-24\n",
      "  2.10612603e-24 -1.18189551e-23  9.99999922e-02  2.62474435e-24\n",
      "  9.99999922e-02  9.99999922e-02 -4.99045415e-24 -1.86933099e-24\n",
      "  9.99999922e-02  1.05629820e-23 -6.78707232e-24  9.99999922e-02\n",
      " -3.37181898e-24  9.99999922e-02 -4.80846074e-24  3.67788149e-24\n",
      "  9.99999922e-02  9.99999922e-02  6.20992933e-24 -5.94485071e-24\n",
      "  3.23737956e-24  1.84407469e-24  9.99999922e-02  9.99999922e-02\n",
      "  1.11701552e-23  1.75627956e-23  9.99999922e-02  9.99999922e-02\n",
      "  2.53331950e-24  9.99999922e-02  9.99999922e-02  9.99999922e-02\n",
      " -6.42898062e-24  9.99999922e-02  7.40177573e-24  9.99999922e-02\n",
      "  9.99999922e-02  9.99999922e-02 -7.93460148e-24  9.99999922e-02\n",
      "  9.99999922e-02  1.26553724e-23 -3.81814179e-24  9.99999922e-02\n",
      "  3.07022333e-24  9.99999922e-02  9.99999922e-02  9.99999922e-02\n",
      "  9.99999922e-02  9.99999922e-02 -5.41194888e-24 -5.90645082e-24\n",
      " -2.06147387e-24 -2.76313739e-24  9.99999922e-02  9.99999922e-02\n",
      "  9.99999922e-02 -1.01314386e-24  9.99999922e-02  9.99999922e-02\n",
      "  8.98060843e-25  5.76157572e-24  9.99999922e-02  9.99999922e-02\n",
      " -3.22525875e-24 -1.55632282e-24 -6.90941669e-24 -9.98720632e-24\n",
      " -1.43270507e-23  9.99999922e-02  9.99999922e-02  9.99999922e-02\n",
      "  9.99999922e-02  9.99999922e-02  9.76657857e-24  9.99999922e-02\n",
      "  1.68435771e-24 -1.71464496e-23  4.60544027e-25  9.99999922e-02\n",
      "  9.99999922e-02  1.35436710e-23  6.45254046e-24  9.99999922e-02\n",
      " -1.20871610e-23  9.99999922e-02  1.28505560e-23  9.99999922e-02\n",
      "  9.99999922e-02 -3.19367920e-24 -9.72031509e-24 -2.28368442e-23\n",
      " -3.78026899e-24  9.99999922e-02  8.95664553e-24  2.70740794e-23\n",
      "  9.99999922e-02  9.99999922e-02  9.99999922e-02  4.64124281e-24\n",
      "  9.99999922e-02  8.97202378e-24 -2.04537264e-23  9.99999922e-02\n",
      "  9.99999922e-02  6.82655385e-25  9.99999922e-02  9.99999922e-02\n",
      "  9.99999922e-02  2.01942931e-23  9.99999922e-02  1.12489873e-24\n",
      " -2.93053156e-24 -5.28239038e-25  9.99999922e-02  9.99999922e-02\n",
      " -9.07216378e-24  9.99999922e-02  9.99999922e-02  1.08683803e-23\n",
      "  2.66244858e-24 -6.46116602e-24 -3.82525908e-25  9.99999922e-02\n",
      "  3.09351138e-24  9.99999922e-02 -5.09376656e-24  2.33720731e-24\n",
      " -1.49234824e-24 -1.05249527e-24  5.45030788e-25 -1.16208658e-23\n",
      "  9.99999922e-02]\n"
     ]
    }
   ],
   "source": [
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_boundary (X, y, alpha):\n",
    "    #cond = (alphas > 1e-3).reshape(-1)\n",
    "    cond = [i for i in range(len(alphas)) if alphas[i] < 1e-2]\n",
    "    w = np.dot(X.T, alpha*y).reshape(-1,1)\n",
    "    w0 = y[cond] - np.dot(X[cond], w)\n",
    "    w0 = np.mean(w0)\n",
    "    return w, w0\n",
    "\n",
    "\n",
    "\n",
    "w, w0 = compute_classification_boundary(X_train, y_train, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which training examples are support vectors\n",
    "support_vectors = []\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    if alphas[i] < 1e-2:\n",
    "        support_vectors.append([X_train[i], y_train[i], i])\n",
    "\n",
    "# print(\"The following are support vectors: \")\n",
    "# for i in range(len(support_vectors)):\n",
    "#     print(support_vectors[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 7, 8, 9, 11, 14, 15, 17, 18, 20, 22, 23, 26, 27, 28, 29, 32, 33, 36, 40, 42, 46, 49, 50, 52, 58, 59, 60, 61, 65, 68, 69, 72, 73, 74, 75, 76, 82, 84, 85, 86, 89, 90, 92, 94, 97, 98, 99, 100, 102, 103, 107, 109, 110, 113, 117, 119, 120, 121, 124, 127, 128, 129, 130, 132, 134, 135, 136, 137, 138, 139]\n"
     ]
    }
   ],
   "source": [
    "def K(xi, xj):\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "alpha_indices = [support_vectors[i][2] for i in range(len(support_vectors))]\n",
    "print(alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dual(x):\n",
    "    summation = 0\n",
    "    for i in range(len(support_vectors)):\n",
    "        summation += alphas[alpha_indices[i]]*y_train[alpha_indices[i]]*K(X_train[alpha_indices[i]],x)\n",
    "    if (summation >= 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM dual classifier on X_test\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(f_dual(X_test[i]))\n",
    "    return predictions\n",
    "\n",
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 47.88732394366197%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
